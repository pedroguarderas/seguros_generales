<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Leonardo Vélez y Pedro Guarderas">

<title>6&nbsp; Distribuciones – Matemáticas Actuariales para Seguros Generales</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./lectura_06.html" rel="next">
<link href="./lectura_04.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-257450a62f84a916ff723c9590a8c97c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>
window.MathJax = {
  tex: {
    macros: {
      R: "\\mathbb{R}",
      C: "\\mathbb{C}",
      N: "\\mathbb{N}",
      Z: "\\mathbb{Z}",
      Fo: "\\mathscr{F}",
      E: "\\mathbb{E}",
      V: "\\mathbb{V}",
      D: "\\partial",
      F: "\\mathcal{F}",
      arginf: "\\operatorname{arginf}",
      argsup: "\\operatorname{argsup}",
      DFT: "\\operatorname{DFT}",
      VaR: "\\operatorname{VaR}",
      CVaR: "\\operatorname{CVaR}",
      TVaR: "\\operatorname{TVaR}",
      EVaR: "\\operatorname{EVaR}",
      CTE: "\\operatorname{CTE}",
      ES: "\\operatorname{ES}",
      Re: "\\operatorname{Re}",
      Dif: "\\operatorname{Dif}",
      VC: "\\operatorname{VC}"
    }
  }
};
</script>
<script src="site_libs/kePrint-0.0.1/kePrint.js"></script>
<link href="site_libs/lightable-0.0.1/lightable.css" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="style.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./lectura_05.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Distribuciones</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Matemáticas Actuariales para Seguros Generales</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/pedroguarderas/seguros_generales" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./Matemáticas-Actuariales-para-Seguros-Generales.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">index.html</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./lectura_01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Materia</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./lectura_02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Preliminares</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./lectura_03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Introducción a las operaciones de seguros</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./lectura_04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Probabilidad y Estadística</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./lectura_05.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Distribuciones</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./lectura_06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Modelos de pérdida agregada</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./lectura_07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Tarificación</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./lectura_08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Modelos lineales generalizados (GLM)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./lectura_09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Reservas técnicas</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./lectura_10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Estimación de valores extremos</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./lectura_11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Bibliografía</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#distribuciones-discretas" id="toc-distribuciones-discretas" class="nav-link active" data-scroll-target="#distribuciones-discretas"><span class="header-section-number">6.1</span> Distribuciones discretas</a>
  <ul class="collapse">
  <li><a href="#distribución-binomial" id="toc-distribución-binomial" class="nav-link" data-scroll-target="#distribución-binomial"><span class="header-section-number">6.1.1</span> Distribución binomial</a></li>
  <li><a href="#distribución-de-poisson" id="toc-distribución-de-poisson" class="nav-link" data-scroll-target="#distribución-de-poisson"><span class="header-section-number">6.1.2</span> Distribución de Poisson</a></li>
  <li><a href="#distribución-binomial-negativa" id="toc-distribución-binomial-negativa" class="nav-link" data-scroll-target="#distribución-binomial-negativa"><span class="header-section-number">6.1.3</span> Distribución binomial negativa</a></li>
  </ul></li>
  <li><a href="#familia-de-panjer" id="toc-familia-de-panjer" class="nav-link" data-scroll-target="#familia-de-panjer"><span class="header-section-number">6.2</span> Familia de Panjer</a></li>
  <li><a href="#distribuciones-continuas" id="toc-distribuciones-continuas" class="nav-link" data-scroll-target="#distribuciones-continuas"><span class="header-section-number">6.3</span> Distribuciones continuas</a>
  <ul class="collapse">
  <li><a href="#distribución-uniforme" id="toc-distribución-uniforme" class="nav-link" data-scroll-target="#distribución-uniforme"><span class="header-section-number">6.3.1</span> Distribución uniforme</a></li>
  <li><a href="#distribución-exponencial" id="toc-distribución-exponencial" class="nav-link" data-scroll-target="#distribución-exponencial"><span class="header-section-number">6.3.2</span> Distribución exponencial</a></li>
  <li><a href="#distribución-gamma" id="toc-distribución-gamma" class="nav-link" data-scroll-target="#distribución-gamma"><span class="header-section-number">6.3.3</span> Distribución gamma</a></li>
  <li><a href="#distribución-normal" id="toc-distribución-normal" class="nav-link" data-scroll-target="#distribución-normal"><span class="header-section-number">6.3.4</span> Distribución normal</a></li>
  <li><a href="#distribución-log-normal" id="toc-distribución-log-normal" class="nav-link" data-scroll-target="#distribución-log-normal"><span class="header-section-number">6.3.5</span> Distribución log-normal</a></li>
  <li><a href="#distribución-de-beta" id="toc-distribución-de-beta" class="nav-link" data-scroll-target="#distribución-de-beta"><span class="header-section-number">6.3.6</span> Distribución de beta</a></li>
  <li><a href="#distribución-de-pareto-generalizada" id="toc-distribución-de-pareto-generalizada" class="nav-link" data-scroll-target="#distribución-de-pareto-generalizada"><span class="header-section-number">6.3.7</span> Distribución de Pareto generalizada</a></li>
  <li><a href="#distribución-de-valores-extremos-generalizada" id="toc-distribución-de-valores-extremos-generalizada" class="nav-link" data-scroll-target="#distribución-de-valores-extremos-generalizada"><span class="header-section-number">6.3.8</span> Distribución de valores extremos generalizada</a></li>
  <li><a href="#distribución-t-de-student" id="toc-distribución-t-de-student" class="nav-link" data-scroll-target="#distribución-t-de-student"><span class="header-section-number">6.3.9</span> Distribución t de Student</a></li>
  <li><a href="#distribución-gamma-transformada" id="toc-distribución-gamma-transformada" class="nav-link" data-scroll-target="#distribución-gamma-transformada"><span class="header-section-number">6.3.10</span> Distribución gamma transformada</a></li>
  <li><a href="#distribución-beta-transformada" id="toc-distribución-beta-transformada" class="nav-link" data-scroll-target="#distribución-beta-transformada"><span class="header-section-number">6.3.11</span> Distribución beta transformada</a></li>
  </ul></li>
  <li><a href="#estimación" id="toc-estimación" class="nav-link" data-scroll-target="#estimación"><span class="header-section-number">6.4</span> Estimación</a>
  <ul class="collapse">
  <li><a href="#método-de-sustitución" id="toc-método-de-sustitución" class="nav-link" data-scroll-target="#método-de-sustitución"><span class="header-section-number">6.4.1</span> Método de sustitución</a></li>
  <li><a href="#método-de-momentos" id="toc-método-de-momentos" class="nav-link" data-scroll-target="#método-de-momentos"><span class="header-section-number">6.4.2</span> Método de momentos</a></li>
  <li><a href="#método-de-la-distancia-mínima" id="toc-método-de-la-distancia-mínima" class="nav-link" data-scroll-target="#método-de-la-distancia-mínima"><span class="header-section-number">6.4.3</span> Método de la distancia mínima</a></li>
  <li><a href="#método-de-maximización-de-la-verosimilitud" id="toc-método-de-maximización-de-la-verosimilitud" class="nav-link" data-scroll-target="#método-de-maximización-de-la-verosimilitud"><span class="header-section-number">6.4.4</span> Método de maximización de la verosimilitud</a></li>
  <li><a href="#pruebas-de-hipótesis" id="toc-pruebas-de-hipótesis" class="nav-link" data-scroll-target="#pruebas-de-hipótesis"><span class="header-section-number">6.4.5</span> Pruebas de hipótesis</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/pedroguarderas/seguros_generales/edit/main/lectura_05.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/pedroguarderas/seguros_generales/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="distribuciones" class="quarto-section-identifier"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Distribuciones</span></span></h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<!------------------------------------------------------------------------------------------------->
<!------------------------------------------------------------------------------------------------->
<section id="distribuciones-discretas" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="distribuciones-discretas"><span class="header-section-number">6.1</span> Distribuciones discretas</h2>
<!------------------------------------------------------------------------------------------------->
<section id="distribución-binomial" class="level3" data-number="6.1.1">
<h3 data-number="6.1.1" class="anchored" data-anchor-id="distribución-binomial"><span class="header-section-number">6.1.1</span> Distribución binomial</h3>
<div class="thmbox">
<div id="dbinom" class="definition" name="Distribución binomial">
<p>Una variable aleatoria <span class="math inline">\(N\)</span> que toma valores en <span class="math inline">\(\N\)</span> se dice que sigue una distribución o ley de binomial <span class="math inline">\(N \rightsquigarrow Bin( n, p )\)</span>, con parámetros <span class="math inline">\(n \in \N\)</span> y <span class="math inline">\(p \in [0, 1]\)</span>, si: <span class="math display">\[
P( N = k ) = \binom{n}{k} p^k ( 1 - p )^{n-k}, \qquad
\forall k \in \{0,\ldots, n\}
\]</span> esta distribución discreta se caracteriza por presentar el valor <span class="math inline">\(k \frac{p_k}{p_{k-1}}\)</span> decreciente conforme cambia <span class="math inline">\(k \in \N\)</span></p>
</div>
</div>
<div class="cell">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a><span class="fu">set.seed</span>(<span class="dv">94312</span>)</span>
<span id="cb1-2"><a href="#cb1-2"></a>n <span class="ot">&lt;-</span> <span class="dv">50</span></span>
<span id="cb1-3"><a href="#cb1-3"></a>p <span class="ot">&lt;-</span> <span class="fl">0.3</span></span>
<span id="cb1-4"><a href="#cb1-4"></a>k <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb1-5"><a href="#cb1-5"></a>m <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb1-6"><a href="#cb1-6"></a>N <span class="ot">&lt;-</span> <span class="fu">rbinom</span>( <span class="at">n =</span> m, <span class="at">size =</span> n, <span class="at">prob =</span> p ) <span class="co"># simular una muestra de tamaño m</span></span>
<span id="cb1-7"><a href="#cb1-7"></a>pk <span class="ot">&lt;-</span> <span class="fu">dbinom</span>( <span class="at">x =</span> k, <span class="at">size =</span> n, <span class="at">prob =</span> p ) <span class="co"># cálculo de probabilidad P( N = k )</span></span>
<span id="cb1-8"><a href="#cb1-8"></a>Pk <span class="ot">&lt;-</span> <span class="fu">pbinom</span>( <span class="at">q =</span> k, <span class="at">size =</span> n, <span class="at">prob =</span> p ) <span class="co"># cálculo de probabilidad P( N &lt;= k )</span></span>
<span id="cb1-9"><a href="#cb1-9"></a></span>
<span id="cb1-10"><a href="#cb1-10"></a>p <span class="ot">&lt;-</span> <span class="fu">dbinom</span>( <span class="at">x =</span> <span class="dv">0</span><span class="sc">:</span>n, <span class="at">size =</span> n, <span class="at">prob =</span> p )</span>
<span id="cb1-11"><a href="#cb1-11"></a>v <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span>n <span class="sc">*</span> p[ <span class="dv">2</span><span class="sc">:</span>(n <span class="sc">+</span> <span class="dv">1</span>) ] <span class="sc">/</span> p[ <span class="dv">1</span><span class="sc">:</span>n ]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-layout-align="center">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource r fold-hide number-lines code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1"></a>plt <span class="ot">&lt;-</span> <span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb2-2"><a href="#cb2-2"></a>  <span class="fu">geom_point</span>( <span class="fu">aes</span>( <span class="at">x =</span> <span class="dv">1</span><span class="sc">:</span>n, <span class="at">y =</span> v ), <span class="at">colour =</span> <span class="st">'darkred'</span> ) <span class="sc">+</span> </span>
<span id="cb2-3"><a href="#cb2-3"></a>  <span class="fu">xlab</span>( <span class="fu">TeX</span>( <span class="st">"$k$"</span> ) ) <span class="sc">+</span> </span>
<span id="cb2-4"><a href="#cb2-4"></a>  <span class="fu">ylab</span>( <span class="fu">TeX</span>( <span class="st">"$k </span><span class="sc">\\</span><span class="st">frac{p_k}{p_{k-1}}$"</span> ) ) <span class="sc">+</span> </span>
<span id="cb2-5"><a href="#cb2-5"></a>  <span class="fu">theme_bw</span>()</span>
<span id="cb2-6"><a href="#cb2-6"></a><span class="fu">plot</span>( plt )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="lectura_05_files/figure-html/l5g1-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
</div>
</div>
<!------------------------------------------------------------------------------------------------->
</section>
<section id="distribución-de-poisson" class="level3" data-number="6.1.2">
<h3 data-number="6.1.2" class="anchored" data-anchor-id="distribución-de-poisson"><span class="header-section-number">6.1.2</span> Distribución de Poisson</h3>
<div class="thmbox">
<div id="dpois" class="definition" name="Distribución de Poisson">
<p>Una variable aleatoria <span class="math inline">\(N\)</span> que toma valores en <span class="math inline">\(\N\)</span> se dice que sigue una distribución o ley de Poisson <span class="math inline">\(N \rightsquigarrow Pois( n, p )\)</span>, con parámetro <span class="math inline">\(\lambda \in \R\)</span>, si: <span class="math display">\[
P( N = k ) = \exp\left( -\lambda \right) \frac{\lambda^k}{k!}, \qquad
\forall k \in \N
\]</span> esta distribución discreta se caracteriza por presentar el valor <span class="math inline">\(k \frac{p_k}{p_{k-1}}\)</span> constante conforme cambia <span class="math inline">\(k \in \N\)</span></p>
</div>
</div>
<div class="cell">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1"></a>lambda <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb3-2"><a href="#cb3-2"></a>k <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb3-3"><a href="#cb3-3"></a>m <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb3-4"><a href="#cb3-4"></a>N <span class="ot">&lt;-</span> <span class="fu">rpois</span>( <span class="at">n =</span> m, <span class="at">lambda =</span> lambda ) <span class="co"># simular una muestra de tamaño m</span></span>
<span id="cb3-5"><a href="#cb3-5"></a>pk <span class="ot">&lt;-</span> <span class="fu">dpois</span>( <span class="at">x =</span> k, <span class="at">lambda =</span> lambda ) <span class="co"># cálculo de probabilidad P( N = k )</span></span>
<span id="cb3-6"><a href="#cb3-6"></a>Pk <span class="ot">&lt;-</span> <span class="fu">ppois</span>( <span class="at">q =</span> k, <span class="at">lambda =</span> lambda ) <span class="co"># cálculo de probabilidad P( N &lt;= k )</span></span>
<span id="cb3-7"><a href="#cb3-7"></a></span>
<span id="cb3-8"><a href="#cb3-8"></a>n <span class="ot">&lt;-</span> <span class="dv">50</span></span>
<span id="cb3-9"><a href="#cb3-9"></a>p <span class="ot">&lt;-</span> <span class="fu">dpois</span>( <span class="at">x =</span> <span class="dv">0</span><span class="sc">:</span>n, <span class="at">lambda =</span> lambda )</span>
<span id="cb3-10"><a href="#cb3-10"></a>v <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span>n <span class="sc">*</span> p[ <span class="dv">2</span><span class="sc">:</span>(n <span class="sc">+</span> <span class="dv">1</span>) ] <span class="sc">/</span> p[ <span class="dv">1</span><span class="sc">:</span>n ]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-layout-align="center">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource r fold-hide number-lines code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1"></a>plt <span class="ot">&lt;-</span> <span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb4-2"><a href="#cb4-2"></a>  <span class="fu">geom_point</span>( <span class="fu">aes</span>( <span class="at">x =</span> <span class="dv">1</span><span class="sc">:</span>n, <span class="at">y =</span> v ), <span class="at">colour =</span> <span class="st">'darkred'</span> ) <span class="sc">+</span> </span>
<span id="cb4-3"><a href="#cb4-3"></a>  <span class="fu">xlab</span>( <span class="fu">TeX</span>( <span class="st">"$k$"</span> ) ) <span class="sc">+</span> </span>
<span id="cb4-4"><a href="#cb4-4"></a>  <span class="fu">ylab</span>( <span class="fu">TeX</span>( <span class="st">"$k </span><span class="sc">\\</span><span class="st">frac{p_k}{p_{k-1}}$"</span> ) ) <span class="sc">+</span> </span>
<span id="cb4-5"><a href="#cb4-5"></a>  <span class="fu">theme_bw</span>()</span>
<span id="cb4-6"><a href="#cb4-6"></a><span class="fu">plot</span>( plt )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="lectura_05_files/figure-html/l5g2-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
</div>
</div>
<!------------------------------------------------------------------------------------------------->
</section>
<section id="distribución-binomial-negativa" class="level3" data-number="6.1.3">
<h3 data-number="6.1.3" class="anchored" data-anchor-id="distribución-binomial-negativa"><span class="header-section-number">6.1.3</span> Distribución binomial negativa</h3>
<div class="thmbox">
<div id="dnbinom" class="definition" name="Distribución binomial negativa">
<p>Una variable aleatoria <span class="math inline">\(N\)</span> que toma valores en <span class="math inline">\(\N\)</span> se dice que sigue una distribución o ley de binomial negativa <span class="math inline">\(N \rightsquigarrow NBin( \alpha, p )\)</span>, con parámetro <span class="math inline">\(\alpha &gt; 0\)</span> y <span class="math inline">\(p \in (0,1)\)</span>, si: <span class="math display">\[
P( N = k ) = \binom{\alpha + k - 1}{k} p^\alpha ( 1 - p )^k
= \frac{\Gamma( \alpha + k )}{\Gamma(k+1) \Gamma(\alpha)}p^\alpha ( 1 - p )^k, \qquad
\forall k \in \N
\]</span> donde <span class="math inline">\(\Gamma( \alpha ) = \int\limits_0^{+\infty} x^{\alpha - 1} \exp(-x)\ dx\)</span>, <span class="math inline">\(\forall \alpha \geq 0\)</span>.</p>
<p>Esta distribución discreta se caracteriza por presentar el valor <span class="math inline">\(k \frac{p_k}{p_{k-1}}\)</span> creciente conforme cambia <span class="math inline">\(k \in \N\)</span></p>
</div>
</div>
<div class="cell">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1"></a>alpha <span class="ot">&lt;-</span> <span class="fl">2.5</span></span>
<span id="cb5-2"><a href="#cb5-2"></a>p <span class="ot">&lt;-</span> <span class="fl">0.3</span></span>
<span id="cb5-3"><a href="#cb5-3"></a>k <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb5-4"><a href="#cb5-4"></a>m <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb5-5"><a href="#cb5-5"></a>N <span class="ot">&lt;-</span> <span class="fu">rnbinom</span>( <span class="at">n =</span> m, <span class="at">size =</span> alpha, <span class="at">prob =</span> p  ) <span class="co"># simular una muestra de tamaño m</span></span>
<span id="cb5-6"><a href="#cb5-6"></a>pk <span class="ot">&lt;-</span> <span class="fu">dnbinom</span>( <span class="at">x =</span> k, <span class="at">size =</span> alpha, <span class="at">prob =</span> p ) <span class="co"># cálculo de probabilidad P( N = k )</span></span>
<span id="cb5-7"><a href="#cb5-7"></a>Pk <span class="ot">&lt;-</span> <span class="fu">pnbinom</span>( <span class="at">q =</span> k, <span class="at">size =</span> alpha, <span class="at">prob =</span> p ) <span class="co"># cálculo de probabilidad P( N &lt;= k )</span></span>
<span id="cb5-8"><a href="#cb5-8"></a></span>
<span id="cb5-9"><a href="#cb5-9"></a>n <span class="ot">&lt;-</span> <span class="dv">50</span></span>
<span id="cb5-10"><a href="#cb5-10"></a>p <span class="ot">&lt;-</span> <span class="fu">dnbinom</span>( <span class="at">x =</span> <span class="dv">0</span><span class="sc">:</span>n, <span class="at">size =</span> alpha, <span class="at">prob =</span> p )</span>
<span id="cb5-11"><a href="#cb5-11"></a>v <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span>n <span class="sc">*</span> p[ <span class="dv">2</span><span class="sc">:</span>(n <span class="sc">+</span> <span class="dv">1</span>) ] <span class="sc">/</span> p[ <span class="dv">1</span><span class="sc">:</span>n ]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-layout-align="center">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource r fold-hide number-lines code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1"></a>plt <span class="ot">&lt;-</span> <span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb6-2"><a href="#cb6-2"></a>  <span class="fu">geom_point</span>( <span class="fu">aes</span>( <span class="at">x =</span> <span class="dv">1</span><span class="sc">:</span>n, <span class="at">y =</span> v ), <span class="at">colour =</span> <span class="st">'darkred'</span> ) <span class="sc">+</span> </span>
<span id="cb6-3"><a href="#cb6-3"></a>  <span class="fu">xlab</span>( <span class="fu">TeX</span>( <span class="st">"$k$"</span> ) ) <span class="sc">+</span> </span>
<span id="cb6-4"><a href="#cb6-4"></a>  <span class="fu">ylab</span>( <span class="fu">TeX</span>( <span class="st">"$k </span><span class="sc">\\</span><span class="st">frac{p_k}{p_{k-1}}$"</span> ) ) <span class="sc">+</span> </span>
<span id="cb6-5"><a href="#cb6-5"></a>  <span class="fu">theme_bw</span>()</span>
<span id="cb6-6"><a href="#cb6-6"></a><span class="fu">plot</span>( plt )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="lectura_05_files/figure-html/l5g3-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
</div>
</div>
<div class="thmbox">
<div id="dgeo" class="definition" name="Distribución geométrica">
<p>Una variable aleatoria <span class="math inline">\(N\)</span> que toma valores en <span class="math inline">\(\N\)</span> se dice que sigue una distribución o ley geométrica <span class="math inline">\(N \rightsquigarrow Geo( p )\)</span>, con parámetro <span class="math inline">\(p \in (0,1]\)</span>, si: <span class="math display">\[
P( N = k ) = ( 1 - p )^k p, \qquad
\forall k \in \N
\]</span></p>
</div>
</div>
<div class="cell">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1"></a>p <span class="ot">&lt;-</span> <span class="fl">0.3</span></span>
<span id="cb7-2"><a href="#cb7-2"></a>k <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb7-3"><a href="#cb7-3"></a>m <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb7-4"><a href="#cb7-4"></a>N <span class="ot">&lt;-</span> <span class="fu">rgeom</span>( <span class="at">n =</span> m, <span class="at">prob =</span> p  ) <span class="co"># simular una muestra de tamaño m</span></span>
<span id="cb7-5"><a href="#cb7-5"></a>pk <span class="ot">&lt;-</span> <span class="fu">dgeom</span>( <span class="at">x =</span> k, <span class="at">prob =</span> p ) <span class="co"># cálculo de probabilidad P( N = k )</span></span>
<span id="cb7-6"><a href="#cb7-6"></a>Pk <span class="ot">&lt;-</span> <span class="fu">pgeom</span>( <span class="at">q =</span> k, <span class="at">prob =</span> p ) <span class="co"># cálculo de probabilidad P( N &lt;= k )</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Es fácil darse cuenta que la distribución geométrica <span class="math inline">\(Geo( p )\)</span> es una binomial negativa <span class="math inline">\(BN( 1, p )\)</span>, con <span class="math inline">\(\alpha = 1\)</span>.</p>
<p>Asociado a estas distribuciones discretas existe un resultado de caracterización, el cual permite seleccionar la distribución de conteo.</p>
<!------------------------------------------------------------------------------------------------->
</section>
</section>
<section id="familia-de-panjer" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="familia-de-panjer"><span class="header-section-number">6.2</span> Familia de Panjer</h2>
<p>El criterio anterior para identificar el tipo de distribución, mediante la observación del comportamiento de la variable <span class="math inline">\(k \frac{p_k}{p_{k-1}}\)</span>, se formaliza precisamente en la definición de la familia de Panjer.</p>
<div class="thmbox">
<div id="fampanjer" class="definition" name="Familia de Panjer">
<p>Una variable aleatoria discreta <span class="math inline">\(N\)</span>, que toma valores enteros positivos <span class="math inline">\(N \in \N\)</span>, se dice que pertenece a la <strong>familia de Panjer</strong>, si sus probabilidades <span class="math inline">\(p_k = P( N = k )\)</span> para cada <span class="math inline">\(k \in \N\)</span>, satisfacen la siguiente relación de recurrencia. <span class="math display">\[
p_k = \left( a + \frac{b}{k} \right)p_{k-1},\qquad \forall k \in \N \setminus \{0\}
\]</span></p>
</div>
</div>
<p>Además tenemos la siguiente proposición que caracteriza a la distribución de las variables aleatorias en la familia de Panjer.</p>
<div class="thmbox">
<div id="chfampanjer" class="proposition" name="Caracterización familia de Panjer">
<p>Las únicas leyes de probabilidad que satisfacen la relación de recurrencia anterior son:</p>
<ol type="1">
<li><p>La ley de Poisson, la cual se obtiene para <span class="math inline">\(a = 0\)</span> y <span class="math inline">\(b &gt; 0\)</span> <span class="math display">\[
k \frac{p_k}{p_{k-1}} = b &gt; 0,\quad \text{constante en $k$}
\]</span></p></li>
<li><p>La ley binomial negativa, la cual se obtiene para <span class="math inline">\(0 &lt; a &lt; 1\)</span> y <span class="math inline">\(a + b &gt; 0\)</span> <span class="math display">\[
k \frac{p_k}{p_{k-1}} = a k + b &gt; 0,\quad \text{creciente en $k$}
\]</span></p></li>
<li><p>La ley binomial, la cual se obtenida para <span class="math inline">\(a &lt; 0\)</span> y <span class="math inline">\(b = -a(m + 1)\)</span>, para cierto <span class="math inline">\(m\)</span> entero y positivo. <span class="math display">\[
k \frac{p_k}{p_{k-1}} = a( k - m - 1 ) &lt; 0, \quad \text{decreciente en $k$}
\]</span></p></li>
</ol>
</div>
</div>
<p>Para una demostración detallada de la proposición anterior se puede consultar <span class="citation" data-cites="MathAssuNV1">[<a href="#ref-MathAssuNV1" role="doc-biblioref">3</a>]</span> o en https://nonlifemaths.github.io/.</p>
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource r fold-hide number-lines code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1"></a><span class="co"># la librería CASdatasets fue previamente cargada</span></span>
<span id="cb8-2"><a href="#cb8-2"></a><span class="fu">data</span>( beMTPL97 )</span>
<span id="cb8-3"><a href="#cb8-3"></a>beMTPL97 <span class="ot">&lt;-</span> <span class="fu">as.data.table</span>( beMTPL97 )</span>
<span id="cb8-4"><a href="#cb8-4"></a>conteo <span class="ot">&lt;-</span> beMTPL97[ , <span class="fu">list</span>( <span class="at">fn =</span> .N ), by <span class="ot">=</span> <span class="fu">list</span>( sex, fuel, <span class="at">N =</span> nclaims ) ]</span>
<span id="cb8-5"><a href="#cb8-5"></a>conteo[ , pn <span class="sc">:</span><span class="er">=</span> fn <span class="sc">/</span> <span class="fu">sum</span>( fn ), by <span class="ot">=</span> <span class="fu">list</span>( sex, fuel ) ]</span>
<span id="cb8-6"><a href="#cb8-6"></a><span class="fu">setorder</span>( conteo, sex, fuel, N )</span>
<span id="cb8-7"><a href="#cb8-7"></a>conteo[ , pns <span class="sc">:</span><span class="er">=</span> <span class="fu">shift</span>( pn, <span class="at">type =</span> <span class="st">'lag'</span>, <span class="at">fill =</span> <span class="dv">0</span> ) ]</span>
<span id="cb8-8"><a href="#cb8-8"></a>conteo[ , jn <span class="sc">:</span><span class="er">=</span> N <span class="sc">*</span> pns <span class="sc">/</span> pn ]</span>
<span id="cb8-9"><a href="#cb8-9"></a></span>
<span id="cb8-10"><a href="#cb8-10"></a>conteo <span class="sc">%&gt;%</span></span>
<span id="cb8-11"><a href="#cb8-11"></a>  <span class="fu">kable</span>(</span>
<span id="cb8-12"><a href="#cb8-12"></a>    <span class="at">label =</span> <span class="cn">NA</span>,</span>
<span id="cb8-13"><a href="#cb8-13"></a>    <span class="at">caption =</span> <span class="st">'Estimación conteos por sexo'</span>,</span>
<span id="cb8-14"><a href="#cb8-14"></a>    <span class="at">row.names =</span> <span class="cn">FALSE</span>,</span>
<span id="cb8-15"><a href="#cb8-15"></a>    <span class="at">col.names =</span> <span class="fu">c</span>( <span class="st">"sexo"</span>, <span class="st">"fuel"</span>, <span class="st">"$N$"</span>, <span class="st">"$f_k$"</span>, <span class="st">"$p_k$"</span>, <span class="st">"$p_{k+1}$"</span>, <span class="st">"$k </span><span class="sc">\\</span><span class="st">frac{p_{k+1}}{p_k}$"</span> ),</span>
<span id="cb8-16"><a href="#cb8-16"></a>    <span class="at">align =</span> <span class="st">'llrrrrr'</span>,</span>
<span id="cb8-17"><a href="#cb8-17"></a>    <span class="at">digits =</span> <span class="fu">c</span>( <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">5</span> ),</span>
<span id="cb8-18"><a href="#cb8-18"></a>    <span class="at">format.args =</span> <span class="fu">list</span>( <span class="at">big.mark =</span> <span class="st">','</span>, <span class="at">decimal.mark =</span> <span class="st">'.'</span>, <span class="at">scientific =</span> <span class="cn">FALSE</span> ),</span>
<span id="cb8-19"><a href="#cb8-19"></a>    <span class="at">escape =</span> <span class="cn">FALSE</span>,</span>
<span id="cb8-20"><a href="#cb8-20"></a>    <span class="at">centering =</span> <span class="cn">TRUE</span>, </span>
<span id="cb8-21"><a href="#cb8-21"></a>    <span class="at">booktabs =</span> <span class="cn">TRUE</span> ) <span class="sc">%&gt;%</span></span>
<span id="cb8-22"><a href="#cb8-22"></a>  <span class="fu">kable_classic</span>( <span class="at">full_width =</span> <span class="cn">FALSE</span>, <span class="at">html_font =</span> <span class="st">"Cambria"</span>, <span class="at">position =</span> <span class="st">"center"</span> ) <span class="sc">%&gt;%</span></span>
<span id="cb8-23"><a href="#cb8-23"></a>  <span class="fu">scroll_box</span>( <span class="at">width =</span> <span class="st">"100%"</span>, <span class="at">height =</span> <span class="st">"500px"</span> )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div style="border: 1px solid #ddd; padding: 0px; overflow-y: scroll; height:500px; overflow-x: scroll; width:100%; ">
<table class="lightable-classic caption-top table" data-quarto-postprocess="true">
<caption>Estimación conteos por sexo</caption>
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th" style="text-align: left; position: sticky; top: 0; background-color: #FFFFFF;">sexo</th>
<th data-quarto-table-cell-role="th" style="text-align: left; position: sticky; top: 0; background-color: #FFFFFF;">fuel</th>
<th data-quarto-table-cell-role="th" style="text-align: right; position: sticky; top: 0; background-color: #FFFFFF;">$N$</th>
<th data-quarto-table-cell-role="th" style="text-align: right; position: sticky; top: 0; background-color: #FFFFFF;">$f_k$</th>
<th data-quarto-table-cell-role="th" style="text-align: right; position: sticky; top: 0; background-color: #FFFFFF;">$p_k$</th>
<th data-quarto-table-cell-role="th" style="text-align: right; position: sticky; top: 0; background-color: #FFFFFF;">$p_{k+1}$</th>
<th data-quarto-table-cell-role="th" style="text-align: right; position: sticky; top: 0; background-color: #FFFFFF;">$k \frac{p_{k+1}}{p_k}$</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">female</td>
<td style="text-align: left;">gasoline</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">29,533</td>
<td style="text-align: right;">0.88741</td>
<td style="text-align: right;">0.00000</td>
<td style="text-align: right;">0.00000</td>
</tr>
<tr class="even">
<td style="text-align: left;">female</td>
<td style="text-align: left;">gasoline</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">3,384</td>
<td style="text-align: right;">0.10168</td>
<td style="text-align: right;">0.88741</td>
<td style="text-align: right;">8.72725</td>
</tr>
<tr class="odd">
<td style="text-align: left;">female</td>
<td style="text-align: left;">gasoline</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">326</td>
<td style="text-align: right;">0.00980</td>
<td style="text-align: right;">0.10168</td>
<td style="text-align: right;">20.76074</td>
</tr>
<tr class="even">
<td style="text-align: left;">female</td>
<td style="text-align: left;">gasoline</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">33</td>
<td style="text-align: right;">0.00099</td>
<td style="text-align: right;">0.00980</td>
<td style="text-align: right;">29.63636</td>
</tr>
<tr class="odd">
<td style="text-align: left;">female</td>
<td style="text-align: left;">gasoline</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">0.00009</td>
<td style="text-align: right;">0.00099</td>
<td style="text-align: right;">44.00000</td>
</tr>
<tr class="even">
<td style="text-align: left;">female</td>
<td style="text-align: left;">gasoline</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0.00003</td>
<td style="text-align: right;">0.00009</td>
<td style="text-align: right;">15.00000</td>
</tr>
<tr class="odd">
<td style="text-align: left;">female</td>
<td style="text-align: left;">diesel</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">8,557</td>
<td style="text-align: right;">0.86539</td>
<td style="text-align: right;">0.00003</td>
<td style="text-align: right;">0.00000</td>
</tr>
<tr class="even">
<td style="text-align: left;">female</td>
<td style="text-align: left;">diesel</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">1,206</td>
<td style="text-align: right;">0.12197</td>
<td style="text-align: right;">0.86539</td>
<td style="text-align: right;">7.09536</td>
</tr>
<tr class="odd">
<td style="text-align: left;">female</td>
<td style="text-align: left;">diesel</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">109</td>
<td style="text-align: right;">0.01102</td>
<td style="text-align: right;">0.12197</td>
<td style="text-align: right;">22.12844</td>
</tr>
<tr class="even">
<td style="text-align: left;">female</td>
<td style="text-align: left;">diesel</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">14</td>
<td style="text-align: right;">0.00142</td>
<td style="text-align: right;">0.01102</td>
<td style="text-align: right;">23.35714</td>
</tr>
<tr class="odd">
<td style="text-align: left;">female</td>
<td style="text-align: left;">diesel</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">0.00020</td>
<td style="text-align: right;">0.00142</td>
<td style="text-align: right;">28.00000</td>
</tr>
<tr class="even">
<td style="text-align: left;">male</td>
<td style="text-align: left;">gasoline</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">71,357</td>
<td style="text-align: right;">0.89714</td>
<td style="text-align: right;">0.00020</td>
<td style="text-align: right;">0.00000</td>
</tr>
<tr class="odd">
<td style="text-align: left;">male</td>
<td style="text-align: left;">gasoline</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">7,417</td>
<td style="text-align: right;">0.09325</td>
<td style="text-align: right;">0.89714</td>
<td style="text-align: right;">9.62074</td>
</tr>
<tr class="even">
<td style="text-align: left;">male</td>
<td style="text-align: left;">gasoline</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">682</td>
<td style="text-align: right;">0.00857</td>
<td style="text-align: right;">0.09325</td>
<td style="text-align: right;">21.75073</td>
</tr>
<tr class="odd">
<td style="text-align: left;">male</td>
<td style="text-align: left;">gasoline</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">74</td>
<td style="text-align: right;">0.00093</td>
<td style="text-align: right;">0.00857</td>
<td style="text-align: right;">27.64865</td>
</tr>
<tr class="even">
<td style="text-align: left;">male</td>
<td style="text-align: left;">gasoline</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">7</td>
<td style="text-align: right;">0.00009</td>
<td style="text-align: right;">0.00093</td>
<td style="text-align: right;">42.28571</td>
</tr>
<tr class="odd">
<td style="text-align: left;">male</td>
<td style="text-align: left;">gasoline</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0.00001</td>
<td style="text-align: right;">0.00009</td>
<td style="text-align: right;">35.00000</td>
</tr>
<tr class="even">
<td style="text-align: left;">male</td>
<td style="text-align: left;">diesel</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">35,489</td>
<td style="text-align: right;">0.87614</td>
<td style="text-align: right;">0.00001</td>
<td style="text-align: right;">0.00000</td>
</tr>
<tr class="odd">
<td style="text-align: left;">male</td>
<td style="text-align: left;">diesel</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">4,532</td>
<td style="text-align: right;">0.11188</td>
<td style="text-align: right;">0.87614</td>
<td style="text-align: right;">7.83076</td>
</tr>
<tr class="even">
<td style="text-align: left;">male</td>
<td style="text-align: left;">diesel</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">439</td>
<td style="text-align: right;">0.01084</td>
<td style="text-align: right;">0.11188</td>
<td style="text-align: right;">20.64692</td>
</tr>
<tr class="odd">
<td style="text-align: left;">male</td>
<td style="text-align: left;">diesel</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">41</td>
<td style="text-align: right;">0.00101</td>
<td style="text-align: right;">0.01084</td>
<td style="text-align: right;">32.12195</td>
</tr>
<tr class="even">
<td style="text-align: left;">male</td>
<td style="text-align: left;">diesel</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">0.00012</td>
<td style="text-align: right;">0.00101</td>
<td style="text-align: right;">32.80000</td>
</tr>
</tbody>
</table>
</div>

<div class="cell" data-layout-align="center">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource r fold-hide number-lines code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1"></a>plt <span class="ot">&lt;-</span> <span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb9-2"><a href="#cb9-2"></a>  <span class="fu">geom_point</span>( <span class="at">data =</span> conteo, <span class="fu">aes</span>( <span class="at">x =</span> N, <span class="at">y =</span> jn ), <span class="at">colour =</span> <span class="st">'purple'</span> ) <span class="sc">+</span> </span>
<span id="cb9-3"><a href="#cb9-3"></a>  <span class="fu">xlab</span>( <span class="fu">TeX</span>( <span class="st">"$k$"</span> ) ) <span class="sc">+</span> </span>
<span id="cb9-4"><a href="#cb9-4"></a>  <span class="fu">ylab</span>( <span class="fu">TeX</span>( <span class="st">"$k </span><span class="sc">\\</span><span class="st">frac{p_k}{p_{k-1}}$"</span> ) ) <span class="sc">+</span> </span>
<span id="cb9-5"><a href="#cb9-5"></a>  <span class="fu">theme_bw</span>()</span>
<span id="cb9-6"><a href="#cb9-6"></a><span class="fu">plot</span>( plt )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="lectura_05_files/figure-html/l5g4-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
</div>
</div>
<p>Además, una forma sencilla de estimar si una variable sigue una distribución de las tres antes descritas es estudiando su coeficiente de variación <span class="math inline">\(\VC( N ) = \frac{\V[N]}{\E[N]}\)</span>.</p>
<ol type="1">
<li><p>Si <span class="math inline">\(N\)</span> sigue una ley de Poisson <span class="math inline">\(Pois(\lambda)\)</span>, entonces: <span class="math display">\[
\VC( N ) = \frac{\V[N]}{\E[N]} = \frac{\lambda}{\lambda} = 1
\]</span></p></li>
<li><p>Si <span class="math inline">\(N\)</span> sigue una ley de binomial negativa <span class="math inline">\(NBinom( \alpha, p )\)</span>, entonces: <span class="math display">\[
\VC( N ) = \frac{\V[N]}{\E[N]} = \frac{\alpha\frac{1-p}{p^2}}{\alpha\frac{1-p}{p}} = \frac{1}{p} &gt; 1
\]</span></p></li>
<li><p>Si <span class="math inline">\(N\)</span> sigue una ley de binomial <span class="math inline">\(Binom( n, p )\)</span>, entonces: <span class="math display">\[
\VC( N ) = \frac{\V[N]}{\E[N]} = \frac{np(1-p)}{np} = 1 - p &lt; 1
\]</span></p></li>
</ol>
<!------------------------------------------------------------------------------------------------->
</section>
<section id="distribuciones-continuas" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="distribuciones-continuas"><span class="header-section-number">6.3</span> Distribuciones continuas</h2>
<!------------------------------------------------------------------------------------------------->
<section id="distribución-uniforme" class="level3" data-number="6.3.1">
<h3 data-number="6.3.1" class="anchored" data-anchor-id="distribución-uniforme"><span class="header-section-number">6.3.1</span> Distribución uniforme</h3>
<div class="thmbox">
<div id="dunif" class="definition" name="Distribución uniforme">
<p>Una variable aleatoria <span class="math inline">\(X\)</span> a valores reales, sigue una distribución uniforme <span class="math inline">\(X \rightsquigarrow Unif( a, b )\)</span> de parámetros <span class="math inline">\(a, b \in \R\)</span>, si su función de distribución acumulada es de la siguiente forma: <span class="math display">\[
F_X( x ) = \frac{x-a}{b-a} \mathbf{1}_{[a,b)}( x ) + \mathbf{1}_{[b,+\infty)}( x )
\]</span> sin mucho esfuerzo se puede verificar que su densidad de probabilidad está dada por la función <span class="math display">\[
f_X( x ) = \frac{1}{b-a}\mathbf{1}_{[a,b]}( x )
\]</span> <span class="math display">\[
M_X( t ) = \frac{\exp(bt)-\exp(at)}{t(b-a)}
\]</span> <span class="math display">\[
\E[X] = \frac{a + b}{2},\qquad \V[X] = \frac{(b - a)^2}{12}
\]</span></p>
</div>
</div>
<div class="cell">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1"></a>a <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb10-2"><a href="#cb10-2"></a>b <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb10-3"><a href="#cb10-3"></a>x <span class="ot">&lt;-</span> <span class="fl">1.5</span></span>
<span id="cb10-4"><a href="#cb10-4"></a>m <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb10-5"><a href="#cb10-5"></a>X <span class="ot">&lt;-</span> <span class="fu">runif</span>( <span class="at">n =</span> m, <span class="at">min =</span> a, <span class="at">max =</span> b ) <span class="co"># simular una muestra de tamaño m</span></span>
<span id="cb10-6"><a href="#cb10-6"></a>fx <span class="ot">&lt;-</span> <span class="fu">dunif</span>( <span class="at">x =</span> x, <span class="at">min =</span> a, <span class="at">max =</span> b ) <span class="co"># cálculo de la densidad f(x)</span></span>
<span id="cb10-7"><a href="#cb10-7"></a>Fk <span class="ot">&lt;-</span> <span class="fu">punif</span>( <span class="at">q =</span> x, <span class="at">min =</span> a, <span class="at">max =</span> b ) <span class="co"># cálculo de probabilidad F(x)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<!------------------------------------------------------------------------------------------------->
</section>
<section id="distribución-exponencial" class="level3" data-number="6.3.2">
<h3 data-number="6.3.2" class="anchored" data-anchor-id="distribución-exponencial"><span class="header-section-number">6.3.2</span> Distribución exponencial</h3>
<div class="thmbox">
<div id="dexp" class="definition" name="Distribución exponencial">
<p>Una variable aleatoria <span class="math inline">\(X\)</span> a valores reales, sigue una distribución exponencial <span class="math inline">\(X \rightsquigarrow Exp( \lambda )\)</span> de parámetros <span class="math inline">\(\lambda &gt; 0\)</span>, si su función de distribución acumulada es de la siguiente forma: <span class="math display">\[
F_X( x ) = \mathbf{1}_{(0,+\infty)}( x ) \left( 1 - \exp\left( -\lambda x \right) \right)
\]</span> sin mucho esfuerzo se puede verificar que su densidad de probabilidad está dada por la función <span class="math display">\[
f_X( x ) = \mathbf{1}_{(0,+\infty)}( x ) \lambda \exp\left( -\lambda x \right)
\]</span> <span class="math display">\[
M_X( t ) = \frac{\lambda}{\lambda - t}
\]</span> <span class="math display">\[
\E[X] = \frac{1}{\lambda},\qquad \V[X] = \frac{1}{\lambda^2}
\]</span></p>
</div>
</div>
<div class="cell">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1"></a>lambda <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb11-2"><a href="#cb11-2"></a>x <span class="ot">&lt;-</span> <span class="fl">1.5</span></span>
<span id="cb11-3"><a href="#cb11-3"></a>m <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb11-4"><a href="#cb11-4"></a>X <span class="ot">&lt;-</span> <span class="fu">rexp</span>( <span class="at">n =</span> m, <span class="at">rate =</span> lambda ) <span class="co"># simular una muestra de tamaño m</span></span>
<span id="cb11-5"><a href="#cb11-5"></a>fx <span class="ot">&lt;-</span> <span class="fu">dexp</span>( <span class="at">x =</span> x, <span class="at">rate =</span> lambda ) <span class="co"># cálculo de la densidad f(x)</span></span>
<span id="cb11-6"><a href="#cb11-6"></a>Fk <span class="ot">&lt;-</span> <span class="fu">pexp</span>( <span class="at">q =</span> x, <span class="at">rate =</span> lambda ) <span class="co"># cálculo de probabilidad F(x)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<!------------------------------------------------------------------------------------------------->
</section>
<section id="distribución-gamma" class="level3" data-number="6.3.3">
<h3 data-number="6.3.3" class="anchored" data-anchor-id="distribución-gamma"><span class="header-section-number">6.3.3</span> Distribución gamma</h3>
<div class="thmbox">
<div id="dgamma" class="definition" name="Distribución gamma">
<p>Una variable aleatoria <span class="math inline">\(X\)</span> a valores reales, sigue una distribución gamma <span class="math inline">\(X \rightsquigarrow Gamma( \alpha, \beta )\)</span> de parámetros <span class="math inline">\(\alpha &gt; 0\)</span>, <span class="math inline">\(\beta &gt; 0\)</span>, si su función de distribución acumulada es de la siguiente forma: <span class="math display">\[
F_X( x ) = \frac{\beta^\alpha}{\Gamma( \alpha )} \int\limits_{0}^{x} u^{\alpha-1} \exp(-\beta u)\ du
\]</span> si en caso <span class="math inline">\(\alpha\)</span> un entero positivo, i.e.&nbsp;<span class="math inline">\(\alpha \in \N^*\)</span>, se puede calcular <span class="math inline">\(F_X( x )\)</span> con la siguiente serie <span class="math display">\[
F_X( x ) = 1 - \exp( -\lambda x ) \sum\limits_{n=0}^{\alpha-1} \frac{(\lambda x)^n}{n!}
= \exp( -\lambda x ) \sum\limits_{n=\alpha}^{+\infty} \frac{(\lambda x)^n}{n!}
\]</span></p>
<p>por su parte, la densidad de probabilidad automáticamente está dada por la función: <span class="math display">\[
f_X( x ) = \mathbf{1}_{[0,+\infty}( x ) \frac{\beta^\alpha}{\Gamma( \alpha )} x^{\alpha-1} \exp(-\beta x)
\]</span> <span class="math display">\[
M_X( t ) = \left( \frac{\beta}{\beta - t} \right)^\alpha,\qquad \text{si}\ t &lt; \beta
\]</span> <span class="math display">\[
\E[X] = \frac{\alpha}{\beta},\qquad
\V[X] = \frac{\alpha}{\beta^2}
\]</span></p>
</div>
</div>
<div class="cell">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1"></a>alpha <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb12-2"><a href="#cb12-2"></a>beta <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb12-3"><a href="#cb12-3"></a>x <span class="ot">&lt;-</span> <span class="dv">3</span></span>
<span id="cb12-4"><a href="#cb12-4"></a>m <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb12-5"><a href="#cb12-5"></a>X <span class="ot">&lt;-</span> <span class="fu">rgamma</span>( <span class="at">n =</span> m, <span class="at">shape =</span> alpha, <span class="at">scale =</span> beta ) <span class="co"># simular una muestra de tamaño m</span></span>
<span id="cb12-6"><a href="#cb12-6"></a>fx <span class="ot">&lt;-</span> <span class="fu">dgamma</span>( <span class="at">x =</span> x, <span class="at">shape =</span> alpha, <span class="at">scale =</span> beta ) <span class="co"># cálculo de la densidad f(x)</span></span>
<span id="cb12-7"><a href="#cb12-7"></a>Fk <span class="ot">&lt;-</span> <span class="fu">pgamma</span>( <span class="at">q =</span> x, <span class="at">shape =</span> alpha, <span class="at">scale =</span> beta ) <span class="co"># cálculo de probabilidad F(x)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<!------------------------------------------------------------------------------------------------->
</section>
<section id="distribución-normal" class="level3" data-number="6.3.4">
<h3 data-number="6.3.4" class="anchored" data-anchor-id="distribución-normal"><span class="header-section-number">6.3.4</span> Distribución normal</h3>
<div class="thmbox">
<div id="dnorm" class="definition" name="Distribución normal">
<p>Una variable aleatoria <span class="math inline">\(X\)</span> a valores reales, sigue una distribución normal <span class="math inline">\(X \rightsquigarrow N( \mu, \sigma )\)</span> de parámetros <span class="math inline">\(\mu \in \R\)</span>, <span class="math inline">\(\sigma &gt; 0\)</span>, si su función de distribución acumulada es de la siguiente forma: <span class="math display">\[
F_X( x ) = \frac{1}{\sqrt{2\pi} \sigma} \int\limits_{-\infty}^x \exp\left( -\frac{(y - \mu)^2}{\sigma^2} \right)\ dy
\]</span> la densidad de probabilidad automáticamente está dada por la función: <span class="math display">\[
f_X( x ) = \frac{1}{\sqrt{2\pi}} \exp\left( -\frac{(x - \mu)^2}{\sigma^2} \right)
\]</span> <span class="math display">\[
M_X( t ) = \exp\left( t \mu + \frac{1}{2} t^2 \sigma^2 \right)
\]</span> <span class="math display">\[
\E[X] = \mu,\qquad \V[X] = \sigma^2
\]</span></p>
</div>
</div>
<div class="cell">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1"></a>mu <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb13-2"><a href="#cb13-2"></a>sigma <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb13-3"><a href="#cb13-3"></a>x <span class="ot">&lt;-</span> <span class="dv">3</span></span>
<span id="cb13-4"><a href="#cb13-4"></a>m <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb13-5"><a href="#cb13-5"></a>X <span class="ot">&lt;-</span> <span class="fu">rnorm</span>( <span class="at">n =</span> m, <span class="at">mean =</span> mu, <span class="at">sd =</span> sigma ) <span class="co"># simular una muestra de tamaño m</span></span>
<span id="cb13-6"><a href="#cb13-6"></a>fx <span class="ot">&lt;-</span> <span class="fu">dnorm</span>( <span class="at">x =</span> x, <span class="at">mean =</span> mu, <span class="at">sd =</span> sigma ) <span class="co"># cálculo de la densidad f(x)</span></span>
<span id="cb13-7"><a href="#cb13-7"></a>Fk <span class="ot">&lt;-</span> <span class="fu">pnorm</span>( <span class="at">q =</span> x, <span class="at">mean =</span> mu, <span class="at">sd =</span> sigma ) <span class="co"># cálculo de probabilidad F(x)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<!------------------------------------------------------------------------------------------------->
</section>
<section id="distribución-log-normal" class="level3" data-number="6.3.5">
<h3 data-number="6.3.5" class="anchored" data-anchor-id="distribución-log-normal"><span class="header-section-number">6.3.5</span> Distribución log-normal</h3>
<div class="thmbox">
<div id="dlnorm" class="definition" name="Distribución log-normal">
<p>Una variable aleatoria <span class="math inline">\(X\)</span> a valores reales, sigue una distribución log-normal <span class="math inline">\(X \rightsquigarrow LN( \mu, \sigma )\)</span> de parámetros <span class="math inline">\(\mu &gt; 0\)</span>, <span class="math inline">\(\sigma &gt; 0\)</span>, si su función de distribución acumulada es de la siguiente forma: <span class="math display">\[
F_X( x ) = \frac{1}{\sqrt{2\pi} \sigma} \int\limits_{0}^{x} \frac{1}{y} \exp\left( -\frac{(\ln(y) - \mu)^2}{\sigma^2} \right)\ dy
\]</span> la densidad de probabilidad automáticamente está dada por la función: <span class="math display">\[
f_X( x ) = \frac{1}{x\sqrt{2\pi} \sigma}  \exp\left( -\frac{(\ln(x) - \mu)^2}{\sigma^2} \right)
\]</span> No hay forma analítica para <span class="math inline">\(M_X\)</span> <span class="math display">\[
\E[X] = \exp\left( \mu + \frac{1}{2}\sigma^2 \right),\qquad
\V[X] = \exp\left( 2 \mu + \sigma^2 \right) \left( \exp( \sigma^2 ) - 1 \right)
\]</span></p>
</div>
</div>
<div class="cell">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1"></a>mu <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb14-2"><a href="#cb14-2"></a>sigma <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb14-3"><a href="#cb14-3"></a>x <span class="ot">&lt;-</span> <span class="dv">3</span></span>
<span id="cb14-4"><a href="#cb14-4"></a>m <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb14-5"><a href="#cb14-5"></a>X <span class="ot">&lt;-</span> <span class="fu">rlnorm</span>( <span class="at">n =</span> m, <span class="at">meanlog =</span> mu, <span class="at">sdlog =</span> sigma ) <span class="co"># simular una muestra de tamaño m</span></span>
<span id="cb14-6"><a href="#cb14-6"></a>fx <span class="ot">&lt;-</span> <span class="fu">dlnorm</span>( <span class="at">x =</span> x, <span class="at">meanlog =</span> mu, <span class="at">sdlog =</span> sigma ) <span class="co"># cálculo de la densidad f(x)</span></span>
<span id="cb14-7"><a href="#cb14-7"></a>Fk <span class="ot">&lt;-</span> <span class="fu">plnorm</span>( <span class="at">q =</span> x, <span class="at">meanlog =</span> mu, <span class="at">sdlog =</span> sigma ) <span class="co"># cálculo de probabilidad F(x)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>En pocas, una variable aleatoria <span class="math inline">\(X \rightsquigarrow LN( \mu, \sigma )\)</span> sigue una distribución log-normal si y solamente si la variable aleatoria dada por su logaritmo <span class="math inline">\(\ln( X ) \rightsquigarrow N( \mu, \sigma )\)</span> sigue una distribución normal.</p>
<!------------------------------------------------------------------------------------------------->
</section>
<section id="distribución-de-beta" class="level3" data-number="6.3.6">
<h3 data-number="6.3.6" class="anchored" data-anchor-id="distribución-de-beta"><span class="header-section-number">6.3.6</span> Distribución de beta</h3>
<p>Una variable aleatoria <span class="math inline">\(X \in [0,1]\)</span> a valores reales, sigue una distribución beta <span class="math inline">\(X \rightsquigarrow Beta( \alpha, \beta )\)</span> de parámetros <span class="math inline">\(\alpha &gt; 0\)</span>, <span class="math inline">\(\beta &gt; 0\)</span>, si su función de distribución acumulada es de la siguiente forma: <span class="math display">\[
F_X( x ) = \frac{1}{B(\alpha,\beta)} \int\limits_{0}^x t^{\alpha-1} ( 1 - t )^{\beta - 1}\ dt
\]</span> la densidad de probabilidad automáticamente está dada por la función: <span class="math display">\[
f_X( x ) = \frac{x^{\alpha-1} ( 1 - x )^{\beta - 1}}{B(\alpha,\beta)}
\]</span> <span class="math display">\[
M_X( t ) = 1 + \sum\limits_{k=1}^{+\infty} \left( \prod\limits_{l=0}^{k-1} \frac{\alpha + l}{\alpha + \beta + l} \right) \frac{t^k}{k!}
\]</span> <span class="math display">\[
\E[X] = \frac{\alpha}{\alpha + \beta},\qquad
\V[X] = \frac{\alpha\beta}{(\alpha + \beta)^2(\alpha + \beta + 1)}
\]</span></p>
<div class="cell">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1"></a>alpha <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb15-2"><a href="#cb15-2"></a>beta <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb15-3"><a href="#cb15-3"></a>x <span class="ot">&lt;-</span> <span class="fl">0.5</span></span>
<span id="cb15-4"><a href="#cb15-4"></a>m <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb15-5"><a href="#cb15-5"></a>X <span class="ot">&lt;-</span> <span class="fu">rbeta</span>( <span class="at">n =</span> m, <span class="at">shape1 =</span> alpha, <span class="at">shape2 =</span> beta ) <span class="co"># simular una muestra de tamaño m</span></span>
<span id="cb15-6"><a href="#cb15-6"></a>fx <span class="ot">&lt;-</span> <span class="fu">dbeta</span>( <span class="at">x =</span> x, <span class="at">shape1 =</span> alpha, <span class="at">shape2 =</span> beta ) <span class="co"># cálculo de la densidad f(x)</span></span>
<span id="cb15-7"><a href="#cb15-7"></a>Fk <span class="ot">&lt;-</span> <span class="fu">pbeta</span>( <span class="at">q =</span> x, <span class="at">shape1 =</span> alpha, <span class="at">shape2 =</span> beta ) <span class="co"># cálculo de probabilidad F(x)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<!------------------------------------------------------------------------------------------------->
</section>
<section id="distribución-de-pareto-generalizada" class="level3" data-number="6.3.7">
<h3 data-number="6.3.7" class="anchored" data-anchor-id="distribución-de-pareto-generalizada"><span class="header-section-number">6.3.7</span> Distribución de Pareto generalizada</h3>
<div class="thmbox">
<div id="dgpd" class="definition" name="Distribución de Pareto generalizada">
<p>Una variable aleatoria <span class="math inline">\(X\)</span> a valores reales, sigue una distribución de Pareto generalizada <span class="math inline">\(X \rightsquigarrow GPD( \mu, \sigma, \xi )\)</span> de parámetros <span class="math inline">\(\mu \in \R, \sigma &gt; 0, \xi \in \R\)</span>, si su función de distribución acumulada es de la siguiente forma: <span class="math display">\[
F_X( x )
= \left \{
\begin{array}{ll}
1 - \left( 1 + \xi \frac{x-\mu}{\sigma} \right)^{-\frac{1}{\xi}} &amp; \text{si}\ \xi \neq 0 \\
1 - \exp\left( -\frac{x-\mu}{\sigma} \right) &amp; \text{si}\ \xi = 0
\end{array}
\right.
\]</span> y su densidad de probabilidad está dada por la función <span class="math display">\[
f_X( x )
= \left \{
\begin{array}{ll}
\frac{1}{\sigma} \left( 1 + \xi \frac{x-\mu}{\sigma} \right)^{-1-\frac{1}{\xi}} &amp; \text{si}\ \xi \neq 0 \\
\frac{1}{\sigma} \exp\left( -\frac{x-\mu}{\sigma} \right) &amp; \text{si}\ \xi = 0
\end{array}
\right.
\]</span> <span class="math display">\[
M_X( t ) = \exp(\theta \mu) \sum\limits_{j=0}^{+\infty} \frac{\theta^j \sigma^j}
{\prod\limits_{k=0}^j ( 1 - k \xi )}
\]</span></p>
</div>
</div>
<div class="cell">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1"></a>xi <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb16-2"><a href="#cb16-2"></a>mu <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb16-3"><a href="#cb16-3"></a>sigma <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb16-4"><a href="#cb16-4"></a>x <span class="ot">&lt;-</span> <span class="dv">3</span></span>
<span id="cb16-5"><a href="#cb16-5"></a>m <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb16-6"><a href="#cb16-6"></a>X <span class="ot">&lt;-</span> <span class="fu">rgpd</span>( <span class="at">n =</span> m, <span class="at">xi =</span> xi, <span class="at">mu =</span> mu, <span class="at">beta =</span> sigma ) <span class="co"># simular una muestra de tamaño m</span></span>
<span id="cb16-7"><a href="#cb16-7"></a>fx <span class="ot">&lt;-</span> <span class="fu">dgpd</span>( <span class="at">x =</span> x, <span class="at">xi =</span> xi, <span class="at">mu =</span> mu, <span class="at">beta =</span> sigma ) <span class="co"># cálculo de la densidad f(x)</span></span>
<span id="cb16-8"><a href="#cb16-8"></a>Fk <span class="ot">&lt;-</span> <span class="fu">pgpd</span>( <span class="at">q =</span> x, <span class="at">xi =</span> xi, <span class="at">mu =</span> mu, <span class="at">beta =</span> sigma ) <span class="co"># cálculo de probabilidad F(x)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<!------------------------------------------------------------------------------------------------->
</section>
<section id="distribución-de-valores-extremos-generalizada" class="level3" data-number="6.3.8">
<h3 data-number="6.3.8" class="anchored" data-anchor-id="distribución-de-valores-extremos-generalizada"><span class="header-section-number">6.3.8</span> Distribución de valores extremos generalizada</h3>
<div class="thmbox">
<div id="dgev" class="definition" name="Distribución de valores extremos generalizada">
<p>Una variable aleatoria <span class="math inline">\(X\)</span> a valores reales, sigue una distribución generalizada de valores extremos <span class="math inline">\(X \rightsquigarrow GEV( \mu, \sigma, \xi )\)</span> de parámetros <span class="math inline">\(\mu \in \R, \sigma &gt; 0, \xi \in \R\)</span>, si su función de distribución acumulada es de la siguiente forma: <span class="math display">\[
F_X( x )
= \left\{
\begin{array}{ll}
\exp\left( -\exp\left( -\frac{x-\mu}{\sigma} \right) \right)
&amp; \text{si}\ \xi = 0 \\
\exp\left( -\left( 1 + \xi \frac{x-\mu}{\sigma} \right)^{-\frac{1}{\xi}} \right)
&amp; \text{si}\ \xi \neq 0, 1 + \xi\frac{x - \mu}{\sigma} &gt; 0
\end{array}
\right.
\]</span> además se puede verificar que su densidad de probabilidad está dada por la función <span class="math display">\[
f_X( x )
= \left\{
\begin{array}{ll}
\exp\left(-\frac{x-\mu}{\sigma}\right)
\exp\left(-\exp\left(-\frac{x-\mu}{\sigma}\right)\right)
&amp; \text{si}\ \xi = 0 \\
\left( 1 + \xi \frac{x - \mu}{\sigma}\right)^{-1-\frac{1}{\xi}}
\exp\left( -\left( 1 + \xi \frac{x-\mu}{\sigma} \right)^{-\frac{1}{\xi}} \right)
&amp; \text{si}\ \xi \neq 0, 1 + \xi\frac{x - \mu}{\sigma} &gt; 0
\end{array}
\right.
\]</span></p>
</div>
</div>
<div class="cell">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1"></a>xi <span class="ot">&lt;-</span> <span class="sc">-</span><span class="dv">1</span></span>
<span id="cb17-2"><a href="#cb17-2"></a>mu <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb17-3"><a href="#cb17-3"></a>sigma <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb17-4"><a href="#cb17-4"></a>x <span class="ot">&lt;-</span> <span class="dv">3</span></span>
<span id="cb17-5"><a href="#cb17-5"></a>m <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb17-6"><a href="#cb17-6"></a>X <span class="ot">&lt;-</span> <span class="fu">rgev</span>( <span class="at">n =</span> m, <span class="at">xi =</span> xi, <span class="at">mu =</span> mu, <span class="at">beta =</span> sigma ) <span class="co"># simular una muestra de tamaño m</span></span>
<span id="cb17-7"><a href="#cb17-7"></a>fx <span class="ot">&lt;-</span> <span class="fu">dgev</span>( <span class="at">x =</span> x, <span class="at">xi =</span> xi, <span class="at">mu =</span> mu, <span class="at">beta =</span> sigma ) <span class="co"># cálculo de la densidad f(x)</span></span>
<span id="cb17-8"><a href="#cb17-8"></a>Fk <span class="ot">&lt;-</span> <span class="fu">pgev</span>( <span class="at">q =</span> x, <span class="at">xi =</span> xi, <span class="at">mu =</span> mu, <span class="at">beta =</span> sigma ) <span class="co"># cálculo de probabilidad F(x)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<!------------------------------------------------------------------------------------------------->
</section>
<section id="distribución-t-de-student" class="level3" data-number="6.3.9">
<h3 data-number="6.3.9" class="anchored" data-anchor-id="distribución-t-de-student"><span class="header-section-number">6.3.9</span> Distribución t de Student</h3>
<div class="thmbox">
<div id="dtst" class="definition" name="Distribución t de Student">
<p>Una variable aleatoria <span class="math inline">\(X\)</span> a valores reales, sigue una distribución t de Student <span class="math inline">\(X \rightsquigarrow t( \nu )\)</span> de parámetros <span class="math inline">\(\nu &gt; 0\)</span>, si su función de distribución acumulada es de la siguiente forma: <span class="math display">\[
F_X( x )
= \frac{1}{2} + \frac{x}{\sqrt{\pi \nu}}
\frac{\Gamma\left( \frac{\nu + 1}{2} \right)}{\Gamma\left( \frac{\nu}{2} \right)}
F\left( \frac{1}{2}, \frac{\nu+1}{2}, \frac{3}{2}, -\frac{x^2}{\nu} \right)
\]</span> donde <span class="math inline">\(F\)</span> es la función hipergeométrica. <span class="math display">\[
F( a, b, c, z ) = \sum\limits_{n=0}^{+\infty} \frac{(a)_n (b)_n}{(c)_n} \frac{z^n}{n!}
\]</span> con <span class="math display">\[
(a)_n
= \left\{
\begin{array}{ll}
1 &amp; n = 0 \\
a( a + 1 ) \cdots (a + n - 1) &amp; n &gt; 0
\end{array}
\right.
\]</span> Además, se puede verificar que su densidad de probabilidad está dada por la función <span class="math display">\[
f_X( x )
= \frac{x}{\sqrt{\pi \nu}}
\frac{\Gamma\left(\frac{\nu+1}{2}\right)}{\Gamma\left( \frac{\nu}{2} \right)}
\left( 1 + \frac{x^2}{\nu} \right)^{-\frac{\nu+1}{2}}
\]</span> La función generadora de momentos <span class="math inline">\(M_X( t )\)</span> no está definida <span class="math display">\[
\E[X]
= \left\{
\begin{array}{ll}
0 &amp; \text{si}\ \nu &gt; 0 \\
\text{no definida} &amp; \text{si}\ \nu \leq 0
\end{array}
\right.
\]</span></p>
<p><span class="math display">\[
\V[X]
= \left\{
\begin{array}{ll}
\frac{\nu}{\nu-2} &amp; \text{si}\ \nu &gt; 2 \\
+\infty &amp; \text{si}\ 1 &lt; \nu \leq 2 \\
\text{no definida} &amp; \text{si}\ \nu \leq 1
\end{array}
\right.
\]</span></p>
</div>
</div>
<div class="cell">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1"></a>nu <span class="ot">&lt;-</span> <span class="dv">3</span></span>
<span id="cb18-2"><a href="#cb18-2"></a>x <span class="ot">&lt;-</span> <span class="dv">4</span></span>
<span id="cb18-3"><a href="#cb18-3"></a>m <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb18-4"><a href="#cb18-4"></a>X <span class="ot">&lt;-</span> <span class="fu">rt</span>( <span class="at">n =</span> m, <span class="at">df =</span> nu ) <span class="co"># simular una muestra de tamaño m</span></span>
<span id="cb18-5"><a href="#cb18-5"></a>fx <span class="ot">&lt;-</span> <span class="fu">dt</span>( <span class="at">x =</span> x, <span class="at">df =</span> nu ) <span class="co"># cálculo de la densidad f(x)</span></span>
<span id="cb18-6"><a href="#cb18-6"></a>Fk <span class="ot">&lt;-</span> <span class="fu">pt</span>( <span class="at">q =</span> x, <span class="at">df =</span> nu ) <span class="co"># cálculo de probabilidad F(x)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<!------------------------------------------------------------------------------------------------->
</section>
<section id="distribución-gamma-transformada" class="level3" data-number="6.3.10">
<h3 data-number="6.3.10" class="anchored" data-anchor-id="distribución-gamma-transformada"><span class="header-section-number">6.3.10</span> Distribución gamma transformada</h3>
<div class="thmbox">
<div id="dgammatra" class="definition" name="Distribución gamma transformada">
<p>Una variable aleatoria <span class="math inline">\(X\)</span> a valores reales, sigue una distribución gamma transformada <span class="math inline">\(X \rightsquigarrow GT( \alpha, \tau, \theta )\)</span> de parámetros <span class="math inline">\(\alpha &gt; 0, \tau &gt; 0, \theta &gt; 0\)</span>, si su función de distribución acumulada es de la siguiente forma: <span class="math display">\[
F_X( x ) = \frac{\tau}{\Gamma( \alpha )}
\int\limits_{0}^x
\frac{1}{u} \left( \frac{u}{\theta} \right)^{\alpha} \exp\left(-\left( \frac{u}{\theta} \right)^{\tau}\right)\ du
\]</span></p>
<p>además se puede verificar que su densidad de probabilidad está dada por la función: <span class="math display">\[
f_X( x )
=
\left\{
\begin{array}{ll}
0 &amp; \text{si}\ x \leq 0 \\
\frac{\tau}{x \Gamma( \alpha )} \left( \frac{x}{\theta} \right)^{\alpha} \exp\left(-\left( \frac{x}{\theta} \right)^{\tau}\right) &amp; \text{si}\ x &gt; 0
\end{array}
\right.
\]</span> <span class="math display">\[
\begin{split}
\E[X^k]
&amp; = \frac{\theta^k \Gamma\left( \alpha + \frac{k}{\tau}\right)}{\Gamma( \alpha )},
\quad \text{si}\ k &gt; -\alpha \tau \\
\E[X]
&amp; = \frac{\theta \Gamma\left( \alpha + \frac{1}{\tau}\right)}{\Gamma( \alpha )},
\quad \text{si}\ 1 &gt; -\alpha \tau \\
\V[X]
&amp; = \frac{\theta^2 \Gamma\left( \alpha + \frac{2}{\tau}\right)}{\Gamma( \alpha )}
- \frac{\theta^2 \Gamma\left( \alpha + \frac{1}{\tau}\right)^2}{\Gamma( \alpha )^2}
\end{split}
\]</span></p>
</div>
</div>
<div class="cell">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1"></a>alpha <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb19-2"><a href="#cb19-2"></a>tau <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb19-3"><a href="#cb19-3"></a>theta <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb19-4"><a href="#cb19-4"></a>x <span class="ot">&lt;-</span> <span class="dv">3</span></span>
<span id="cb19-5"><a href="#cb19-5"></a>m <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb19-6"><a href="#cb19-6"></a>X <span class="ot">&lt;-</span> <span class="fu">rtrgamma</span>( <span class="at">n =</span> m, <span class="at">shape1 =</span> alpha, <span class="at">shape2 =</span> tau, <span class="at">scale =</span> theta ) <span class="co"># simular una muestra de tamaño m</span></span>
<span id="cb19-7"><a href="#cb19-7"></a>fx <span class="ot">&lt;-</span> <span class="fu">dtrgamma</span>( <span class="at">x =</span> x, <span class="at">shape1 =</span> alpha, <span class="at">shape2 =</span> tau, <span class="at">scale =</span> theta ) <span class="co"># cálculo de la densidad f(x)</span></span>
<span id="cb19-8"><a href="#cb19-8"></a>Fk <span class="ot">&lt;-</span> <span class="fu">ptrgamma</span>( <span class="at">q =</span> x, <span class="at">shape1 =</span> alpha, <span class="at">shape2 =</span> tau, <span class="at">scale =</span> theta ) <span class="co"># cálculo de probabilidad F(x)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>En la familia gamma se incluyen las siguientes distribuciones:</p>
<ol type="1">
<li><p>La distribución inversa gamma transformada, es decir es una familia estable por inversión</p></li>
<li><p>La distribución gamma para <span class="math inline">\(\alpha = n/2\)</span> y <span class="math inline">\(\theta = 2\)</span></p></li>
<li><p>La distribución inversa gamma</p></li>
<li><p>La distribución de Weibull</p></li>
<li><p>La distribución inversa de Weibull</p></li>
<li><p>La distribución exponencial</p></li>
<li><p>La distribución inversa exponencial</p></li>
</ol>
<!------------------------------------------------------------------------------------------------->
</section>
<section id="distribución-beta-transformada" class="level3" data-number="6.3.11">
<h3 data-number="6.3.11" class="anchored" data-anchor-id="distribución-beta-transformada"><span class="header-section-number">6.3.11</span> Distribución beta transformada</h3>
<div class="thmbox">
<div id="dbetra" class="definition" name="Distribución beta transformada">
<p>Una variable aleatoria <span class="math inline">\(X\)</span> a valores reales, sigue una distribución beta transformada <span class="math inline">\(X \rightsquigarrow BT( \alpha, \gamma, \tau, \theta )\)</span> de parámetros <span class="math inline">\(\alpha &gt; 0, \gamma &gt; 0, \tau &gt; 0, \theta &gt; 0\)</span>, si su función de distribución acumulada es de la siguiente forma: <span class="math display">\[
F_X( x ) = \frac{\Gamma(\alpha + \tau)}{\Gamma( \alpha ) \Gamma( \tau )}
\int\limits_0^x
\frac{\gamma \left( \frac{u}{\theta} \right)^{\gamma \tau}}{u\left( 1 + \left( \frac{u}{\theta} \right)^{\gamma}\right)^{\alpha + \tau}}\ du
\]</span></p>
<p>además se puede verificar que su densidad de probabilidad está dada por la función: <span class="math display">\[
f_X( x )
= \mathbf{1}_{[0,+\infty)}( x )
\frac{\Gamma(\alpha + \tau)}{\Gamma( \alpha ) \Gamma( \tau )}
\frac{ \gamma \left( \frac{x}{\theta} \right)^{\gamma \tau}}{x\left( 1 + \left( \frac{x}{\theta} \right)^{\gamma}\right)^{\alpha + \tau}}
\]</span></p>
<p><span class="math display">\[
\begin{split}
\E[X^k]
&amp; = \frac{\theta^k \Gamma\left( \tau + \frac{k}{\gamma}\right) \Gamma\left( \tau - \frac{k}{\gamma}\right)}{\Gamma( \alpha ) \Gamma( \tau )},
\quad \text{si}\ -\tau \gamma &lt; k &lt; \tau \gamma \\
\E[X]
&amp; = \frac{\theta \Gamma\left( \tau + \frac{1}{\gamma}\right) \Gamma\left( \tau - \frac{1}{\gamma}\right)}{\Gamma( \alpha ) \Gamma( \tau )} \\
\V[X]
&amp; = \frac{\theta^2 \Gamma\left( \tau + \frac{2}{\gamma}\right) \Gamma\left( \tau - \frac{2}{\gamma}\right)}{\Gamma( \alpha ) \Gamma( \tau )}
- \frac{\theta^2 \Gamma\left( \tau + \frac{1}{\gamma}\right)^2 \Gamma\left( \tau - \frac{1}{\gamma}\right)^2}{\Gamma( \alpha )^2 \Gamma( \tau )^2}
\end{split}
\]</span></p>
</div>
</div>
<div class="cell">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1"></a>alpha <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb20-2"><a href="#cb20-2"></a>gamma <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb20-3"><a href="#cb20-3"></a>tau <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb20-4"><a href="#cb20-4"></a>theta <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb20-5"><a href="#cb20-5"></a>x <span class="ot">&lt;-</span> <span class="dv">3</span></span>
<span id="cb20-6"><a href="#cb20-6"></a>m <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb20-7"><a href="#cb20-7"></a>X <span class="ot">&lt;-</span> <span class="fu">rtrbeta</span>( <span class="at">n =</span> m, <span class="at">shape1 =</span> alpha, <span class="at">shape2 =</span> gamma, <span class="at">shape3 =</span> tau, <span class="at">scale =</span> theta ) <span class="co"># simular una muestra de tamaño m</span></span>
<span id="cb20-8"><a href="#cb20-8"></a>fx <span class="ot">&lt;-</span> <span class="fu">dtrbeta</span>( <span class="at">x =</span> x, <span class="at">shape1 =</span> alpha, <span class="at">shape2 =</span> gamma, <span class="at">shape3 =</span> tau, <span class="at">scale =</span> theta ) <span class="co"># cálculo de la densidad f(x)</span></span>
<span id="cb20-9"><a href="#cb20-9"></a>Fk <span class="ot">&lt;-</span> <span class="fu">ptrbeta</span>( <span class="at">q =</span> x, <span class="at">shape1 =</span> alpha, <span class="at">shape2 =</span> gamma, <span class="at">shape3 =</span> tau, <span class="at">scale =</span> theta ) <span class="co"># cálculo de probabilidad F(x)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Dentro de la familia beta transformada se cuenta algunas distribuciones de probabilidad:</p>
<ol type="1">
<li><p>La distribución de Burr para <span class="math inline">\(\tau = 1\)</span></p></li>
<li><p>La distribución de log-logística para <span class="math inline">\(\alpha = \tau = 1\)</span></p></li>
<li><p>La distribución de paralogística para <span class="math inline">\(\alpha = \gamma, \tau = 1\)</span></p></li>
<li><p>La distribución de generalizada de Pareto para <span class="math inline">\(\gamma = 1\)</span></p></li>
<li><p>La distribución de Pareto para <span class="math inline">\(\gamma = \tau = 1\)</span></p></li>
<li><p>La distribución de inversa de Burr para <span class="math inline">\(\alpha = 1\)</span></p></li>
<li><p>La distribución de inversa de Pareto para <span class="math inline">\(\alpha = \gamma = 1\)</span></p></li>
<li><p>La distribución de inversa paralogística para <span class="math inline">\(\alpha = 1, \gamma = \tau\)</span></p></li>
</ol>
<p>La distribución transformada gamma es un caso límite de la distribución transformada beta, cuando <span class="math inline">\(\theta \rightarrow +\infty, \alpha \rightarrow +\infty\)</span> y <span class="math inline">\(\theta \alpha^{-\frac{1}{\gamma}} \rightarrow \xi\)</span></p>
<!------------------------------------------------------------------------------------------------->
</section>
</section>
<section id="estimación" class="level2" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="estimación"><span class="header-section-number">6.4</span> Estimación</h2>
<p>En la práctica se observa la realización de una variable aleatoria <span class="math inline">\(X\)</span>, es decir se tiene una muestra de la misma <span class="math inline">\(X_1, \ldots, X_n\)</span>. Pero, no se dispone de la distribución <span class="math inline">\(F\)</span> o de la densidad <span class="math inline">\(f\)</span> que la describe. Como ya hemos mencionado, conociendo la distribución se puede inferir algunas propiedades sobre la variable. De ahí surge la necesidad de buscar la mejor distribución <span class="math inline">\(F\)</span> posible a partir de la muestra <span class="citation" data-cites="est_math_bor">[<a href="#ref-est_math_bor" role="doc-biblioref">2</a>]</span>, <span class="citation" data-cites="KNight">[<a href="#ref-KNight" role="doc-biblioref">5</a>]</span>, <span class="citation" data-cites="mathstat1">[<a href="#ref-mathstat1" role="doc-biblioref">1</a>]</span>, <span class="citation" data-cites="funmathstat">[<a href="#ref-funmathstat" role="doc-biblioref">4</a>]</span>.</p>
<p>La estimación usualmente consiste en tratar de determinar la probabilidad <span class="math inline">\(P_X\)</span> asociada a <span class="math inline">\(X\)</span>, o en su defecto su función de distribución acumulada. Bajo la consideración, usualmente, de alguna información adicional, se considera que para la medida de de probabilidad <span class="math inline">\(P_X\)</span> que caracteriza a la variable aleatoria <span class="math inline">\(X\)</span> está dentro de una familia probabilidades que dependen de un parámetro <span class="math inline">\(\theta\)</span> el cual pertenece a un conjunto <span class="math inline">\(\Theta\)</span>, precisamente parametriza a la familia, es decir, para cada <span class="math inline">\(\theta \in \Theta\)</span>, <span class="math inline">\(P_{\theta}\)</span> es una medida de probabilidad y para algún <span class="math inline">\(\theta \in \Theta, P_X = P_{\theta}\)</span>.</p>
<p>Ya en la práctica se tiene una muestra, por lo regular finita, <span class="math inline">\(X_1, \ldots, X_n\)</span> de la variable aleatoria <span class="math inline">\(X\)</span> que se desea comprender. Es a partir de la muestra que se intenta crear un estimador <span class="math inline">\(\tau\)</span> el cual sea precisamente el “mejor” bajo un cierto criterio que aproxime a la probabilidad <span class="math inline">\(P_X\)</span>, en términos algo más matemáticos <span class="math inline">\(P_X \approx P_{\tau}\)</span>. Por lo regular, se propone un estimador <span class="math inline">\(\tau\)</span> como función de la muestra y su tamaño <span class="math inline">\(\tau_n( X_1, \ldots, X_n ) = \tau( n, X_1, \ldots, X_n )\)</span>, en muchos casos se considera construir una función medible, que precisamente genere eventos observables. Es de notar que el estimador <span class="math inline">\(\tau_n\)</span> al depender de una muestra, la cual está constituida por variables aleatorias, como función de variables aleatorias se convierte también en una variable aleatoria al ser evaluada en estas.</p>
<p>Hay algunas propiedades deseables para una familia de estimadores <span class="math inline">\(\mathcal{E}\)</span>, estas son:</p>
<ol type="1">
<li><p>Consistencia</p></li>
<li><p>Insesgamiento</p></li>
<li><p>Eficiencia</p></li>
<li><p>Suficiencia</p></li>
</ol>
<p>Para, cada adjuntamos sus respectivas definiciones</p>
<div class="thmbox">
<div id="destconsis" class="definition" name="Estimadores consistentes">
<p>Consideramos una variable aleatoria <span class="math inline">\(X\)</span>, cuya medida de probabilidad asociada <span class="math inline">\(P_X\)</span> pertenece a una familia de distribuciones de probabilidad <span class="math inline">\(\{P_\theta\}_{\theta \in \Theta}\)</span>, esto quiere decir que <span class="math inline">\(P_X\)</span> está determinada por algún <span class="math inline">\(\theta \in \Theta\)</span>, i.e.&nbsp;<span class="math inline">\(P_X = P_{\theta}\)</span>. Entonces, la familia de estimadores <span class="math inline">\(\mathcal{E} = \{ \tau_n \}_n\)</span> es <strong>consistente</strong> para la familia de probabilidades <span class="math inline">\(\{P_\theta\}_{\theta \in \Theta}\)</span> si para cualquier <span class="math inline">\(n \in \N\)</span> y muestra <span class="math inline">\(X_1, \ldots, X_n\)</span> de <span class="math inline">\(X\)</span> el estimador <span class="math inline">\(\tau_n( X_1,\ldots,X_n)\)</span> converge en probabilidad a <span class="math inline">\(\theta\)</span>. Para todo <span class="math inline">\(\varepsilon &gt; 0\)</span> <span class="math display">\[
\underset{n \rightarrow +\infty}{\lim} P_{\theta}\left( \left| \tau_n( X_1,\ldots,X_n) - \theta \right| &gt; \varepsilon\right) = 0
\]</span> también se dice que la familia de estimadores es convergente.</p>
<p>Decimos que <strong>converge fuertemente</strong> si la convergencia de la familia de estimadores <span class="math inline">\(\mathcal{E} = \{ \tau_n \}_n\)</span> al parámetro <span class="math inline">\(\theta\)</span> se da <strong>casi seguramente</strong> o en <strong>casi todas partes</strong>. Esto en término de límites implica que: <span class="math display">\[
P_{\theta}\left( \underset{n \rightarrow +\infty}{\lim}
\tau_n\left( X_1, \ldots, X_n \right) = \theta \right) = 1
\]</span> En otras palabras, la probabilidad de que el límite del estimador <span class="math inline">\(\tau_n( X_1, \ldots, X_n )\)</span> sea igual a <span class="math inline">\(\theta\)</span> es <span class="math inline">\(1\)</span>, salvo un conjunto de medida nula para <span class="math inline">\(P_{\theta}\)</span>. De ahí resulta la terminología de convergencia casi segura o convergencia en casi todas partes.</p>
</div>
</div>
<div class="thmbox">
<div id="destinsesg" class="definition" name="Estimadores insesgados">
<p>Bajo el mismo contexto de la definición anterior. Decimos que el estimador <span class="math inline">\(\tau_n\)</span> es insesgado si precisamente su esperanza es igual al parámetro a estimar <span class="math inline">\(\theta \in \Theta\)</span>. <span class="math display">\[
\E\left[ \tau_n( X_1,\ldots,X_n) \right] = \theta
\]</span> La familia de estimadores <span class="math inline">\(\mathcal{E} = \{ \tau_n \}_n\)</span> será insesgada si para cada <span class="math inline">\(n\)</span> se satisface lo anterior.</p>
</div>
</div>
<p>El siguiente teorema es de importancia para caracterizar una familia de estimadores consistentes.</p>
<div class="thmbox">
<div id="sufestconsis" class="theorem">
<p>Una familia de estimadores <span class="math inline">\(\{ \tau_n \}_n\)</span> para la cual se cumplen los siguientes límites</p>
<ol type="1">
<li><p><span class="math inline">\(\E\left[ \tau_n( X_1,\ldots,X_n) \right] \rightarrow \theta\)</span>, conforme <span class="math inline">\(n \rightarrow +\infty\)</span>,</p></li>
<li><p><span class="math inline">\(\V\left[ \tau_n( X_1,\ldots,X_n) \right] \rightarrow 0\)</span>, conforme <span class="math inline">\(n \rightarrow +\infty\)</span>.</p></li>
</ol>
<p>Entonces, la familia <span class="math inline">\(\{ \tau_n \}_n\)</span> es consistente como estimadores de <span class="math inline">\(\theta\)</span>.</p>
</div>
</div>
<div class="thmbox">
<div id="destefi" class="definition" name="Estimador más eficiente">
<p>De igual forma en el contexto anterior. En una familia de estimadores consistentes para estimar un parámetro, si entre estos estimadores existe uno para el cual su varianza sea mínima, entonces si existe tal estimador se dice que este es el <strong>estimador más eficiente</strong>. Sin <span class="math inline">\(\mathcal{E}\)</span> es la familia de estimadores y <span class="math inline">\(\xi\)</span> es el estimador más eficiente, entonces para cualquier otro estimador <span class="math inline">\(\tau \in \mathcal{E}\)</span>, se tiene la siguiente desigualdad <span class="math display">\[
\V\left[ \xi \right] \leq \V\left[ \tau \right]
\]</span> la <strong>eficiencia</strong> <span class="math inline">\(E\)</span> de los estimadores en la familia <span class="math inline">\(\mathcal{E}\)</span> está dada por <span class="math display">\[
0 \leq E( \tau ) = \frac{\V\left[ \xi \right]}{\V\left[ \tau \right]} \leq 1
\]</span> Si además el estimador más eficiente <span class="math inline">\(\xi\)</span> es insesgado se dirá que este es un <strong>estimador de varianza mínima insesgado</strong>, o de forma corta <strong>MVUE</strong> por el término en inglés (minimum variance unbiased estimator).</p>
</div>
</div>
<div class="thmbox">
<div id="thmvueunicity" class="theorem">
<p>Si en una familia de estimadores <span class="math inline">\(\mathcal{E}\)</span> tenemos dos estimadores MVUE para el mismo parámetro <span class="math inline">\(\theta \in \Theta\)</span>, entonces estos dos estimadores son iguales en casi todas partes o lo que es lo mismo solo son diferentes en un conjunto de medida nula.</p>
</div>
</div>
<div class="thmbox">
<div id="destsufic" class="definition" name="Estimador suficiente">
<p>Dada una muestra <span class="math inline">\(X_1, \ldots, X_n\)</span> de la variable aleatoria <span class="math inline">\(X\)</span>. Un estimador <span class="math inline">\(\tau\left( X_1, \ldots, X_n \right)\)</span> se dice un <strong>estimador suficiente</strong> para el parámetro <span class="math inline">\(\theta \in \Theta\)</span>, si la distribución condicionada de la muestra <span class="math inline">\(X_1, \ldots X_n\)</span> dado el estimador <span class="math inline">\(\tau\)</span> es independiente del parámetro <span class="math inline">\(\theta\)</span>.</p>
</div>
</div>
<p>El resultado a continuación es una caracterización del criterio de suficiencia, para una demostración se puede consultar <span class="citation" data-cites="stat_theo_infer">[<a href="#ref-stat_theo_infer" role="doc-biblioref">7</a>]</span>, <span class="citation" data-cites="funmathstat">[<a href="#ref-funmathstat" role="doc-biblioref">4</a>]</span>.</p>
<div class="thmbox">
<div id="thfactorsufic" class="theorem" name="Teorema de factorización">
<p>Un estimador <span class="math inline">\(\tau\)</span> es suficiente para el parámetro <span class="math inline">\(\theta\)</span> si y solo si la densidad conjunta de la muestra <span class="math inline">\(X_1, \ldots, X_n\)</span> puede ser expresada en la forma. <span class="math display">\[
f\left( x_1, \ldots, x_n \right)
= g\left( \theta, \tau\left( x_1, \ldots, x_n \right) \right) h\left( x_1, \ldots, x_n \right)
\]</span></p>
</div>
</div>
<div class="thmbox">
<div id="dfamicomp" class="definition" name="Familia completa">
<p>Data un estimador <span class="math inline">\(\tau\)</span> que depende de una muestra aleatoria <span class="math inline">\(X_1, \ldots, X_n\)</span> y una familia de medidas de probabilidad <span class="math inline">\(\{ P_\theta \}_{\theta \in \Theta}\)</span>, para cada <span class="math inline">\(\theta \in \Theta\)</span>, se puede construir la medida <span class="math inline">\(P_{\tau,\theta}\)</span> asociada a la variable aleatoria <span class="math inline">\(\tau(X_1, \ldots,X_n)\)</span> y al parámetro <span class="math inline">\(\theta\)</span> que determina la medida <span class="math inline">\(P_\theta\)</span>. Para cualquier evento <span class="math inline">\(A \in \F\)</span>. <span class="math display">\[
P_{\tau,\theta}( A ) = P_\theta\left( \tau\left(X_1, \ldots,X_n\right) \in A \right)
\]</span></p>
<p>Así, la familia de medidas de probabilidad <span class="math inline">\(\{ P_{\tau,\theta} \}_{\theta \in \Theta}\)</span> se dice <strong>completa</strong> si se satisface la siguiente implicación. Si una función <span class="math inline">\(h : \R \longrightarrow \R\)</span>, tiene esperanza nula para cualquier esperanza tomada bajo la medida <span class="math inline">\(P_{\tau, \theta}\)</span>, entonces la función <span class="math inline">\(h\)</span> es nula en casi todas partes para toda medida <span class="math inline">\(P_\theta\)</span>. De forma más compacta. <span class="math display">\[
\forall P_{\tau,\theta}, \E_{P_{\tau,\theta}}\left[ h\left( \tau\left( X_1,\ldots, X_n \right) \right) \right] = 0
\Longrightarrow
\forall P_\theta, P_\theta\left( h\left( \tau\left( X_1,\ldots, X_n \right) \right) = 0 \right) = 1
\]</span> Por extensión si sucede esta implicación, se dice que <span class="math inline">\(\tau\)</span> es un <strong>estimador completo</strong> respecto de la familia <span class="math inline">\(\{ P_\theta \}_{\theta \in \Theta}\)</span>.</p>
</div>
</div>
<p>Para ello se ha formulado diferentes aproximaciones, entre las cuales citamos las siguientes:</p>
<ol type="1">
<li><p>Método de sustitución,</p></li>
<li><p>Método de los momentos,</p></li>
<li><p>Método de la distancia mínima,</p>
<p>3.1 Método de mínimos cuadrados,</p>
<p>3.2 Método de mínimo Chi-cuadrado,</p>
<p>3.3 Método de varianza mínima,</p>
<p>3.4 Método de minimización de la divergencia de Kullback–Leibler (máxima verosimilitud).</p></li>
<li><p>Método de máxima verosimilitud,</p></li>
<li><p>Método de Stein.</p></li>
</ol>
<!------------------------------------------------------------------------------------------------->
<section id="método-de-sustitución" class="level3" data-number="6.4.1">
<h3 data-number="6.4.1" class="anchored" data-anchor-id="método-de-sustitución"><span class="header-section-number">6.4.1</span> Método de sustitución</h3>
<p>Entonces, se parte de suponer que existe funcional <span class="math inline">\(G\)</span> actuando sobre el conjunto de medidas de probabilidad <span class="math inline">\(\mathcal{P}\)</span> que contiene al conjunto <span class="math inline">\(P( \Theta ) = \{ P_{\theta} \mid \theta \in \Theta\} \subset \mathcal{P}\)</span> y que toma valores en <span class="math inline">\(\Theta\)</span>, i.e.&nbsp;<span class="math inline">\(G: \mathcal{P} \longrightarrow \Theta\)</span>. De tal forma que <span class="math inline">\(P_{\theta}\)</span> es invariante, es decir: <span class="math display">\[
G( P_{\theta} ) = \theta,\quad \forall \theta \in \Theta
\]</span></p>
<p>Así se construye un <strong>estimador por el método de sustitución</strong> si a partir de la medida de probabilidad empírica <span class="math inline">\(P_n\)</span> construida con una muestra <span class="math inline">\(X_1, \ldots, X_n\)</span> de la variable aleatoria <span class="math inline">\(X\)</span>, se toma como parámetro el dado por: <span class="math display">\[
\widehat{\theta} = G\left( P_n \right)
\]</span></p>
<p>En otras palabras se sustituye el parámetro <span class="math inline">\(\theta\)</span> por <span class="math inline">\(\widehat{\theta}\)</span>. Esto implica que se aproxima la medida <span class="math inline">\(P_X\)</span> que caracteriza a <span class="math inline">\(X\)</span>, con la aproximación <span class="math inline">\(P_X \approx P_{\widehat{\theta}}\)</span>.</p>
<p>Es de notar que a priori no se hace establece ninguna medida de la calidad de la aproximación, para ello hay que adjuntar algunos otros criterios que caracterizan un buen tipo de estimador.</p>
<p>En otras ocasiones a partir de la muestra se define un estimador del parámetro <span class="math inline">\(\theta \in \Theta\)</span> a partir de una familia de funciones medibles que dependen directamente de la muestra <span class="math inline">\(X_1, \ldots, X_n\)</span> y su tamaño <span class="math inline">\(n\)</span>, i.e.&nbsp;una función <span class="math inline">\(\theta_n( X_1, \ldots, X_n )\)</span>. Se puede considerar el caso anterior como un caso en particular de este tipo de funciones, ya que se puede definir <span class="math inline">\(\theta_n( X_1, \ldots, X_n ) = G( P_n )\)</span>, pero hay que tener cuidado que <span class="math inline">\(G\)</span> es un funcional y como tal puede resultar una función que no es medible.</p>
<!------------------------------------------------------------------------------------------------->
</section>
<section id="método-de-momentos" class="level3" data-number="6.4.2">
<h3 data-number="6.4.2" class="anchored" data-anchor-id="método-de-momentos"><span class="header-section-number">6.4.2</span> Método de momentos</h3>
<p>Un estimador de momento se construye a partir de una relación función entre la media de una función medible <span class="math inline">\(g\)</span> y el parámetro <span class="math inline">\(\theta \in \Theta\)</span> que caracteriza a la distribución o medida de probabilidad de la variable aleatoria <span class="math inline">\(X\)</span>. Es decir, existen funciones <span class="math inline">\(g\)</span> y <span class="math inline">\(m\)</span> a valores reales de tal forma que <span class="math display">\[
m( \theta )
= \E_{P_X} \left[ g( X ) \right]
= \int\limits_{\R} g( x ) dP_X( x )
= \int\limits_{\R} g( x ) dP_{\theta}( x )
\]</span></p>
<p>Si de alguna forma se puede invertir <span class="math inline">\(m\)</span>, de tal forma que se pueda determinar <span class="math inline">\(\theta\)</span>, en tal caso se pude utilizar un <strong>estimador por momentos</strong> a partir de una muestra de la variable aleatoria <span class="math inline">\(X_1, \ldots, X_n\)</span> <span class="math display">\[
\widehat{\theta}
= \theta_n\left( X_1, \ldots, X_n \right)
= m^{-1}\left( \overline{g} \right)
\]</span> donde <span class="math display">\[
\overline{g}
= \int\limits_{\R} g( x ) dP_n( x )
= \frac{1}{n}\sum\limits_{i=1}^n g\left( X_i \right)
\]</span></p>
<p>Si en caso <span class="math inline">\(\overline{g}\)</span> está fuera de la imagen de <span class="math inline">\(m\)</span>, <span class="math inline">\(\overline{g} \notin m( \Theta )\)</span>, la inversión no sería posible. Para resolver este problema se puede recurrir a una distancia <span class="math inline">\(d\)</span> y buscar <span class="math inline">\(\hat{g}\)</span> tal que minimize la distancia a <span class="math inline">\(m( \Theta )\)</span>, i.e.&nbsp; <span class="math inline">\(\hat{g} = \underset{h \in m( \Theta )}{\arginf}\ d( \overline{g}, h)\)</span>. Una vez determinado <span class="math inline">\(\hat{g}\)</span> se toma como estimador de momentos su inversa con <span class="math inline">\(m\)</span>, i.e.&nbsp; <span class="math inline">\(\widehat{\theta} = m^{-1}( \hat{g} )\)</span>.</p>
<p>Es de notar que en el caso anterior, la selección de la mejor distancia <span class="math inline">\(d\)</span> no es para nada evidente y puede ser un problema tanto o más difícil que la misma estimación del parámetro <span class="math inline">\(\theta\)</span> o la inversión de <span class="math inline">\(m\)</span>.</p>
<p>La estimación por momentos en los casos más elaborados lleva a buscar la solución de problemas no lineales, que no suelen ser estables y que deben estar bien definidos para proveer una solución única.</p>
<!------------------------------------------------------------------------------------------------->
</section>
<section id="método-de-la-distancia-mínima" class="level3" data-number="6.4.3">
<h3 data-number="6.4.3" class="anchored" data-anchor-id="método-de-la-distancia-mínima"><span class="header-section-number">6.4.3</span> Método de la distancia mínima</h3>
<p>Este método requiere de la definición de una distancia sobre el espacio de distribuciones de probabilidad <span class="math inline">\(\mathcal{P}\)</span>, i.e.&nbsp;una función <span class="math inline">\(d : \mathcal{P} \times \mathcal{P} \longrightarrow \R_+\)</span> que satisface las siguientes propiedades:</p>
<ol type="1">
<li><p>Propiedad de simetría, para cualquier <span class="math inline">\(P, Q \in \mathcal{P}\)</span>, <span class="math inline">\(d( P, Q ) = d( Q, P )\)</span>,</p></li>
<li><p>Para cualquier <span class="math inline">\(P \in \mathcal{P}\)</span>, <span class="math inline">\(d( P, P ) = 0\)</span>.</p></li>
<li><p>Desigualdad triangular, para cualquier <span class="math inline">\(P,Q,R \in \mathcal{P}\)</span> <span class="math display">\[
d( P, Q ) \leq d( P, R ) + d( R, Q )
\]</span></p></li>
</ol>
<p>A partir de la medida empírica de probabilidad <span class="math inline">\(P_n\)</span>, se busca una probabilidad <span class="math inline">\(P_{\theta}\)</span> que minimize la distancia medida con <span class="math inline">\(d\)</span>. Así resulta el estimador con el <strong>método de distancia mínima</strong> <span class="math display">\[
\widehat{\theta} = \underset{\theta \in \Theta}{\arginf} d\left( P_{\theta}, P_n \right)
\]</span></p>
<p>Entra las distancias que se puede considerar tenemos las siguientes</p>
<ul>
<li><p>Distancia del supremo entre las distribuciones acumuladas correspondientes <span class="math display">\[
d(P,Q) = \underset{x}{\sup} \left| F_P( x ) - F_Q( x ) \right|
\]</span></p></li>
<li><p>Distancia cuadrática entre las distribuciones acumuladas correspondientes <span class="math display">\[
d( P, Q ) = \int\limits_{\R} \left( F_P( x ) - F_Q( x ) \right)^2\ dF_Q( x )
\]</span></p></li>
<li><p>Distancia de Wasserstein, para <span class="math inline">\(p &gt; 1\)</span> <span class="math display">\[
d( P, Q )
= W_p( P, Q )
= \underset{(X,Y) \rightsquigarrow \mu \in \Gamma( P, Q )}{\inf} \E_{\mu}\left[ |X - Y|^p \right]^{\frac{1}{p}}
\]</span> donde <span class="math display">\[
\Gamma( P, Q ) = \left\{ \mu \middle| \text{$\mu$ es medida de probabilidad sobre $\Omega \times \Omega$,
cuyas distribuciones marginales son $P$ y $Q$}\right\}
\]</span></p></li>
</ul>
<!------------------------------------------------------------------------------------------------->
</section>
<section id="método-de-maximización-de-la-verosimilitud" class="level3" data-number="6.4.4">
<h3 data-number="6.4.4" class="anchored" data-anchor-id="método-de-maximización-de-la-verosimilitud"><span class="header-section-number">6.4.4</span> Método de maximización de la verosimilitud</h3>
<p>La estimación de verosimilitud parte de asumir que se observan una cierta cantidad de eventos independientes <span class="math inline">\(B_1, \ldots, B_n\)</span>, relacionados precisamente a la variable aleatoria en estudio <span class="math inline">\(X\)</span>.</p>
<p>Se postula precisamente que la mejor medida de probabilidad es aquella que maximiza la probabilidad de observar estos eventos independientes. En el caso en particular de la estimación de una medida de probabilidad en una familia <span class="math inline">\(\{P_{\theta}\}\)</span> con <span class="math inline">\(\theta \in \Theta\)</span>, se busca maximizar la probabilidad: <span class="math display">\[
\underset{\theta \in \Theta}{\sup} \prod\limits_{i=1}^n P_{\theta}\left( B_i \right)
\]</span></p>
<p>o de forma equivalente, se puede utilizar una transformación con un logaritmo y tratar de maximizar lo que conocemos como <strong>función de verosimilitud logarítmica</strong> <span class="math display">\[
\ell = \sum\limits_{i=1}^n \log P_{\theta} \left( B_i \right)
\]</span></p>
<p>el problema de estimación se reduce a la siguiente optimización <span class="math display">\[
\underset{\theta \in \Theta}{\sup} \ell( \theta )
\]</span></p>
<p>Precisamente, el estimador de verosimilitud <span class="math inline">\(\widehat{\theta}\)</span>, es el que resuelve el problema de optimización donde <span class="math inline">\(\ell\)</span> es la función objetivo y <span class="math inline">\(\Theta\)</span> representa el conjunto de restricciones. <span class="math display">\[
\widehat{\theta}
= \underset{\theta \in \Theta}{\argsup} \ell( \theta )
\]</span></p>
<p>En la práctica los eventos que se observa <span class="math inline">\(B_i\)</span> son puntuales y son precisamente los valores que toma la variable aleatoria <span class="math inline">\(X\)</span> para diferentes valores en el espacio muestra <span class="math inline">\(\Omega\)</span>. Es decir, se observa una muestra <span class="math inline">\(x_1 = X( \omega_1 ), \ldots, x_n = X( \omega_n )\)</span>.</p>
<p>Una forma de atacar el problema de estimación, cuando las observaciones son puntuales es trabar de encerrar cada una de las observaciones en intervalos lo suficientemente pequeños. Se puede tomar un valor <span class="math inline">\(h &gt; 0\)</span>, y los eventos <span class="math inline">\(B_i = \left\{ \omega \in \Omega \middle| x_i - h \leq X( \omega ) \leq x_i + h \right\}\)</span> y una aproximación de la función de verosimilitud en función de <span class="math inline">\(h\)</span>, con la siguiente forma: <span class="math display">\[
\begin{split}
\ell_h( \theta )
&amp; = \sum\limits_{i=1}^n \log P_{\theta} \left( B_i \right) \\
&amp; = \sum\limits_{i=1}^n \log P_{\theta} \left( x_i - h \leq X \leq x_i + h \right) \\
&amp; = \sum\limits_{i=1}^n \log \left( F\left( x_i + h, \theta \right) - F\left( x_i - h, \theta \right) \right) \\
\end{split}
\]</span></p>
<p>Sabemos de los diferentes resultados de análisis para funciones de variación acotada, como es el caso de las distribuciones de probabilidad, que tienen diferenciales en casi todo punto, es decir que las singularidades de estas son conjuntos con medida nula. Además la cantidad de singularidades, de saltos que puede presentar una función de variación acotada es a lo sumo numerable. Entonces, cada valor observado <span class="math inline">\(x_i \in \Dif F\)</span> es un punto de continuidad donde la función de distribución <span class="math inline">\(F\)</span> es diferenciable o es un punto de singularidad, así los valores observados en la muestra se pueden clasificar en estos dos tipos <span class="citation" data-cites="KolmoFunAnalysis">[<a href="#ref-KolmoFunAnalysis" role="doc-biblioref">6</a>]</span>. De ello se puede dividir la suma anterior. <span class="math display">\[
\begin{split}
\ell_h( \theta )
&amp; = \sum\limits_{x_i \in \Dif F} \log \left( F\left( x_i + h, \theta \right) - F\left( x_i - h, \theta \right) \right)
+ \sum\limits_{x_i \notin \Dif F} \log \left( F\left( x_i + h, \theta \right) - F\left( x_i - h, \theta \right) \right) \\
\end{split}
\]</span> maximizar <span class="math inline">\(\ell_h\)</span> es equivalente a maximizar la siguiente función equivalente que notaremos con el mismo nombre <span class="math display">\[
\begin{split}
\ell_h( \theta )
&amp; = \frac{1}{h} \sum\limits_{x_i \in \Dif F} \log \left( F\left( x_i + h, \theta \right) - F\left( x_i - h, \theta \right) \right)
+ \sum\limits_{x_i \notin \Dif F} \log \left( F\left( x_i + h, \theta \right) - F\left( x_i - h, \theta \right) \right) \\
\end{split}
\]</span> la multiplicación de la primera suma por la constante <span class="math inline">\(\frac{1}{h}\)</span> tan solo escala la función, pero no cambia sus puntos críticos.</p>
<p>Tomando al límite, un valor <span class="math inline">\(h\)</span> cada vez más pequeño, para encerrar aún más en un intervalo los valores observados <span class="math inline">\(\{x_i\}\)</span>, tenemos lo siguiente: <span class="math display">\[
\begin{split}
\underset{h \searrow 0}{\lim} \ell_h( \theta )
&amp; = \underset{h \searrow 0}{\lim} \frac{1}{h} \sum\limits_{x_i \in \Dif F} \log \left( F\left( x_i + h, \theta \right) - F\left( x_i - h, \theta \right) \right)
+ \underset{h \searrow 0}{\lim} \sum\limits_{x_i \notin \Dif F} \log \left( F\left( x_i + h, \theta \right) - F\left( x_i - h, \theta \right) \right) \\
&amp; = \sum\limits_{x_i \in \Dif F} \log \left( f(x_i,\theta) \right)
+ \sum\limits_{x_i \notin \Dif F} \log \left( F\left( x_i, \theta \right) - F\left( x_i-, \theta \right) \right),\quad
\text{si existe el límite por izquierda de $F$}
\end{split}
\]</span></p>
<p>siendo <span class="math inline">\(F\left( x_i-, \theta \right)\)</span> el límite por izquierda de la función de distribución <span class="math inline">\(F\)</span> en el punto <span class="math inline">\(x_i\)</span>. Además, recordamos que toda función de distribución es continua por derecha, esto implica que <span class="math inline">\(F\left( x_i, \theta \right) = F\left( x_i+, \theta \right)\)</span>.</p>
<p>Para el caso puntual esta es precisamente la expresión de la función de verosimilitud logarítmica, la cual podemos utilizar para estimar el mejor parámetro <span class="math inline">\(\theta \in \Theta\)</span> que maximice el valor de verosimilitud.</p>
<div class="thmbox">
<p><span class="math display">\[
\ell( \theta ) =
\sum\limits_{x_i \in \Dif F} \log \left( f(x_i,\theta) \right)
+ \sum\limits_{x_i \notin \Dif F} \log \left( F\left( x_i, \theta \right) - F\left( x_i-, \theta \right) \right)
(\#eq:loglik)
\]</span></p>
</div>
<p>En los casos más sencillos de estimación por verosimilitud se suele cumplir una de las siguientes hipótesis: todos los puntos <span class="math inline">\(x_i\)</span> sobre los cuales se calcula son puntos de continuidad, o la función de distribución acumulada <span class="math inline">\(F\)</span> no tiene puntos singulares, siendo este último caso el más usual. Pero, en la práctica esto no sucede y la suma adicional a la derecha en la expresión anterior es precisamente necesaria, ya que la distribución <span class="math inline">\(F\)</span> si puede tener singularidades. Al utilizar solo la primera suma sobre los puntos donde hay diferenciabilidad, la estimación por verosimilitud para el parámetro <span class="math inline">\(\theta\)</span> no será la correcta.</p>
<p>Los estimadores que resulta de la maximización de verosimilitud satisfacen algunas propiedades deseables.</p>
<div class="thmbox">
<div id="mleconsistent" class="theorem">
<p>Los estimadores por maximización de verosimilitud, si en caso existen, estos son consistentes.</p>
</div>
</div>
<div class="thmbox">
<div id="mleefficient" class="theorem">
<p>Los estimadores por máximización de verosimilitud, si en caso existen, estos son eficientes.</p>
</div>
</div>
<div class="thmbox">
<div id="mlesufficient" class="theorem">
<p>Si en caso existe un estimador suficiente, este es función del estimador de máxima verosimilitud.</p>
</div>
</div>
<p>Un estimador de máxima verosimilitud no necesariamente es insesgado.</p>
<div class="thmbox">
<div id="mlenormality" class="theorem">
<p>Un estimador por maximización de verosimilitud se comporta normalmente de forma asintótica alrededor del verdadero parámetro <span class="math inline">\(\theta \in \Theta\)</span>. Más claramente, si <span class="math inline">\(\tau_n\)</span> es el estimador por maximización de verosimilitud, se tiene que <span class="math inline">\(\tau_n( X_1, \ldots, X_n ) \rightsquigarrow F_n\)</span>, entonces en el límite <span class="math inline">\(n \rightarrow +\infty\)</span>, se tiene la convergencia en distribución <span class="math inline">\(F_n \rightarrow N\left( \theta, I( \theta )^{-1} \right)\)</span>.</p>
<p>Donde <span class="math inline">\(I(\theta)\)</span>, es un criterio de información, dado por la matriz <span class="math display">\[
\left[ I(\theta) \right]_{i,j}
= \left[ \E\left[ -\frac{\D^2}{\D \theta_i \D \theta_j}\log f( X, \theta )\, \middle|\, \theta \right] \right]_{i,j}
\]</span></p>
</div>
</div>
<!------------------------------------------------------------------------------------------------->
</section>
<section id="pruebas-de-hipótesis" class="level3" data-number="6.4.5">
<h3 data-number="6.4.5" class="anchored" data-anchor-id="pruebas-de-hipótesis"><span class="header-section-number">6.4.5</span> Pruebas de hipótesis</h3>
<p>Usualmente sobre un variable aleatoria <span class="math inline">\(X\)</span> a valores en <span class="math inline">\(\R^n\)</span>, se suele establecer una decisión basada en el valores que toma la variable aleatoria, la selección se realiza entre diferentes hipótesis. De manera más formal, esta decisión puede ser representada por una función <span class="math inline">\(\delta : \R^n \longrightarrow D\)</span>, donde <span class="math inline">\(D = \{d_1,\ldots,d_n\}\)</span> es un conjunto discreto que representa las hipótesis. Se conoce a <span class="math inline">\(\delta\)</span> como un <strong>test estadístico de elección</strong>.</p>
<p>En muchos casos uno nos interesa encontrar el test estadístico <span class="math inline">\(\delta\)</span> que tenga el menor error de cometer una selección errada, así para cada decisión en <span class="math inline">\(d \in D\)</span>, el test <span class="math inline">\(\delta\)</span> tendrá probabilidades <span class="math inline">\(P( \delta( X ) \neq d )\)</span>, es decir, para cualquier otro test <span class="math inline">\(\eta\)</span>, se debe tener que <span class="math display">\[
P( \delta( X ) \neq d ) \leq P( \eta( X ) \neq d ),\qquad \forall d \in D
\]</span></p>
<div class="thmbox">
<div id="dpowertest" class="definition" name="Test más potente">
<p>En muchos casos uno no se interesa en cualquier tipo de test estadístico, sino en un cierto tipo de pruebas, más interesados en un subconjunto <span class="math inline">\(K \subset \{ \delta : \R^n \longrightarrow D \}\)</span>, a este se lo conoce como una <strong>clase</strong>. Así se dice que un test <span class="math inline">\(\delta \in K\)</span> es el <strong>test más potente</strong> en la clase <span class="math inline">\(K\)</span> para la hipótesis <span class="math inline">\(d \in D\)</span> si para cualquier otro test <span class="math inline">\(\eta \in K\)</span>, se tiene la desigualdad. <span class="math display">\[
P( \delta( X ) \neq d ) \leq P( \eta( X ) \neq d )
\]</span></p>
</div>
</div>


<!-- -->

<div id="refs" class="references csl-bib-body" role="list">
<div id="ref-mathstat1" class="csl-entry" role="listitem">
<div class="csl-left-margin">1. </div><div class="csl-right-inline">Bickel PJ, Doksum KA (2015) Mathematical statistics: Basic ideas and selected topics. CRC Press</div>
</div>
<div id="ref-est_math_bor" class="csl-entry" role="listitem">
<div class="csl-left-margin">2. </div><div class="csl-right-inline">Borovkov A (1988) <span>Estadística Matemática</span>: <span class="nocase">Estimación de los parámetros, Verificación de las hipótesis, Capítulos adicionales</span>. MIR</div>
</div>
<div id="ref-MathAssuNV1" class="csl-entry" role="listitem">
<div class="csl-left-margin">3. </div><div class="csl-right-inline">Denuit M, Charpentier A (2005) Mathématiques de l’assurance non-vie. Economica, Paris</div>
</div>
<div id="ref-funmathstat" class="csl-entry" role="listitem">
<div class="csl-left-margin">4. </div><div class="csl-right-inline">Gupta SG, Kapoor VK (2020) <span class="nocase">Fundamental of Mathematical Statistics</span>: A modern approach, 12th ed. Sultan Chand &amp; Sons</div>
</div>
<div id="ref-KNight" class="csl-entry" role="listitem">
<div class="csl-left-margin">5. </div><div class="csl-right-inline">Knight K (1999) Mathematical statistics. Chapman &amp; Hall/CRC</div>
</div>
<div id="ref-KolmoFunAnalysis" class="csl-entry" role="listitem">
<div class="csl-left-margin">6. </div><div class="csl-right-inline">Kolmogorov A, Fomin S, Silverman R (2012) <span>Introductory Real Analysis</span>. Dover Publications</div>
</div>
<div id="ref-stat_theo_infer" class="csl-entry" role="listitem">
<div class="csl-left-margin">7. </div><div class="csl-right-inline">Olive DJ (2010) <span class="nocase">Statistical Theory and Inference</span>. Springer</div>
</div>
</div>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./lectura_04.html" class="pagination-link" aria-label="Probabilidad y Estadística">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Probabilidad y Estadística</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./lectura_06.html" class="pagination-link" aria-label="Modelos de pérdida agregada">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Modelos de pérdida agregada</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb21" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb21-1"><a href="#cb21-1"></a><span class="co">&lt;!-------------------------------------------------------------------------------------------------&gt;</span></span>
<span id="cb21-2"><a href="#cb21-2"></a><span class="fu"># Distribuciones {#distribuciones}</span></span>
<span id="cb21-3"><a href="#cb21-3"></a></span>
<span id="cb21-6"><a href="#cb21-6"></a><span class="in">```{r}</span></span>
<span id="cb21-7"><a href="#cb21-7"></a><span class="co">#| echo: false</span></span>
<span id="cb21-8"><a href="#cb21-8"></a><span class="co">#| warning: false</span></span>
<span id="cb21-9"><a href="#cb21-9"></a><span class="fu">source</span>(<span class="st">"_common.R"</span>)</span>
<span id="cb21-10"><a href="#cb21-10"></a><span class="in">```</span></span>
<span id="cb21-11"><a href="#cb21-11"></a></span>
<span id="cb21-12"><a href="#cb21-12"></a><span class="co">&lt;!-------------------------------------------------------------------------------------------------&gt;</span></span>
<span id="cb21-13"><a href="#cb21-13"></a><span class="fu">## Distribuciones discretas</span></span>
<span id="cb21-14"><a href="#cb21-14"></a></span>
<span id="cb21-15"><a href="#cb21-15"></a><span class="co">&lt;!-------------------------------------------------------------------------------------------------&gt;</span></span>
<span id="cb21-16"><a href="#cb21-16"></a><span class="fu">### Distribución binomial</span></span>
<span id="cb21-17"><a href="#cb21-17"></a></span>
<span id="cb21-18"><a href="#cb21-18"></a>:::: {.thmbox data-latex=""}</span>
<span id="cb21-19"><a href="#cb21-19"></a>::: {.definition #dbinom name="Distribución binomial"}</span>
<span id="cb21-20"><a href="#cb21-20"></a>Una variable aleatoria $N$ que toma valores en $\N$ se dice que sigue una distribución </span>
<span id="cb21-21"><a href="#cb21-21"></a>o ley de binomial $N \rightsquigarrow Bin( n, p )$, con parámetros $n \in \N$ y </span>
<span id="cb21-22"><a href="#cb21-22"></a>$p \in <span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span>$, si:</span>
<span id="cb21-23"><a href="#cb21-23"></a>$$</span>
<span id="cb21-24"><a href="#cb21-24"></a>P( N = k ) = \binom{n}{k} p^k ( 1 - p )^{n-k}, \qquad</span>
<span id="cb21-25"><a href="#cb21-25"></a>\forall k \in <span class="sc">\{</span>0,\ldots, n<span class="sc">\}</span></span>
<span id="cb21-26"><a href="#cb21-26"></a>$$</span>
<span id="cb21-27"><a href="#cb21-27"></a>esta distribución discreta se caracteriza por presentar el valor $k \frac{p_k}{p_{k-1}}$ decreciente</span>
<span id="cb21-28"><a href="#cb21-28"></a>conforme cambia $k \in \N$</span>
<span id="cb21-29"><a href="#cb21-29"></a>:::</span>
<span id="cb21-30"><a href="#cb21-30"></a>::::</span>
<span id="cb21-31"><a href="#cb21-31"></a><span class="in">```{r l5c1}</span></span>
<span id="cb21-32"><a href="#cb21-32"></a><span class="in">set.seed(94312)</span></span>
<span id="cb21-33"><a href="#cb21-33"></a><span class="in">n &lt;- 50</span></span>
<span id="cb21-34"><a href="#cb21-34"></a><span class="in">p &lt;- 0.3</span></span>
<span id="cb21-35"><a href="#cb21-35"></a><span class="in">k &lt;- 2</span></span>
<span id="cb21-36"><a href="#cb21-36"></a><span class="in">m &lt;- 100</span></span>
<span id="cb21-37"><a href="#cb21-37"></a><span class="in">N &lt;- rbinom( n = m, size = n, prob = p ) # simular una muestra de tamaño m</span></span>
<span id="cb21-38"><a href="#cb21-38"></a><span class="in">pk &lt;- dbinom( x = k, size = n, prob = p ) # cálculo de probabilidad P( N = k )</span></span>
<span id="cb21-39"><a href="#cb21-39"></a><span class="in">Pk &lt;- pbinom( q = k, size = n, prob = p ) # cálculo de probabilidad P( N &lt;= k )</span></span>
<span id="cb21-40"><a href="#cb21-40"></a></span>
<span id="cb21-41"><a href="#cb21-41"></a><span class="in">p &lt;- dbinom( x = 0:n, size = n, prob = p )</span></span>
<span id="cb21-42"><a href="#cb21-42"></a><span class="in">v &lt;- 1:n * p[ 2:(n + 1) ] / p[ 1:n ]</span></span>
<span id="cb21-43"><a href="#cb21-43"></a><span class="in">```</span></span>
<span id="cb21-44"><a href="#cb21-44"></a></span>
<span id="cb21-45"><a href="#cb21-45"></a><span class="in">```{r l5g1, out.width='80%', fig.align='center', class.source = 'fold-hide'}</span></span>
<span id="cb21-46"><a href="#cb21-46"></a><span class="in">plt &lt;- ggplot() +</span></span>
<span id="cb21-47"><a href="#cb21-47"></a><span class="in">  geom_point( aes( x = 1:n, y = v ), colour = 'darkred' ) + </span></span>
<span id="cb21-48"><a href="#cb21-48"></a><span class="in">  xlab( TeX( "$k$" ) ) + </span></span>
<span id="cb21-49"><a href="#cb21-49"></a><span class="in">  ylab( TeX( "$k \\frac{p_k}{p_{k-1}}$" ) ) + </span></span>
<span id="cb21-50"><a href="#cb21-50"></a><span class="in">  theme_bw()</span></span>
<span id="cb21-51"><a href="#cb21-51"></a><span class="in">plot( plt )</span></span>
<span id="cb21-52"><a href="#cb21-52"></a><span class="in">```</span></span>
<span id="cb21-53"><a href="#cb21-53"></a></span>
<span id="cb21-54"><a href="#cb21-54"></a><span class="co">&lt;!-------------------------------------------------------------------------------------------------&gt;</span></span>
<span id="cb21-55"><a href="#cb21-55"></a><span class="fu">### Distribución de Poisson</span></span>
<span id="cb21-56"><a href="#cb21-56"></a>:::: {.thmbox data-latex=""}</span>
<span id="cb21-57"><a href="#cb21-57"></a>::: {.definition #dpois name="Distribución de Poisson"}</span>
<span id="cb21-58"><a href="#cb21-58"></a>Una variable aleatoria $N$ que toma valores en $\N$ se dice que sigue una distribución </span>
<span id="cb21-59"><a href="#cb21-59"></a>o ley de Poisson $N \rightsquigarrow Pois( n, p )$, con parámetro $\lambda \in \R$, si:</span>
<span id="cb21-60"><a href="#cb21-60"></a>$$</span>
<span id="cb21-61"><a href="#cb21-61"></a>P( N = k ) = \exp\left( -\lambda \right) \frac{\lambda^k}{k!}, \qquad</span>
<span id="cb21-62"><a href="#cb21-62"></a>\forall k \in \N</span>
<span id="cb21-63"><a href="#cb21-63"></a>$$</span>
<span id="cb21-64"><a href="#cb21-64"></a>esta distribución discreta se caracteriza por presentar el valor $k \frac{p_k}{p_{k-1}}$ constante</span>
<span id="cb21-65"><a href="#cb21-65"></a>conforme cambia $k \in \N$</span>
<span id="cb21-66"><a href="#cb21-66"></a>:::</span>
<span id="cb21-67"><a href="#cb21-67"></a>::::</span>
<span id="cb21-68"><a href="#cb21-68"></a><span class="in">```{r l5c2}</span></span>
<span id="cb21-69"><a href="#cb21-69"></a><span class="in">lambda &lt;- 2</span></span>
<span id="cb21-70"><a href="#cb21-70"></a><span class="in">k &lt;- 2</span></span>
<span id="cb21-71"><a href="#cb21-71"></a><span class="in">m &lt;- 100</span></span>
<span id="cb21-72"><a href="#cb21-72"></a><span class="in">N &lt;- rpois( n = m, lambda = lambda ) # simular una muestra de tamaño m</span></span>
<span id="cb21-73"><a href="#cb21-73"></a><span class="in">pk &lt;- dpois( x = k, lambda = lambda ) # cálculo de probabilidad P( N = k )</span></span>
<span id="cb21-74"><a href="#cb21-74"></a><span class="in">Pk &lt;- ppois( q = k, lambda = lambda ) # cálculo de probabilidad P( N &lt;= k )</span></span>
<span id="cb21-75"><a href="#cb21-75"></a></span>
<span id="cb21-76"><a href="#cb21-76"></a><span class="in">n &lt;- 50</span></span>
<span id="cb21-77"><a href="#cb21-77"></a><span class="in">p &lt;- dpois( x = 0:n, lambda = lambda )</span></span>
<span id="cb21-78"><a href="#cb21-78"></a><span class="in">v &lt;- 1:n * p[ 2:(n + 1) ] / p[ 1:n ]</span></span>
<span id="cb21-79"><a href="#cb21-79"></a><span class="in">```</span></span>
<span id="cb21-80"><a href="#cb21-80"></a></span>
<span id="cb21-81"><a href="#cb21-81"></a><span class="in">```{r l5g2, out.width='80%', fig.align='center', class.source = 'fold-hide'}</span></span>
<span id="cb21-82"><a href="#cb21-82"></a><span class="in">plt &lt;- ggplot() +</span></span>
<span id="cb21-83"><a href="#cb21-83"></a><span class="in">  geom_point( aes( x = 1:n, y = v ), colour = 'darkred' ) + </span></span>
<span id="cb21-84"><a href="#cb21-84"></a><span class="in">  xlab( TeX( "$k$" ) ) + </span></span>
<span id="cb21-85"><a href="#cb21-85"></a><span class="in">  ylab( TeX( "$k \\frac{p_k}{p_{k-1}}$" ) ) + </span></span>
<span id="cb21-86"><a href="#cb21-86"></a><span class="in">  theme_bw()</span></span>
<span id="cb21-87"><a href="#cb21-87"></a><span class="in">plot( plt )</span></span>
<span id="cb21-88"><a href="#cb21-88"></a><span class="in">```</span></span>
<span id="cb21-89"><a href="#cb21-89"></a></span>
<span id="cb21-90"><a href="#cb21-90"></a><span class="co">&lt;!-------------------------------------------------------------------------------------------------&gt;</span></span>
<span id="cb21-91"><a href="#cb21-91"></a><span class="fu">### Distribución binomial negativa</span></span>
<span id="cb21-92"><a href="#cb21-92"></a>:::: {.thmbox data-latex=""}</span>
<span id="cb21-93"><a href="#cb21-93"></a>::: {.definition #dnbinom name="Distribución binomial negativa"}</span>
<span id="cb21-94"><a href="#cb21-94"></a>Una variable aleatoria $N$ que toma valores en $\N$ se dice que sigue una distribución </span>
<span id="cb21-95"><a href="#cb21-95"></a>o ley de  binomial negativa $N \rightsquigarrow NBin( \alpha, p )$, con parámetro $\alpha &gt; 0$ y </span>
<span id="cb21-96"><a href="#cb21-96"></a>$p \in (0,1)$, si:</span>
<span id="cb21-97"><a href="#cb21-97"></a>$$</span>
<span id="cb21-98"><a href="#cb21-98"></a>P( N = k ) = \binom{\alpha + k - 1}{k} p^\alpha ( 1 - p )^k</span>
<span id="cb21-99"><a href="#cb21-99"></a>= \frac{\Gamma( \alpha + k )}{\Gamma(k+1) \Gamma(\alpha)}p^\alpha ( 1 - p )^k, \qquad</span>
<span id="cb21-100"><a href="#cb21-100"></a>\forall k \in \N</span>
<span id="cb21-101"><a href="#cb21-101"></a>$$</span>
<span id="cb21-102"><a href="#cb21-102"></a>donde $\Gamma( \alpha ) = \int\limits_0^{+\infty} x^{\alpha - 1} \exp(-x)\ dx$, $\forall \alpha \geq 0$.</span>
<span id="cb21-103"><a href="#cb21-103"></a></span>
<span id="cb21-104"><a href="#cb21-104"></a>Esta distribución discreta se caracteriza por presentar el valor $k \frac{p_k}{p_{k-1}}$ creciente</span>
<span id="cb21-105"><a href="#cb21-105"></a>conforme cambia $k \in \N$</span>
<span id="cb21-106"><a href="#cb21-106"></a>:::</span>
<span id="cb21-107"><a href="#cb21-107"></a>::::</span>
<span id="cb21-108"><a href="#cb21-108"></a><span class="in">```{r l5c3}</span></span>
<span id="cb21-109"><a href="#cb21-109"></a><span class="in">alpha &lt;- 2.5</span></span>
<span id="cb21-110"><a href="#cb21-110"></a><span class="in">p &lt;- 0.3</span></span>
<span id="cb21-111"><a href="#cb21-111"></a><span class="in">k &lt;- 2</span></span>
<span id="cb21-112"><a href="#cb21-112"></a><span class="in">m &lt;- 100</span></span>
<span id="cb21-113"><a href="#cb21-113"></a><span class="in">N &lt;- rnbinom( n = m, size = alpha, prob = p  ) # simular una muestra de tamaño m</span></span>
<span id="cb21-114"><a href="#cb21-114"></a><span class="in">pk &lt;- dnbinom( x = k, size = alpha, prob = p ) # cálculo de probabilidad P( N = k )</span></span>
<span id="cb21-115"><a href="#cb21-115"></a><span class="in">Pk &lt;- pnbinom( q = k, size = alpha, prob = p ) # cálculo de probabilidad P( N &lt;= k )</span></span>
<span id="cb21-116"><a href="#cb21-116"></a></span>
<span id="cb21-117"><a href="#cb21-117"></a><span class="in">n &lt;- 50</span></span>
<span id="cb21-118"><a href="#cb21-118"></a><span class="in">p &lt;- dnbinom( x = 0:n, size = alpha, prob = p )</span></span>
<span id="cb21-119"><a href="#cb21-119"></a><span class="in">v &lt;- 1:n * p[ 2:(n + 1) ] / p[ 1:n ]</span></span>
<span id="cb21-120"><a href="#cb21-120"></a><span class="in">```</span></span>
<span id="cb21-121"><a href="#cb21-121"></a></span>
<span id="cb21-122"><a href="#cb21-122"></a><span class="in">```{r l5g3, out.width='80%', fig.align='center', class.source = 'fold-hide'}</span></span>
<span id="cb21-123"><a href="#cb21-123"></a><span class="in">plt &lt;- ggplot() +</span></span>
<span id="cb21-124"><a href="#cb21-124"></a><span class="in">  geom_point( aes( x = 1:n, y = v ), colour = 'darkred' ) + </span></span>
<span id="cb21-125"><a href="#cb21-125"></a><span class="in">  xlab( TeX( "$k$" ) ) + </span></span>
<span id="cb21-126"><a href="#cb21-126"></a><span class="in">  ylab( TeX( "$k \\frac{p_k}{p_{k-1}}$" ) ) + </span></span>
<span id="cb21-127"><a href="#cb21-127"></a><span class="in">  theme_bw()</span></span>
<span id="cb21-128"><a href="#cb21-128"></a><span class="in">plot( plt )</span></span>
<span id="cb21-129"><a href="#cb21-129"></a><span class="in">```</span></span>
<span id="cb21-130"><a href="#cb21-130"></a></span>
<span id="cb21-131"><a href="#cb21-131"></a>:::: {.thmbox data-latex=""}</span>
<span id="cb21-132"><a href="#cb21-132"></a>::: {.definition #dgeo name="Distribución geométrica"}</span>
<span id="cb21-133"><a href="#cb21-133"></a>Una variable aleatoria $N$ que toma valores en $\N$ se dice que sigue una distribución </span>
<span id="cb21-134"><a href="#cb21-134"></a>o ley geométrica $N \rightsquigarrow Geo( p )$, con parámetro $p \in (0,1]$, si:</span>
<span id="cb21-135"><a href="#cb21-135"></a>$$</span>
<span id="cb21-136"><a href="#cb21-136"></a>P( N = k ) = ( 1 - p )^k p, \qquad</span>
<span id="cb21-137"><a href="#cb21-137"></a>\forall k \in \N</span>
<span id="cb21-138"><a href="#cb21-138"></a>$$</span>
<span id="cb21-139"><a href="#cb21-139"></a>:::</span>
<span id="cb21-140"><a href="#cb21-140"></a>::::</span>
<span id="cb21-141"><a href="#cb21-141"></a><span class="in">```{r l5c4}</span></span>
<span id="cb21-142"><a href="#cb21-142"></a><span class="in">p &lt;- 0.3</span></span>
<span id="cb21-143"><a href="#cb21-143"></a><span class="in">k &lt;- 2</span></span>
<span id="cb21-144"><a href="#cb21-144"></a><span class="in">m &lt;- 100</span></span>
<span id="cb21-145"><a href="#cb21-145"></a><span class="in">N &lt;- rgeom( n = m, prob = p  ) # simular una muestra de tamaño m</span></span>
<span id="cb21-146"><a href="#cb21-146"></a><span class="in">pk &lt;- dgeom( x = k, prob = p ) # cálculo de probabilidad P( N = k )</span></span>
<span id="cb21-147"><a href="#cb21-147"></a><span class="in">Pk &lt;- pgeom( q = k, prob = p ) # cálculo de probabilidad P( N &lt;= k )</span></span>
<span id="cb21-148"><a href="#cb21-148"></a><span class="in">```</span></span>
<span id="cb21-149"><a href="#cb21-149"></a>Es fácil darse cuenta que la distribución geométrica $Geo( p )$ es una binomial negativa $BN( 1, p )$,</span>
<span id="cb21-150"><a href="#cb21-150"></a>con $\alpha = 1$.</span>
<span id="cb21-151"><a href="#cb21-151"></a></span>
<span id="cb21-152"><a href="#cb21-152"></a>Asociado a estas distribuciones discretas existe un resultado de caracterización, el cual permite</span>
<span id="cb21-153"><a href="#cb21-153"></a>seleccionar la distribución de conteo.</span>
<span id="cb21-154"><a href="#cb21-154"></a></span>
<span id="cb21-155"><a href="#cb21-155"></a><span class="co">&lt;!-------------------------------------------------------------------------------------------------&gt;</span></span>
<span id="cb21-156"><a href="#cb21-156"></a><span class="fu">## Familia de Panjer</span></span>
<span id="cb21-157"><a href="#cb21-157"></a></span>
<span id="cb21-158"><a href="#cb21-158"></a>El criterio anterior para identificar el tipo de distribución, mediante la observación del</span>
<span id="cb21-159"><a href="#cb21-159"></a>comportamiento de la variable $k \frac{p_k}{p_{k-1}}$, se formaliza precisamente en la definición</span>
<span id="cb21-160"><a href="#cb21-160"></a>de la familia de Panjer.</span>
<span id="cb21-161"><a href="#cb21-161"></a></span>
<span id="cb21-162"><a href="#cb21-162"></a>:::: {.thmbox data-latex=""}</span>
<span id="cb21-163"><a href="#cb21-163"></a>::: {.definition #fampanjer name="Familia de Panjer"}</span>
<span id="cb21-164"><a href="#cb21-164"></a>Una variable aleatoria discreta $N$, que toma valores enteros positivos $N \in \N$, se dice</span>
<span id="cb21-165"><a href="#cb21-165"></a>que pertenece a la **familia de Panjer**, si sus probabilidades $p_k = P( N = k )$ para cada </span>
<span id="cb21-166"><a href="#cb21-166"></a>$k \in \N$, satisfacen la siguiente relación de recurrencia.</span>
<span id="cb21-167"><a href="#cb21-167"></a>$$</span>
<span id="cb21-168"><a href="#cb21-168"></a>p_k = \left( a + \frac{b}{k} \right)p_{k-1},\qquad \forall k \in \N \setminus <span class="sc">\{</span>0<span class="sc">\}</span> </span>
<span id="cb21-169"><a href="#cb21-169"></a>$$</span>
<span id="cb21-170"><a href="#cb21-170"></a>:::</span>
<span id="cb21-171"><a href="#cb21-171"></a>::::</span>
<span id="cb21-172"><a href="#cb21-172"></a></span>
<span id="cb21-173"><a href="#cb21-173"></a>Además tenemos la siguiente proposición que caracteriza a la distribución de las variables </span>
<span id="cb21-174"><a href="#cb21-174"></a>aleatorias en la familia de Panjer.</span>
<span id="cb21-175"><a href="#cb21-175"></a></span>
<span id="cb21-176"><a href="#cb21-176"></a>:::: {.thmbox data-latex=""}</span>
<span id="cb21-177"><a href="#cb21-177"></a>::: {.proposition #chfampanjer name="Caracterización familia de Panjer"}</span>
<span id="cb21-178"><a href="#cb21-178"></a>Las únicas leyes de probabilidad que satisfacen la relación de recurrencia anterior son:</span>
<span id="cb21-179"><a href="#cb21-179"></a></span>
<span id="cb21-180"><a href="#cb21-180"></a><span class="ss">1. </span>La ley de Poisson, la cual se obtiene para $a = 0$ y $b &gt; 0$</span>
<span id="cb21-181"><a href="#cb21-181"></a>$$</span>
<span id="cb21-182"><a href="#cb21-182"></a>k \frac{p_k}{p_{k-1}} = b &gt; 0,\quad \text{constante en $k$}</span>
<span id="cb21-183"><a href="#cb21-183"></a>$$</span>
<span id="cb21-184"><a href="#cb21-184"></a></span>
<span id="cb21-185"><a href="#cb21-185"></a><span class="ss">2. </span>La ley binomial negativa, la cual se obtiene para $0 &lt; a &lt; 1$ y $a + b &gt; 0$</span>
<span id="cb21-186"><a href="#cb21-186"></a>$$</span>
<span id="cb21-187"><a href="#cb21-187"></a>k \frac{p_k}{p_{k-1}} = a k + b &gt; 0,\quad \text{creciente en $k$}</span>
<span id="cb21-188"><a href="#cb21-188"></a>$$</span>
<span id="cb21-189"><a href="#cb21-189"></a></span>
<span id="cb21-190"><a href="#cb21-190"></a><span class="ss">3. </span>La ley binomial, la cual se obtenida para $a &lt; 0$ y $b = -a(m + 1)$, para cierto $m$ entero y </span>
<span id="cb21-191"><a href="#cb21-191"></a>positivo.</span>
<span id="cb21-192"><a href="#cb21-192"></a>$$</span>
<span id="cb21-193"><a href="#cb21-193"></a>k \frac{p_k}{p_{k-1}} = a( k - m - 1 ) &lt; 0, \quad \text{decreciente en $k$}</span>
<span id="cb21-194"><a href="#cb21-194"></a>$$</span>
<span id="cb21-195"><a href="#cb21-195"></a>:::</span>
<span id="cb21-196"><a href="#cb21-196"></a>::::</span>
<span id="cb21-197"><a href="#cb21-197"></a></span>
<span id="cb21-198"><a href="#cb21-198"></a>Para una demostración detallada de la proposición anterior se puede consultar @MathAssuNV1 o </span>
<span id="cb21-199"><a href="#cb21-199"></a>en https://nonlifemaths.github.io/.</span>
<span id="cb21-200"><a href="#cb21-200"></a></span>
<span id="cb21-201"><a href="#cb21-201"></a></span>
<span id="cb21-202"><a href="#cb21-202"></a><span class="in">```{r l5t1, echo = TRUE, results = 'asis', out.width='100%', class.source = 'fold-hide'}</span></span>
<span id="cb21-203"><a href="#cb21-203"></a><span class="in"># la librería CASdatasets fue previamente cargada</span></span>
<span id="cb21-204"><a href="#cb21-204"></a><span class="in">data( beMTPL97 )</span></span>
<span id="cb21-205"><a href="#cb21-205"></a><span class="in">beMTPL97 &lt;- as.data.table( beMTPL97 )</span></span>
<span id="cb21-206"><a href="#cb21-206"></a><span class="in">conteo &lt;- beMTPL97[ , list( fn = .N ), by = list( sex, fuel, N = nclaims ) ]</span></span>
<span id="cb21-207"><a href="#cb21-207"></a><span class="in">conteo[ , pn := fn / sum( fn ), by = list( sex, fuel ) ]</span></span>
<span id="cb21-208"><a href="#cb21-208"></a><span class="in">setorder( conteo, sex, fuel, N )</span></span>
<span id="cb21-209"><a href="#cb21-209"></a><span class="in">conteo[ , pns := shift( pn, type = 'lag', fill = 0 ) ]</span></span>
<span id="cb21-210"><a href="#cb21-210"></a><span class="in">conteo[ , jn := N * pns / pn ]</span></span>
<span id="cb21-211"><a href="#cb21-211"></a></span>
<span id="cb21-212"><a href="#cb21-212"></a><span class="in">conteo %&gt;%</span></span>
<span id="cb21-213"><a href="#cb21-213"></a><span class="in">  kable(</span></span>
<span id="cb21-214"><a href="#cb21-214"></a><span class="in">    label = NA,</span></span>
<span id="cb21-215"><a href="#cb21-215"></a><span class="in">    caption = 'Estimación conteos por sexo',</span></span>
<span id="cb21-216"><a href="#cb21-216"></a><span class="in">    row.names = FALSE,</span></span>
<span id="cb21-217"><a href="#cb21-217"></a><span class="in">    col.names = c( "sexo", "fuel", "$N$", "$f_k$", "$p_k$", "$p_{k+1}$", "$k \\frac{p_{k+1}}{p_k}$" ),</span></span>
<span id="cb21-218"><a href="#cb21-218"></a><span class="in">    align = 'llrrrrr',</span></span>
<span id="cb21-219"><a href="#cb21-219"></a><span class="in">    digits = c( 0, 0, 0, 0, 5, 5, 5 ),</span></span>
<span id="cb21-220"><a href="#cb21-220"></a><span class="in">    format.args = list( big.mark = ',', decimal.mark = '.', scientific = FALSE ),</span></span>
<span id="cb21-221"><a href="#cb21-221"></a><span class="in">    escape = FALSE,</span></span>
<span id="cb21-222"><a href="#cb21-222"></a><span class="in">    centering = TRUE, </span></span>
<span id="cb21-223"><a href="#cb21-223"></a><span class="in">    booktabs = TRUE ) %&gt;%</span></span>
<span id="cb21-224"><a href="#cb21-224"></a><span class="in">  kable_classic( full_width = FALSE, html_font = "Cambria", position = "center" ) %&gt;%</span></span>
<span id="cb21-225"><a href="#cb21-225"></a><span class="in">  scroll_box( width = "100%", height = "500px" )</span></span>
<span id="cb21-226"><a href="#cb21-226"></a><span class="in">```</span></span>
<span id="cb21-227"><a href="#cb21-227"></a></span>
<span id="cb21-228"><a href="#cb21-228"></a><span class="in">```{r l5g4, out.width='80%', fig.align='center', class.source = 'fold-hide'}</span></span>
<span id="cb21-229"><a href="#cb21-229"></a><span class="in">plt &lt;- ggplot() +</span></span>
<span id="cb21-230"><a href="#cb21-230"></a><span class="in">  geom_point( data = conteo, aes( x = N, y = jn ), colour = 'purple' ) + </span></span>
<span id="cb21-231"><a href="#cb21-231"></a><span class="in">  xlab( TeX( "$k$" ) ) + </span></span>
<span id="cb21-232"><a href="#cb21-232"></a><span class="in">  ylab( TeX( "$k \\frac{p_k}{p_{k-1}}$" ) ) + </span></span>
<span id="cb21-233"><a href="#cb21-233"></a><span class="in">  theme_bw()</span></span>
<span id="cb21-234"><a href="#cb21-234"></a><span class="in">plot( plt )</span></span>
<span id="cb21-235"><a href="#cb21-235"></a><span class="in">```</span></span>
<span id="cb21-236"><a href="#cb21-236"></a></span>
<span id="cb21-237"><a href="#cb21-237"></a>Además, una forma sencilla de estimar si una variable sigue una distribución de las tres antes </span>
<span id="cb21-238"><a href="#cb21-238"></a>descritas es estudiando su coeficiente de variación $\VC( N ) = \frac{\V<span class="co">[</span><span class="ot">N</span><span class="co">]</span>}{\E<span class="co">[</span><span class="ot">N</span><span class="co">]</span>}$.</span>
<span id="cb21-239"><a href="#cb21-239"></a></span>
<span id="cb21-240"><a href="#cb21-240"></a><span class="ss">1. </span>Si $N$ sigue una ley de Poisson $Pois(\lambda)$, entonces:</span>
<span id="cb21-241"><a href="#cb21-241"></a>$$</span>
<span id="cb21-242"><a href="#cb21-242"></a>\VC( N ) = \frac{\V<span class="co">[</span><span class="ot">N</span><span class="co">]</span>}{\E<span class="co">[</span><span class="ot">N</span><span class="co">]</span>} = \frac{\lambda}{\lambda} = 1</span>
<span id="cb21-243"><a href="#cb21-243"></a>$$</span>
<span id="cb21-244"><a href="#cb21-244"></a><span class="ss">2.  </span>Si $N$ sigue una ley de binomial negativa $NBinom( \alpha, p )$, entonces:</span>
<span id="cb21-245"><a href="#cb21-245"></a>$$</span>
<span id="cb21-246"><a href="#cb21-246"></a>\VC( N ) = \frac{\V<span class="co">[</span><span class="ot">N</span><span class="co">]</span>}{\E<span class="co">[</span><span class="ot">N</span><span class="co">]</span>} = \frac{\alpha\frac{1-p}{p^2}}{\alpha\frac{1-p}{p}} = \frac{1}{p} &gt; 1</span>
<span id="cb21-247"><a href="#cb21-247"></a>$$</span>
<span id="cb21-248"><a href="#cb21-248"></a></span>
<span id="cb21-249"><a href="#cb21-249"></a><span class="ss">3.  </span>Si $N$ sigue una ley de binomial $Binom( n, p )$, entonces:</span>
<span id="cb21-250"><a href="#cb21-250"></a>$$</span>
<span id="cb21-251"><a href="#cb21-251"></a>\VC( N ) = \frac{\V<span class="co">[</span><span class="ot">N</span><span class="co">]</span>}{\E<span class="co">[</span><span class="ot">N</span><span class="co">]</span>} = \frac{np(1-p)}{np} = 1 - p &lt; 1</span>
<span id="cb21-252"><a href="#cb21-252"></a>$$</span>
<span id="cb21-253"><a href="#cb21-253"></a></span>
<span id="cb21-254"><a href="#cb21-254"></a><span class="co">&lt;!-------------------------------------------------------------------------------------------------&gt;</span></span>
<span id="cb21-255"><a href="#cb21-255"></a><span class="fu">## Distribuciones continuas</span></span>
<span id="cb21-256"><a href="#cb21-256"></a></span>
<span id="cb21-257"><a href="#cb21-257"></a><span class="co">&lt;!-------------------------------------------------------------------------------------------------&gt;</span></span>
<span id="cb21-258"><a href="#cb21-258"></a><span class="fu">### Distribución uniforme</span></span>
<span id="cb21-259"><a href="#cb21-259"></a></span>
<span id="cb21-260"><a href="#cb21-260"></a>:::: {.thmbox data-latex=""}</span>
<span id="cb21-261"><a href="#cb21-261"></a>::: {.definition #dunif name="Distribución uniforme"}</span>
<span id="cb21-262"><a href="#cb21-262"></a>Una variable aleatoria $X$ a valores reales, sigue una distribución uniforme </span>
<span id="cb21-263"><a href="#cb21-263"></a>$X \rightsquigarrow Unif( a, b )$ de parámetros $a, b \in \R$, si su función de distribución</span>
<span id="cb21-264"><a href="#cb21-264"></a>acumulada es de la siguiente forma:</span>
<span id="cb21-265"><a href="#cb21-265"></a>$$</span>
<span id="cb21-266"><a href="#cb21-266"></a>F_X( x ) = \frac{x-a}{b-a} \mathbf{1}_{[a,b)}( x ) + \mathbf{1}_{[b,+\infty)}( x )</span>
<span id="cb21-267"><a href="#cb21-267"></a>$$</span>
<span id="cb21-268"><a href="#cb21-268"></a>sin mucho esfuerzo se puede verificar que su densidad de probabilidad está dada por la función</span>
<span id="cb21-269"><a href="#cb21-269"></a>$$</span>
<span id="cb21-270"><a href="#cb21-270"></a>f_X( x ) = \frac{1}{b-a}\mathbf{1}_{<span class="co">[</span><span class="ot">a,b</span><span class="co">]</span>}( x )</span>
<span id="cb21-271"><a href="#cb21-271"></a>$$</span>
<span id="cb21-272"><a href="#cb21-272"></a>$$</span>
<span id="cb21-273"><a href="#cb21-273"></a>M_X( t ) = \frac{\exp(bt)-\exp(at)}{t(b-a)}</span>
<span id="cb21-274"><a href="#cb21-274"></a>$$</span>
<span id="cb21-275"><a href="#cb21-275"></a>$$</span>
<span id="cb21-276"><a href="#cb21-276"></a>\E<span class="co">[</span><span class="ot">X</span><span class="co">]</span> = \frac{a + b}{2},\qquad \V<span class="co">[</span><span class="ot">X</span><span class="co">]</span> = \frac{(b - a)^2}{12}</span>
<span id="cb21-277"><a href="#cb21-277"></a>$$</span>
<span id="cb21-278"><a href="#cb21-278"></a>:::</span>
<span id="cb21-279"><a href="#cb21-279"></a>::::</span>
<span id="cb21-280"><a href="#cb21-280"></a><span class="in">```{r l5c6}</span></span>
<span id="cb21-281"><a href="#cb21-281"></a><span class="in">a &lt;- 1</span></span>
<span id="cb21-282"><a href="#cb21-282"></a><span class="in">b &lt;- 2</span></span>
<span id="cb21-283"><a href="#cb21-283"></a><span class="in">x &lt;- 1.5</span></span>
<span id="cb21-284"><a href="#cb21-284"></a><span class="in">m &lt;- 100</span></span>
<span id="cb21-285"><a href="#cb21-285"></a><span class="in">X &lt;- runif( n = m, min = a, max = b ) # simular una muestra de tamaño m</span></span>
<span id="cb21-286"><a href="#cb21-286"></a><span class="in">fx &lt;- dunif( x = x, min = a, max = b ) # cálculo de la densidad f(x)</span></span>
<span id="cb21-287"><a href="#cb21-287"></a><span class="in">Fk &lt;- punif( q = x, min = a, max = b ) # cálculo de probabilidad F(x)</span></span>
<span id="cb21-288"><a href="#cb21-288"></a><span class="in">```</span></span>
<span id="cb21-289"><a href="#cb21-289"></a></span>
<span id="cb21-290"><a href="#cb21-290"></a><span class="co">&lt;!-------------------------------------------------------------------------------------------------&gt;</span></span>
<span id="cb21-291"><a href="#cb21-291"></a><span class="fu">### Distribución exponencial</span></span>
<span id="cb21-292"><a href="#cb21-292"></a>:::: {.thmbox data-latex=""}</span>
<span id="cb21-293"><a href="#cb21-293"></a>::: {.definition #dexp name="Distribución exponencial"}</span>
<span id="cb21-294"><a href="#cb21-294"></a>Una variable aleatoria $X$ a valores reales, sigue una distribución exponencial </span>
<span id="cb21-295"><a href="#cb21-295"></a>$X \rightsquigarrow Exp( \lambda )$ de parámetros $\lambda &gt; 0$, si su función de distribución</span>
<span id="cb21-296"><a href="#cb21-296"></a>acumulada es de la siguiente forma:</span>
<span id="cb21-297"><a href="#cb21-297"></a>$$</span>
<span id="cb21-298"><a href="#cb21-298"></a>F_X( x ) = \mathbf{1}_{(0,+\infty)}( x ) \left( 1 - \exp\left( -\lambda x \right) \right)</span>
<span id="cb21-299"><a href="#cb21-299"></a>$$</span>
<span id="cb21-300"><a href="#cb21-300"></a>sin mucho esfuerzo se puede verificar que su densidad de probabilidad está dada por la función</span>
<span id="cb21-301"><a href="#cb21-301"></a>$$</span>
<span id="cb21-302"><a href="#cb21-302"></a>f_X( x ) = \mathbf{1}_{(0,+\infty)}( x ) \lambda \exp\left( -\lambda x \right)</span>
<span id="cb21-303"><a href="#cb21-303"></a>$$</span>
<span id="cb21-304"><a href="#cb21-304"></a>$$</span>
<span id="cb21-305"><a href="#cb21-305"></a>M_X( t ) = \frac{\lambda}{\lambda - t}</span>
<span id="cb21-306"><a href="#cb21-306"></a>$$</span>
<span id="cb21-307"><a href="#cb21-307"></a>$$</span>
<span id="cb21-308"><a href="#cb21-308"></a>\E<span class="co">[</span><span class="ot">X</span><span class="co">]</span> = \frac{1}{\lambda},\qquad \V<span class="co">[</span><span class="ot">X</span><span class="co">]</span> = \frac{1}{\lambda^2}</span>
<span id="cb21-309"><a href="#cb21-309"></a>$$</span>
<span id="cb21-310"><a href="#cb21-310"></a>:::</span>
<span id="cb21-311"><a href="#cb21-311"></a>::::</span>
<span id="cb21-312"><a href="#cb21-312"></a><span class="in">```{r l5c7}</span></span>
<span id="cb21-313"><a href="#cb21-313"></a><span class="in">lambda &lt;- 2</span></span>
<span id="cb21-314"><a href="#cb21-314"></a><span class="in">x &lt;- 1.5</span></span>
<span id="cb21-315"><a href="#cb21-315"></a><span class="in">m &lt;- 100</span></span>
<span id="cb21-316"><a href="#cb21-316"></a><span class="in">X &lt;- rexp( n = m, rate = lambda ) # simular una muestra de tamaño m</span></span>
<span id="cb21-317"><a href="#cb21-317"></a><span class="in">fx &lt;- dexp( x = x, rate = lambda ) # cálculo de la densidad f(x)</span></span>
<span id="cb21-318"><a href="#cb21-318"></a><span class="in">Fk &lt;- pexp( q = x, rate = lambda ) # cálculo de probabilidad F(x)</span></span>
<span id="cb21-319"><a href="#cb21-319"></a><span class="in">```</span></span>
<span id="cb21-320"><a href="#cb21-320"></a></span>
<span id="cb21-321"><a href="#cb21-321"></a><span class="co">&lt;!-------------------------------------------------------------------------------------------------&gt;</span></span>
<span id="cb21-322"><a href="#cb21-322"></a><span class="fu">### Distribución gamma</span></span>
<span id="cb21-323"><a href="#cb21-323"></a></span>
<span id="cb21-324"><a href="#cb21-324"></a>:::: {.thmbox data-latex=""}</span>
<span id="cb21-325"><a href="#cb21-325"></a>::: {.definition #dgamma name="Distribución gamma"}</span>
<span id="cb21-326"><a href="#cb21-326"></a>Una variable aleatoria $X$ a valores reales, sigue una distribución gamma </span>
<span id="cb21-327"><a href="#cb21-327"></a>$X \rightsquigarrow Gamma( \alpha, \beta )$ de parámetros $\alpha &gt; 0$, $\beta &gt; 0$, si su </span>
<span id="cb21-328"><a href="#cb21-328"></a>función de distribución acumulada es de la siguiente forma:</span>
<span id="cb21-329"><a href="#cb21-329"></a>$$</span>
<span id="cb21-330"><a href="#cb21-330"></a>F_X( x ) = \frac{\beta^\alpha}{\Gamma( \alpha )} \int\limits_{0}^{x} u^{\alpha-1} \exp(-\beta u)\ du</span>
<span id="cb21-331"><a href="#cb21-331"></a>$$</span>
<span id="cb21-332"><a href="#cb21-332"></a>si en caso $\alpha$ un entero positivo, i.e. $\alpha \in \N^*$, se puede calcular $F_X( x )$ con la siguiente serie</span>
<span id="cb21-333"><a href="#cb21-333"></a>$$</span>
<span id="cb21-334"><a href="#cb21-334"></a>F_X( x ) = 1 - \exp( -\lambda x ) \sum\limits_{n=0}^{\alpha-1} \frac{(\lambda x)^n}{n!}</span>
<span id="cb21-335"><a href="#cb21-335"></a>= \exp( -\lambda x ) \sum\limits_{n=\alpha}^{+\infty} \frac{(\lambda x)^n}{n!}</span>
<span id="cb21-336"><a href="#cb21-336"></a>$$</span>
<span id="cb21-337"><a href="#cb21-337"></a></span>
<span id="cb21-338"><a href="#cb21-338"></a>por su parte, la densidad de probabilidad automáticamente está dada por la función:</span>
<span id="cb21-339"><a href="#cb21-339"></a>$$</span>
<span id="cb21-340"><a href="#cb21-340"></a>f_X( x ) = \mathbf{1}_{[0,+\infty}( x ) \frac{\beta^\alpha}{\Gamma( \alpha )} x^{\alpha-1} \exp(-\beta x)</span>
<span id="cb21-341"><a href="#cb21-341"></a>$$</span>
<span id="cb21-342"><a href="#cb21-342"></a>$$</span>
<span id="cb21-343"><a href="#cb21-343"></a>M_X( t ) = \left( \frac{\beta}{\beta - t} \right)^\alpha,\qquad \text{si}\ t &lt; \beta</span>
<span id="cb21-344"><a href="#cb21-344"></a>$$</span>
<span id="cb21-345"><a href="#cb21-345"></a>$$</span>
<span id="cb21-346"><a href="#cb21-346"></a>\E<span class="co">[</span><span class="ot">X</span><span class="co">]</span> = \frac{\alpha}{\beta},\qquad </span>
<span id="cb21-347"><a href="#cb21-347"></a>\V<span class="co">[</span><span class="ot">X</span><span class="co">]</span> = \frac{\alpha}{\beta^2}</span>
<span id="cb21-348"><a href="#cb21-348"></a>$$</span>
<span id="cb21-349"><a href="#cb21-349"></a>:::</span>
<span id="cb21-350"><a href="#cb21-350"></a>::::</span>
<span id="cb21-351"><a href="#cb21-351"></a><span class="in">```{r l5c8}</span></span>
<span id="cb21-352"><a href="#cb21-352"></a><span class="in">alpha &lt;- 2</span></span>
<span id="cb21-353"><a href="#cb21-353"></a><span class="in">beta &lt;- 1</span></span>
<span id="cb21-354"><a href="#cb21-354"></a><span class="in">x &lt;- 3</span></span>
<span id="cb21-355"><a href="#cb21-355"></a><span class="in">m &lt;- 100</span></span>
<span id="cb21-356"><a href="#cb21-356"></a><span class="in">X &lt;- rgamma( n = m, shape = alpha, scale = beta ) # simular una muestra de tamaño m</span></span>
<span id="cb21-357"><a href="#cb21-357"></a><span class="in">fx &lt;- dgamma( x = x, shape = alpha, scale = beta ) # cálculo de la densidad f(x)</span></span>
<span id="cb21-358"><a href="#cb21-358"></a><span class="in">Fk &lt;- pgamma( q = x, shape = alpha, scale = beta ) # cálculo de probabilidad F(x)</span></span>
<span id="cb21-359"><a href="#cb21-359"></a><span class="in">```</span></span>
<span id="cb21-360"><a href="#cb21-360"></a></span>
<span id="cb21-361"><a href="#cb21-361"></a><span class="co">&lt;!-------------------------------------------------------------------------------------------------&gt;</span></span>
<span id="cb21-362"><a href="#cb21-362"></a><span class="fu">### Distribución normal</span></span>
<span id="cb21-363"><a href="#cb21-363"></a></span>
<span id="cb21-364"><a href="#cb21-364"></a>:::: {.thmbox data-latex=""}</span>
<span id="cb21-365"><a href="#cb21-365"></a>::: {.definition #dnorm name="Distribución normal"}</span>
<span id="cb21-366"><a href="#cb21-366"></a>Una variable aleatoria $X$ a valores reales, sigue una distribución normal </span>
<span id="cb21-367"><a href="#cb21-367"></a>$X \rightsquigarrow N( \mu, \sigma )$ de parámetros $\mu \in \R$, $\sigma &gt; 0$, si su </span>
<span id="cb21-368"><a href="#cb21-368"></a>función de distribución acumulada es de la siguiente forma:</span>
<span id="cb21-369"><a href="#cb21-369"></a>$$</span>
<span id="cb21-370"><a href="#cb21-370"></a>F_X( x ) = \frac{1}{\sqrt{2\pi} \sigma} \int\limits_{-\infty}^x \exp\left( -\frac{(y - \mu)^2}{\sigma^2} \right)\ dy</span>
<span id="cb21-371"><a href="#cb21-371"></a>$$</span>
<span id="cb21-372"><a href="#cb21-372"></a>la densidad de probabilidad automáticamente está dada por la función:</span>
<span id="cb21-373"><a href="#cb21-373"></a>$$</span>
<span id="cb21-374"><a href="#cb21-374"></a>f_X( x ) = \frac{1}{\sqrt{2\pi}} \exp\left( -\frac{(x - \mu)^2}{\sigma^2} \right)</span>
<span id="cb21-375"><a href="#cb21-375"></a>$$</span>
<span id="cb21-376"><a href="#cb21-376"></a>$$</span>
<span id="cb21-377"><a href="#cb21-377"></a>M_X( t ) = \exp\left( t \mu + \frac{1}{2} t^2 \sigma^2 \right)</span>
<span id="cb21-378"><a href="#cb21-378"></a>$$</span>
<span id="cb21-379"><a href="#cb21-379"></a>$$</span>
<span id="cb21-380"><a href="#cb21-380"></a>\E<span class="co">[</span><span class="ot">X</span><span class="co">]</span> = \mu,\qquad \V<span class="co">[</span><span class="ot">X</span><span class="co">]</span> = \sigma^2</span>
<span id="cb21-381"><a href="#cb21-381"></a>$$</span>
<span id="cb21-382"><a href="#cb21-382"></a>:::</span>
<span id="cb21-383"><a href="#cb21-383"></a>::::</span>
<span id="cb21-384"><a href="#cb21-384"></a><span class="in">```{r l5c9}</span></span>
<span id="cb21-385"><a href="#cb21-385"></a><span class="in">mu &lt;- 2</span></span>
<span id="cb21-386"><a href="#cb21-386"></a><span class="in">sigma &lt;- 1</span></span>
<span id="cb21-387"><a href="#cb21-387"></a><span class="in">x &lt;- 3</span></span>
<span id="cb21-388"><a href="#cb21-388"></a><span class="in">m &lt;- 100</span></span>
<span id="cb21-389"><a href="#cb21-389"></a><span class="in">X &lt;- rnorm( n = m, mean = mu, sd = sigma ) # simular una muestra de tamaño m</span></span>
<span id="cb21-390"><a href="#cb21-390"></a><span class="in">fx &lt;- dnorm( x = x, mean = mu, sd = sigma ) # cálculo de la densidad f(x)</span></span>
<span id="cb21-391"><a href="#cb21-391"></a><span class="in">Fk &lt;- pnorm( q = x, mean = mu, sd = sigma ) # cálculo de probabilidad F(x)</span></span>
<span id="cb21-392"><a href="#cb21-392"></a><span class="in">```</span></span>
<span id="cb21-393"><a href="#cb21-393"></a></span>
<span id="cb21-394"><a href="#cb21-394"></a><span class="co">&lt;!-------------------------------------------------------------------------------------------------&gt;</span></span>
<span id="cb21-395"><a href="#cb21-395"></a><span class="fu">### Distribución log-normal</span></span>
<span id="cb21-396"><a href="#cb21-396"></a></span>
<span id="cb21-397"><a href="#cb21-397"></a>:::: {.thmbox data-latex=""}</span>
<span id="cb21-398"><a href="#cb21-398"></a>::: {.definition #dlnorm name="Distribución log-normal"}</span>
<span id="cb21-399"><a href="#cb21-399"></a>Una variable aleatoria $X$ a valores reales, sigue una distribución log-normal </span>
<span id="cb21-400"><a href="#cb21-400"></a>$X \rightsquigarrow LN( \mu, \sigma )$ de parámetros $\mu &gt; 0$, $\sigma &gt; 0$, si su </span>
<span id="cb21-401"><a href="#cb21-401"></a>función de distribución acumulada es de la siguiente forma:</span>
<span id="cb21-402"><a href="#cb21-402"></a>$$</span>
<span id="cb21-403"><a href="#cb21-403"></a>F_X( x ) = \frac{1}{\sqrt{2\pi} \sigma} \int\limits_{0}^{x} \frac{1}{y} \exp\left( -\frac{(\ln(y) - \mu)^2}{\sigma^2} \right)\ dy</span>
<span id="cb21-404"><a href="#cb21-404"></a>$$</span>
<span id="cb21-405"><a href="#cb21-405"></a>la densidad de probabilidad automáticamente está dada por la función:</span>
<span id="cb21-406"><a href="#cb21-406"></a>$$</span>
<span id="cb21-407"><a href="#cb21-407"></a>f_X( x ) = \frac{1}{x\sqrt{2\pi} \sigma}  \exp\left( -\frac{(\ln(x) - \mu)^2}{\sigma^2} \right)</span>
<span id="cb21-408"><a href="#cb21-408"></a>$$</span>
<span id="cb21-409"><a href="#cb21-409"></a>No hay forma analítica para $M_X$</span>
<span id="cb21-410"><a href="#cb21-410"></a>$$</span>
<span id="cb21-411"><a href="#cb21-411"></a>\E<span class="co">[</span><span class="ot">X</span><span class="co">]</span> = \exp\left( \mu + \frac{1}{2}\sigma^2 \right),\qquad </span>
<span id="cb21-412"><a href="#cb21-412"></a>\V<span class="co">[</span><span class="ot">X</span><span class="co">]</span> = \exp\left( 2 \mu + \sigma^2 \right) \left( \exp( \sigma^2 ) - 1 \right)</span>
<span id="cb21-413"><a href="#cb21-413"></a>$$</span>
<span id="cb21-414"><a href="#cb21-414"></a>:::</span>
<span id="cb21-415"><a href="#cb21-415"></a>::::</span>
<span id="cb21-416"><a href="#cb21-416"></a><span class="in">```{r l5c10}</span></span>
<span id="cb21-417"><a href="#cb21-417"></a><span class="in">mu &lt;- 2</span></span>
<span id="cb21-418"><a href="#cb21-418"></a><span class="in">sigma &lt;- 1</span></span>
<span id="cb21-419"><a href="#cb21-419"></a><span class="in">x &lt;- 3</span></span>
<span id="cb21-420"><a href="#cb21-420"></a><span class="in">m &lt;- 100</span></span>
<span id="cb21-421"><a href="#cb21-421"></a><span class="in">X &lt;- rlnorm( n = m, meanlog = mu, sdlog = sigma ) # simular una muestra de tamaño m</span></span>
<span id="cb21-422"><a href="#cb21-422"></a><span class="in">fx &lt;- dlnorm( x = x, meanlog = mu, sdlog = sigma ) # cálculo de la densidad f(x)</span></span>
<span id="cb21-423"><a href="#cb21-423"></a><span class="in">Fk &lt;- plnorm( q = x, meanlog = mu, sdlog = sigma ) # cálculo de probabilidad F(x)</span></span>
<span id="cb21-424"><a href="#cb21-424"></a><span class="in">```</span></span>
<span id="cb21-425"><a href="#cb21-425"></a>En pocas, una variable aleatoria $X \rightsquigarrow LN( \mu, \sigma )$ sigue una distribución </span>
<span id="cb21-426"><a href="#cb21-426"></a>log-normal si y solamente si la variable aleatoria dada por su logaritmo $\ln( X ) \rightsquigarrow N( \mu, \sigma )$ </span>
<span id="cb21-427"><a href="#cb21-427"></a>sigue una distribución normal.</span>
<span id="cb21-428"><a href="#cb21-428"></a></span>
<span id="cb21-429"><a href="#cb21-429"></a><span class="co">&lt;!-------------------------------------------------------------------------------------------------&gt;</span></span>
<span id="cb21-430"><a href="#cb21-430"></a><span class="fu">### Distribución de beta</span></span>
<span id="cb21-431"><a href="#cb21-431"></a>Una variable aleatoria $X \in <span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$ a valores reales, sigue una distribución beta </span>
<span id="cb21-432"><a href="#cb21-432"></a>$X \rightsquigarrow Beta( \alpha, \beta )$ de parámetros $\alpha &gt; 0$, $\beta &gt; 0$, si su </span>
<span id="cb21-433"><a href="#cb21-433"></a>función de distribución acumulada es de la siguiente forma:</span>
<span id="cb21-434"><a href="#cb21-434"></a>$$</span>
<span id="cb21-435"><a href="#cb21-435"></a>F_X( x ) = \frac{1}{B(\alpha,\beta)} \int\limits_{0}^x t^{\alpha-1} ( 1 - t )^{\beta - 1}\ dt</span>
<span id="cb21-436"><a href="#cb21-436"></a>$$</span>
<span id="cb21-437"><a href="#cb21-437"></a>la densidad de probabilidad automáticamente está dada por la función:</span>
<span id="cb21-438"><a href="#cb21-438"></a>$$</span>
<span id="cb21-439"><a href="#cb21-439"></a>f_X( x ) = \frac{x^{\alpha-1} ( 1 - x )^{\beta - 1}}{B(\alpha,\beta)} </span>
<span id="cb21-440"><a href="#cb21-440"></a>$$</span>
<span id="cb21-441"><a href="#cb21-441"></a>$$</span>
<span id="cb21-442"><a href="#cb21-442"></a>M_X( t ) = 1 + \sum\limits_{k=1}^{+\infty} \left( \prod\limits_{l=0}^{k-1} \frac{\alpha + l}{\alpha + \beta + l} \right) \frac{t^k}{k!}</span>
<span id="cb21-443"><a href="#cb21-443"></a>$$</span>
<span id="cb21-444"><a href="#cb21-444"></a>$$</span>
<span id="cb21-445"><a href="#cb21-445"></a>\E<span class="co">[</span><span class="ot">X</span><span class="co">]</span> = \frac{\alpha}{\alpha + \beta},\qquad</span>
<span id="cb21-446"><a href="#cb21-446"></a>\V<span class="co">[</span><span class="ot">X</span><span class="co">]</span> = \frac{\alpha\beta}{(\alpha + \beta)^2(\alpha + \beta + 1)}</span>
<span id="cb21-447"><a href="#cb21-447"></a>$$</span>
<span id="cb21-448"><a href="#cb21-448"></a><span class="in">```{r l5c11}</span></span>
<span id="cb21-449"><a href="#cb21-449"></a><span class="in">alpha &lt;- 1</span></span>
<span id="cb21-450"><a href="#cb21-450"></a><span class="in">beta &lt;- 2</span></span>
<span id="cb21-451"><a href="#cb21-451"></a><span class="in">x &lt;- 0.5</span></span>
<span id="cb21-452"><a href="#cb21-452"></a><span class="in">m &lt;- 100</span></span>
<span id="cb21-453"><a href="#cb21-453"></a><span class="in">X &lt;- rbeta( n = m, shape1 = alpha, shape2 = beta ) # simular una muestra de tamaño m</span></span>
<span id="cb21-454"><a href="#cb21-454"></a><span class="in">fx &lt;- dbeta( x = x, shape1 = alpha, shape2 = beta ) # cálculo de la densidad f(x)</span></span>
<span id="cb21-455"><a href="#cb21-455"></a><span class="in">Fk &lt;- pbeta( q = x, shape1 = alpha, shape2 = beta ) # cálculo de probabilidad F(x)</span></span>
<span id="cb21-456"><a href="#cb21-456"></a><span class="in">```</span></span>
<span id="cb21-457"><a href="#cb21-457"></a></span>
<span id="cb21-458"><a href="#cb21-458"></a><span class="co">&lt;!-------------------------------------------------------------------------------------------------&gt;</span></span>
<span id="cb21-459"><a href="#cb21-459"></a><span class="fu">### Distribución de Pareto generalizada</span></span>
<span id="cb21-460"><a href="#cb21-460"></a></span>
<span id="cb21-461"><a href="#cb21-461"></a>:::: {.thmbox data-latex=""}</span>
<span id="cb21-462"><a href="#cb21-462"></a>::: {.definition #dgpd name="Distribución de Pareto generalizada"}</span>
<span id="cb21-463"><a href="#cb21-463"></a>Una variable aleatoria $X$ a valores reales, sigue una distribución de Pareto generalizada</span>
<span id="cb21-464"><a href="#cb21-464"></a>$X \rightsquigarrow GPD( \mu, \sigma, \xi )$ de parámetros $\mu \in \R, \sigma &gt; 0, \xi \in \R$, </span>
<span id="cb21-465"><a href="#cb21-465"></a>si su función de distribución acumulada es de la siguiente forma:</span>
<span id="cb21-466"><a href="#cb21-466"></a>$$</span>
<span id="cb21-467"><a href="#cb21-467"></a>F_X( x )</span>
<span id="cb21-468"><a href="#cb21-468"></a>= \left <span class="sc">\{</span></span>
<span id="cb21-469"><a href="#cb21-469"></a>\begin{array}{ll}</span>
<span id="cb21-470"><a href="#cb21-470"></a>1 - \left( 1 + \xi \frac{x-\mu}{\sigma} \right)^{-\frac{1}{\xi}} &amp; \text{si}\ \xi \neq 0 <span class="sc">\\</span></span>
<span id="cb21-471"><a href="#cb21-471"></a>1 - \exp\left( -\frac{x-\mu}{\sigma} \right) &amp; \text{si}\ \xi = 0</span>
<span id="cb21-472"><a href="#cb21-472"></a>\end{array}</span>
<span id="cb21-473"><a href="#cb21-473"></a>\right.</span>
<span id="cb21-474"><a href="#cb21-474"></a>$$</span>
<span id="cb21-475"><a href="#cb21-475"></a>y su densidad de probabilidad está dada por la función</span>
<span id="cb21-476"><a href="#cb21-476"></a>$$</span>
<span id="cb21-477"><a href="#cb21-477"></a>f_X( x )</span>
<span id="cb21-478"><a href="#cb21-478"></a>= \left <span class="sc">\{</span></span>
<span id="cb21-479"><a href="#cb21-479"></a>\begin{array}{ll}</span>
<span id="cb21-480"><a href="#cb21-480"></a>\frac{1}{\sigma} \left( 1 + \xi \frac{x-\mu}{\sigma} \right)^{-1-\frac{1}{\xi}} &amp; \text{si}\ \xi \neq 0 <span class="sc">\\</span></span>
<span id="cb21-481"><a href="#cb21-481"></a>\frac{1}{\sigma} \exp\left( -\frac{x-\mu}{\sigma} \right) &amp; \text{si}\ \xi = 0</span>
<span id="cb21-482"><a href="#cb21-482"></a>\end{array}</span>
<span id="cb21-483"><a href="#cb21-483"></a>\right.</span>
<span id="cb21-484"><a href="#cb21-484"></a>$$</span>
<span id="cb21-485"><a href="#cb21-485"></a>$$</span>
<span id="cb21-486"><a href="#cb21-486"></a>M_X( t ) = \exp(\theta \mu) \sum\limits_{j=0}^{+\infty} \frac{\theta^j \sigma^j}</span>
<span id="cb21-487"><a href="#cb21-487"></a>{\prod\limits_{k=0}^j ( 1 - k \xi )}</span>
<span id="cb21-488"><a href="#cb21-488"></a>$$</span>
<span id="cb21-489"><a href="#cb21-489"></a>:::</span>
<span id="cb21-490"><a href="#cb21-490"></a>::::</span>
<span id="cb21-491"><a href="#cb21-491"></a><span class="in">```{r l5c12}</span></span>
<span id="cb21-492"><a href="#cb21-492"></a><span class="in">xi &lt;- 1</span></span>
<span id="cb21-493"><a href="#cb21-493"></a><span class="in">mu &lt;- 2</span></span>
<span id="cb21-494"><a href="#cb21-494"></a><span class="in">sigma &lt;- 1</span></span>
<span id="cb21-495"><a href="#cb21-495"></a><span class="in">x &lt;- 3</span></span>
<span id="cb21-496"><a href="#cb21-496"></a><span class="in">m &lt;- 100</span></span>
<span id="cb21-497"><a href="#cb21-497"></a><span class="in">X &lt;- rgpd( n = m, xi = xi, mu = mu, beta = sigma ) # simular una muestra de tamaño m</span></span>
<span id="cb21-498"><a href="#cb21-498"></a><span class="in">fx &lt;- dgpd( x = x, xi = xi, mu = mu, beta = sigma ) # cálculo de la densidad f(x)</span></span>
<span id="cb21-499"><a href="#cb21-499"></a><span class="in">Fk &lt;- pgpd( q = x, xi = xi, mu = mu, beta = sigma ) # cálculo de probabilidad F(x)</span></span>
<span id="cb21-500"><a href="#cb21-500"></a><span class="in">```</span></span>
<span id="cb21-501"><a href="#cb21-501"></a></span>
<span id="cb21-502"><a href="#cb21-502"></a><span class="co">&lt;!-------------------------------------------------------------------------------------------------&gt;</span></span>
<span id="cb21-503"><a href="#cb21-503"></a><span class="fu">### Distribución de valores extremos generalizada</span></span>
<span id="cb21-504"><a href="#cb21-504"></a></span>
<span id="cb21-505"><a href="#cb21-505"></a>:::: {.thmbox data-latex=""}</span>
<span id="cb21-506"><a href="#cb21-506"></a>::: {.definition #dgev  name="Distribución de valores extremos generalizada"}</span>
<span id="cb21-507"><a href="#cb21-507"></a>Una variable aleatoria $X$ a valores reales, sigue una distribución generalizada de valores extremos</span>
<span id="cb21-508"><a href="#cb21-508"></a>$X \rightsquigarrow GEV( \mu, \sigma, \xi )$ de parámetros $\mu \in \R, \sigma &gt; 0, \xi \in \R$, </span>
<span id="cb21-509"><a href="#cb21-509"></a>si su función de distribución acumulada es de la siguiente forma:</span>
<span id="cb21-510"><a href="#cb21-510"></a>$$</span>
<span id="cb21-511"><a href="#cb21-511"></a>F_X( x ) </span>
<span id="cb21-512"><a href="#cb21-512"></a>= \left<span class="sc">\{</span></span>
<span id="cb21-513"><a href="#cb21-513"></a>\begin{array}{ll}</span>
<span id="cb21-514"><a href="#cb21-514"></a>\exp\left( -\exp\left( -\frac{x-\mu}{\sigma} \right) \right) </span>
<span id="cb21-515"><a href="#cb21-515"></a>&amp; \text{si}\ \xi = 0 <span class="sc">\\</span></span>
<span id="cb21-516"><a href="#cb21-516"></a>\exp\left( -\left( 1 + \xi \frac{x-\mu}{\sigma} \right)^{-\frac{1}{\xi}} \right) </span>
<span id="cb21-517"><a href="#cb21-517"></a>&amp; \text{si}\ \xi \neq 0, 1 + \xi\frac{x - \mu}{\sigma} &gt; 0</span>
<span id="cb21-518"><a href="#cb21-518"></a>\end{array}</span>
<span id="cb21-519"><a href="#cb21-519"></a>\right.</span>
<span id="cb21-520"><a href="#cb21-520"></a>$$</span>
<span id="cb21-521"><a href="#cb21-521"></a>además se puede verificar que su densidad de probabilidad está dada por la función</span>
<span id="cb21-522"><a href="#cb21-522"></a>$$</span>
<span id="cb21-523"><a href="#cb21-523"></a>f_X( x )</span>
<span id="cb21-524"><a href="#cb21-524"></a>= \left<span class="sc">\{</span></span>
<span id="cb21-525"><a href="#cb21-525"></a>\begin{array}{ll}</span>
<span id="cb21-526"><a href="#cb21-526"></a>\exp\left(-\frac{x-\mu}{\sigma}\right) </span>
<span id="cb21-527"><a href="#cb21-527"></a>\exp\left(-\exp\left(-\frac{x-\mu}{\sigma}\right)\right) </span>
<span id="cb21-528"><a href="#cb21-528"></a>&amp; \text{si}\ \xi = 0 <span class="sc">\\</span></span>
<span id="cb21-529"><a href="#cb21-529"></a>\left( 1 + \xi \frac{x - \mu}{\sigma}\right)^{-1-\frac{1}{\xi}} </span>
<span id="cb21-530"><a href="#cb21-530"></a>\exp\left( -\left( 1 + \xi \frac{x-\mu}{\sigma} \right)^{-\frac{1}{\xi}} \right)</span>
<span id="cb21-531"><a href="#cb21-531"></a> &amp; \text{si}\ \xi \neq 0, 1 + \xi\frac{x - \mu}{\sigma} &gt; 0</span>
<span id="cb21-532"><a href="#cb21-532"></a>\end{array}</span>
<span id="cb21-533"><a href="#cb21-533"></a>\right.</span>
<span id="cb21-534"><a href="#cb21-534"></a>$$</span>
<span id="cb21-535"><a href="#cb21-535"></a>:::</span>
<span id="cb21-536"><a href="#cb21-536"></a>::::</span>
<span id="cb21-537"><a href="#cb21-537"></a><span class="in">```{r l5c13}</span></span>
<span id="cb21-538"><a href="#cb21-538"></a><span class="in">xi &lt;- -1</span></span>
<span id="cb21-539"><a href="#cb21-539"></a><span class="in">mu &lt;- 2</span></span>
<span id="cb21-540"><a href="#cb21-540"></a><span class="in">sigma &lt;- 1</span></span>
<span id="cb21-541"><a href="#cb21-541"></a><span class="in">x &lt;- 3</span></span>
<span id="cb21-542"><a href="#cb21-542"></a><span class="in">m &lt;- 100</span></span>
<span id="cb21-543"><a href="#cb21-543"></a><span class="in">X &lt;- rgev( n = m, xi = xi, mu = mu, beta = sigma ) # simular una muestra de tamaño m</span></span>
<span id="cb21-544"><a href="#cb21-544"></a><span class="in">fx &lt;- dgev( x = x, xi = xi, mu = mu, beta = sigma ) # cálculo de la densidad f(x)</span></span>
<span id="cb21-545"><a href="#cb21-545"></a><span class="in">Fk &lt;- pgev( q = x, xi = xi, mu = mu, beta = sigma ) # cálculo de probabilidad F(x)</span></span>
<span id="cb21-546"><a href="#cb21-546"></a><span class="in">```</span></span>
<span id="cb21-547"><a href="#cb21-547"></a></span>
<span id="cb21-548"><a href="#cb21-548"></a><span class="co">&lt;!-------------------------------------------------------------------------------------------------&gt;</span></span>
<span id="cb21-549"><a href="#cb21-549"></a><span class="fu">### Distribución t de Student</span></span>
<span id="cb21-550"><a href="#cb21-550"></a></span>
<span id="cb21-551"><a href="#cb21-551"></a>:::: {.thmbox data-latex=""}</span>
<span id="cb21-552"><a href="#cb21-552"></a>::: {.definition #dtst  name="Distribución t de Student"}</span>
<span id="cb21-553"><a href="#cb21-553"></a>Una variable aleatoria $X$ a valores reales, sigue una distribución t de Student</span>
<span id="cb21-554"><a href="#cb21-554"></a>$X \rightsquigarrow t( \nu )$ de parámetros $\nu &gt; 0$, </span>
<span id="cb21-555"><a href="#cb21-555"></a>si su función de distribución acumulada es de la siguiente forma:</span>
<span id="cb21-556"><a href="#cb21-556"></a>$$</span>
<span id="cb21-557"><a href="#cb21-557"></a>F_X( x ) </span>
<span id="cb21-558"><a href="#cb21-558"></a>= \frac{1}{2} + \frac{x}{\sqrt{\pi \nu}} </span>
<span id="cb21-559"><a href="#cb21-559"></a>\frac{\Gamma\left( \frac{\nu + 1}{2} \right)}{\Gamma\left( \frac{\nu}{2} \right)}</span>
<span id="cb21-560"><a href="#cb21-560"></a>F\left( \frac{1}{2}, \frac{\nu+1}{2}, \frac{3}{2}, -\frac{x^2}{\nu} \right)</span>
<span id="cb21-561"><a href="#cb21-561"></a>$$</span>
<span id="cb21-562"><a href="#cb21-562"></a>donde $F$ es la función hipergeométrica.</span>
<span id="cb21-563"><a href="#cb21-563"></a>$$</span>
<span id="cb21-564"><a href="#cb21-564"></a>F( a, b, c, z ) = \sum\limits_{n=0}^{+\infty} \frac{(a)_n (b)_n}{(c)_n} \frac{z^n}{n!}</span>
<span id="cb21-565"><a href="#cb21-565"></a>$$</span>
<span id="cb21-566"><a href="#cb21-566"></a>con</span>
<span id="cb21-567"><a href="#cb21-567"></a>$$</span>
<span id="cb21-568"><a href="#cb21-568"></a>(a)_n </span>
<span id="cb21-569"><a href="#cb21-569"></a>= \left<span class="sc">\{</span></span>
<span id="cb21-570"><a href="#cb21-570"></a>\begin{array}{ll}</span>
<span id="cb21-571"><a href="#cb21-571"></a>1 &amp; n = 0 <span class="sc">\\</span></span>
<span id="cb21-572"><a href="#cb21-572"></a>a( a + 1 ) \cdots (a + n - 1) &amp; n &gt; 0</span>
<span id="cb21-573"><a href="#cb21-573"></a>\end{array}</span>
<span id="cb21-574"><a href="#cb21-574"></a>\right.</span>
<span id="cb21-575"><a href="#cb21-575"></a>$$</span>
<span id="cb21-576"><a href="#cb21-576"></a>Además, se puede verificar que su densidad de probabilidad está dada por la función</span>
<span id="cb21-577"><a href="#cb21-577"></a>$$</span>
<span id="cb21-578"><a href="#cb21-578"></a>f_X( x )</span>
<span id="cb21-579"><a href="#cb21-579"></a>= \frac{x}{\sqrt{\pi \nu}} </span>
<span id="cb21-580"><a href="#cb21-580"></a>\frac{\Gamma\left(\frac{\nu+1}{2}\right)}{\Gamma\left( \frac{\nu}{2} \right)}</span>
<span id="cb21-581"><a href="#cb21-581"></a>\left( 1 + \frac{x^2}{\nu} \right)^{-\frac{\nu+1}{2}}</span>
<span id="cb21-582"><a href="#cb21-582"></a>$$</span>
<span id="cb21-583"><a href="#cb21-583"></a>La función generadora de momentos $M_X( t )$ no está definida</span>
<span id="cb21-584"><a href="#cb21-584"></a>$$</span>
<span id="cb21-585"><a href="#cb21-585"></a>\E<span class="co">[</span><span class="ot">X</span><span class="co">]</span> </span>
<span id="cb21-586"><a href="#cb21-586"></a>= \left<span class="sc">\{</span></span>
<span id="cb21-587"><a href="#cb21-587"></a>\begin{array}{ll}</span>
<span id="cb21-588"><a href="#cb21-588"></a>0 &amp; \text{si}\ \nu &gt; 0 <span class="sc">\\</span></span>
<span id="cb21-589"><a href="#cb21-589"></a>\text{no definida} &amp; \text{si}\ \nu \leq 0</span>
<span id="cb21-590"><a href="#cb21-590"></a>\end{array}</span>
<span id="cb21-591"><a href="#cb21-591"></a>\right.</span>
<span id="cb21-592"><a href="#cb21-592"></a>$$</span>
<span id="cb21-593"><a href="#cb21-593"></a></span>
<span id="cb21-594"><a href="#cb21-594"></a>$$</span>
<span id="cb21-595"><a href="#cb21-595"></a>\V<span class="co">[</span><span class="ot">X</span><span class="co">]</span> </span>
<span id="cb21-596"><a href="#cb21-596"></a>= \left<span class="sc">\{</span></span>
<span id="cb21-597"><a href="#cb21-597"></a>\begin{array}{ll}</span>
<span id="cb21-598"><a href="#cb21-598"></a>\frac{\nu}{\nu-2} &amp; \text{si}\ \nu &gt; 2 <span class="sc">\\</span></span>
<span id="cb21-599"><a href="#cb21-599"></a>+\infty &amp; \text{si}\ 1 &lt; \nu \leq 2 <span class="sc">\\</span></span>
<span id="cb21-600"><a href="#cb21-600"></a>\text{no definida} &amp; \text{si}\ \nu \leq 1</span>
<span id="cb21-601"><a href="#cb21-601"></a>\end{array}</span>
<span id="cb21-602"><a href="#cb21-602"></a>\right.</span>
<span id="cb21-603"><a href="#cb21-603"></a>$$</span>
<span id="cb21-604"><a href="#cb21-604"></a>:::</span>
<span id="cb21-605"><a href="#cb21-605"></a>::::</span>
<span id="cb21-606"><a href="#cb21-606"></a><span class="in">```{r l5c14}</span></span>
<span id="cb21-607"><a href="#cb21-607"></a><span class="in">nu &lt;- 3</span></span>
<span id="cb21-608"><a href="#cb21-608"></a><span class="in">x &lt;- 4</span></span>
<span id="cb21-609"><a href="#cb21-609"></a><span class="in">m &lt;- 100</span></span>
<span id="cb21-610"><a href="#cb21-610"></a><span class="in">X &lt;- rt( n = m, df = nu ) # simular una muestra de tamaño m</span></span>
<span id="cb21-611"><a href="#cb21-611"></a><span class="in">fx &lt;- dt( x = x, df = nu ) # cálculo de la densidad f(x)</span></span>
<span id="cb21-612"><a href="#cb21-612"></a><span class="in">Fk &lt;- pt( q = x, df = nu ) # cálculo de probabilidad F(x)</span></span>
<span id="cb21-613"><a href="#cb21-613"></a><span class="in">```</span></span>
<span id="cb21-614"><a href="#cb21-614"></a></span>
<span id="cb21-615"><a href="#cb21-615"></a><span class="co">&lt;!-------------------------------------------------------------------------------------------------&gt;</span></span>
<span id="cb21-616"><a href="#cb21-616"></a><span class="fu">### Distribución gamma transformada</span></span>
<span id="cb21-617"><a href="#cb21-617"></a></span>
<span id="cb21-618"><a href="#cb21-618"></a>:::: {.thmbox data-latex=""}</span>
<span id="cb21-619"><a href="#cb21-619"></a>::: {.definition #dgammatra  name="Distribución gamma transformada"}</span>
<span id="cb21-620"><a href="#cb21-620"></a>Una variable aleatoria $X$ a valores reales, sigue una distribución gamma transformada</span>
<span id="cb21-621"><a href="#cb21-621"></a>$X \rightsquigarrow GT( \alpha, \tau, \theta )$ de parámetros $\alpha &gt; 0, \tau &gt; 0, \theta &gt; 0$, </span>
<span id="cb21-622"><a href="#cb21-622"></a>si su función de distribución acumulada es de la siguiente forma:</span>
<span id="cb21-623"><a href="#cb21-623"></a>$$</span>
<span id="cb21-624"><a href="#cb21-624"></a>F_X( x ) = \frac{\tau}{\Gamma( \alpha )}</span>
<span id="cb21-625"><a href="#cb21-625"></a>\int\limits_{0}^x </span>
<span id="cb21-626"><a href="#cb21-626"></a>\frac{1}{u} \left( \frac{u}{\theta} \right)^{\alpha} \exp\left(-\left( \frac{u}{\theta} \right)^{\tau}\right)\ du</span>
<span id="cb21-627"><a href="#cb21-627"></a>$$</span>
<span id="cb21-628"><a href="#cb21-628"></a></span>
<span id="cb21-629"><a href="#cb21-629"></a>además se puede verificar que su densidad de probabilidad está dada por la función:</span>
<span id="cb21-630"><a href="#cb21-630"></a>$$</span>
<span id="cb21-631"><a href="#cb21-631"></a>f_X( x )</span>
<span id="cb21-632"><a href="#cb21-632"></a>= </span>
<span id="cb21-633"><a href="#cb21-633"></a>\left<span class="sc">\{</span></span>
<span id="cb21-634"><a href="#cb21-634"></a>\begin{array}{ll}</span>
<span id="cb21-635"><a href="#cb21-635"></a>0 &amp; \text{si}\ x \leq 0 <span class="sc">\\</span></span>
<span id="cb21-636"><a href="#cb21-636"></a>\frac{\tau}{x \Gamma( \alpha )} \left( \frac{x}{\theta} \right)^{\alpha} \exp\left(-\left( \frac{x}{\theta} \right)^{\tau}\right) &amp; \text{si}\ x &gt; 0</span>
<span id="cb21-637"><a href="#cb21-637"></a>\end{array}</span>
<span id="cb21-638"><a href="#cb21-638"></a>\right.</span>
<span id="cb21-639"><a href="#cb21-639"></a>$$</span>
<span id="cb21-640"><a href="#cb21-640"></a>$$</span>
<span id="cb21-641"><a href="#cb21-641"></a>\begin{split}</span>
<span id="cb21-642"><a href="#cb21-642"></a>\E<span class="co">[</span><span class="ot">X^k</span><span class="co">]</span> </span>
<span id="cb21-643"><a href="#cb21-643"></a>&amp; = \frac{\theta^k \Gamma\left( \alpha + \frac{k}{\tau}\right)}{\Gamma( \alpha )},</span>
<span id="cb21-644"><a href="#cb21-644"></a>\quad \text{si}\ k &gt; -\alpha \tau <span class="sc">\\</span></span>
<span id="cb21-645"><a href="#cb21-645"></a>\E<span class="co">[</span><span class="ot">X</span><span class="co">]</span></span>
<span id="cb21-646"><a href="#cb21-646"></a>&amp; = \frac{\theta \Gamma\left( \alpha + \frac{1}{\tau}\right)}{\Gamma( \alpha )},</span>
<span id="cb21-647"><a href="#cb21-647"></a>\quad \text{si}\ 1 &gt; -\alpha \tau <span class="sc">\\</span></span>
<span id="cb21-648"><a href="#cb21-648"></a>\V<span class="co">[</span><span class="ot">X</span><span class="co">]</span> </span>
<span id="cb21-649"><a href="#cb21-649"></a>&amp; = \frac{\theta^2 \Gamma\left( \alpha + \frac{2}{\tau}\right)}{\Gamma( \alpha )}</span>
<span id="cb21-650"><a href="#cb21-650"></a><span class="ss">- </span>\frac{\theta^2 \Gamma\left( \alpha + \frac{1}{\tau}\right)^2}{\Gamma( \alpha )^2}</span>
<span id="cb21-651"><a href="#cb21-651"></a>\end{split}</span>
<span id="cb21-652"><a href="#cb21-652"></a>$$</span>
<span id="cb21-653"><a href="#cb21-653"></a>:::</span>
<span id="cb21-654"><a href="#cb21-654"></a>::::</span>
<span id="cb21-655"><a href="#cb21-655"></a><span class="in">```{r l5c15}</span></span>
<span id="cb21-656"><a href="#cb21-656"></a><span class="in">alpha &lt;- 1</span></span>
<span id="cb21-657"><a href="#cb21-657"></a><span class="in">tau &lt;- 1</span></span>
<span id="cb21-658"><a href="#cb21-658"></a><span class="in">theta &lt;- 1</span></span>
<span id="cb21-659"><a href="#cb21-659"></a><span class="in">x &lt;- 3</span></span>
<span id="cb21-660"><a href="#cb21-660"></a><span class="in">m &lt;- 100</span></span>
<span id="cb21-661"><a href="#cb21-661"></a><span class="in">X &lt;- rtrgamma( n = m, shape1 = alpha, shape2 = tau, scale = theta ) # simular una muestra de tamaño m</span></span>
<span id="cb21-662"><a href="#cb21-662"></a><span class="in">fx &lt;- dtrgamma( x = x, shape1 = alpha, shape2 = tau, scale = theta ) # cálculo de la densidad f(x)</span></span>
<span id="cb21-663"><a href="#cb21-663"></a><span class="in">Fk &lt;- ptrgamma( q = x, shape1 = alpha, shape2 = tau, scale = theta ) # cálculo de probabilidad F(x)</span></span>
<span id="cb21-664"><a href="#cb21-664"></a><span class="in">```</span></span>
<span id="cb21-665"><a href="#cb21-665"></a></span>
<span id="cb21-666"><a href="#cb21-666"></a>En la familia gamma se incluyen las siguientes distribuciones:</span>
<span id="cb21-667"><a href="#cb21-667"></a></span>
<span id="cb21-668"><a href="#cb21-668"></a><span class="ss">1. </span>La distribución inversa gamma transformada, es decir es una familia estable por inversión</span>
<span id="cb21-669"><a href="#cb21-669"></a></span>
<span id="cb21-670"><a href="#cb21-670"></a><span class="ss">2. </span>La distribución gamma para $\alpha = n/2$ y $\theta = 2$</span>
<span id="cb21-671"><a href="#cb21-671"></a></span>
<span id="cb21-672"><a href="#cb21-672"></a><span class="ss">3. </span>La distribución inversa gamma</span>
<span id="cb21-673"><a href="#cb21-673"></a></span>
<span id="cb21-674"><a href="#cb21-674"></a><span class="ss">4. </span>La distribución de Weibull</span>
<span id="cb21-675"><a href="#cb21-675"></a></span>
<span id="cb21-676"><a href="#cb21-676"></a><span class="ss">5. </span>La distribución inversa de Weibull</span>
<span id="cb21-677"><a href="#cb21-677"></a></span>
<span id="cb21-678"><a href="#cb21-678"></a><span class="ss">6. </span>La distribución exponencial</span>
<span id="cb21-679"><a href="#cb21-679"></a></span>
<span id="cb21-680"><a href="#cb21-680"></a><span class="ss">7. </span>La distribución inversa exponencial</span>
<span id="cb21-681"><a href="#cb21-681"></a></span>
<span id="cb21-682"><a href="#cb21-682"></a><span class="co">&lt;!-------------------------------------------------------------------------------------------------&gt;</span></span>
<span id="cb21-683"><a href="#cb21-683"></a><span class="fu">### Distribución beta transformada</span></span>
<span id="cb21-684"><a href="#cb21-684"></a></span>
<span id="cb21-685"><a href="#cb21-685"></a>:::: {.thmbox data-latex=""}</span>
<span id="cb21-686"><a href="#cb21-686"></a>::: {.definition #dbetra  name="Distribución beta transformada"}</span>
<span id="cb21-687"><a href="#cb21-687"></a>Una variable aleatoria $X$ a valores reales, sigue una distribución beta transformada</span>
<span id="cb21-688"><a href="#cb21-688"></a>$X \rightsquigarrow BT( \alpha, \gamma, \tau, \theta )$ de parámetros $\alpha &gt; 0, \gamma &gt; 0, \tau &gt; 0, \theta &gt; 0$, </span>
<span id="cb21-689"><a href="#cb21-689"></a>si su función de distribución acumulada es de la siguiente forma:</span>
<span id="cb21-690"><a href="#cb21-690"></a>$$</span>
<span id="cb21-691"><a href="#cb21-691"></a>F_X( x ) = \frac{\Gamma(\alpha + \tau)}{\Gamma( \alpha ) \Gamma( \tau )}</span>
<span id="cb21-692"><a href="#cb21-692"></a>\int\limits_0^x</span>
<span id="cb21-693"><a href="#cb21-693"></a>\frac{\gamma \left( \frac{u}{\theta} \right)^{\gamma \tau}}{u\left( 1 + \left( \frac{u}{\theta} \right)^{\gamma}\right)^{\alpha + \tau}}\ du</span>
<span id="cb21-694"><a href="#cb21-694"></a>$$</span>
<span id="cb21-695"><a href="#cb21-695"></a></span>
<span id="cb21-696"><a href="#cb21-696"></a>además se puede verificar que su densidad de probabilidad está dada por la función:</span>
<span id="cb21-697"><a href="#cb21-697"></a>$$</span>
<span id="cb21-698"><a href="#cb21-698"></a>f_X( x )</span>
<span id="cb21-699"><a href="#cb21-699"></a>= \mathbf{1}_{[0,+\infty)}( x )</span>
<span id="cb21-700"><a href="#cb21-700"></a>\frac{\Gamma(\alpha + \tau)}{\Gamma( \alpha ) \Gamma( \tau )} </span>
<span id="cb21-701"><a href="#cb21-701"></a>\frac{ \gamma \left( \frac{x}{\theta} \right)^{\gamma \tau}}{x\left( 1 + \left( \frac{x}{\theta} \right)^{\gamma}\right)^{\alpha + \tau}}</span>
<span id="cb21-702"><a href="#cb21-702"></a>$$</span>
<span id="cb21-703"><a href="#cb21-703"></a></span>
<span id="cb21-704"><a href="#cb21-704"></a>$$</span>
<span id="cb21-705"><a href="#cb21-705"></a>\begin{split}</span>
<span id="cb21-706"><a href="#cb21-706"></a>\E<span class="co">[</span><span class="ot">X^k</span><span class="co">]</span> </span>
<span id="cb21-707"><a href="#cb21-707"></a>&amp; = \frac{\theta^k \Gamma\left( \tau + \frac{k}{\gamma}\right) \Gamma\left( \tau - \frac{k}{\gamma}\right)}{\Gamma( \alpha ) \Gamma( \tau )},</span>
<span id="cb21-708"><a href="#cb21-708"></a>\quad \text{si}\ -\tau \gamma &lt; k &lt; \tau \gamma <span class="sc">\\</span></span>
<span id="cb21-709"><a href="#cb21-709"></a>\E<span class="co">[</span><span class="ot">X</span><span class="co">]</span></span>
<span id="cb21-710"><a href="#cb21-710"></a>&amp; = \frac{\theta \Gamma\left( \tau + \frac{1}{\gamma}\right) \Gamma\left( \tau - \frac{1}{\gamma}\right)}{\Gamma( \alpha ) \Gamma( \tau )} <span class="sc">\\</span></span>
<span id="cb21-711"><a href="#cb21-711"></a>\V<span class="co">[</span><span class="ot">X</span><span class="co">]</span> </span>
<span id="cb21-712"><a href="#cb21-712"></a>&amp; = \frac{\theta^2 \Gamma\left( \tau + \frac{2}{\gamma}\right) \Gamma\left( \tau - \frac{2}{\gamma}\right)}{\Gamma( \alpha ) \Gamma( \tau )}</span>
<span id="cb21-713"><a href="#cb21-713"></a><span class="ss">- </span>\frac{\theta^2 \Gamma\left( \tau + \frac{1}{\gamma}\right)^2 \Gamma\left( \tau - \frac{1}{\gamma}\right)^2}{\Gamma( \alpha )^2 \Gamma( \tau )^2}</span>
<span id="cb21-714"><a href="#cb21-714"></a>\end{split}</span>
<span id="cb21-715"><a href="#cb21-715"></a>$$</span>
<span id="cb21-716"><a href="#cb21-716"></a>:::</span>
<span id="cb21-717"><a href="#cb21-717"></a>::::</span>
<span id="cb21-718"><a href="#cb21-718"></a><span class="in">```{r l5c16}</span></span>
<span id="cb21-719"><a href="#cb21-719"></a><span class="in">alpha &lt;- 1</span></span>
<span id="cb21-720"><a href="#cb21-720"></a><span class="in">gamma &lt;- 1</span></span>
<span id="cb21-721"><a href="#cb21-721"></a><span class="in">tau &lt;- 1</span></span>
<span id="cb21-722"><a href="#cb21-722"></a><span class="in">theta &lt;- 1</span></span>
<span id="cb21-723"><a href="#cb21-723"></a><span class="in">x &lt;- 3</span></span>
<span id="cb21-724"><a href="#cb21-724"></a><span class="in">m &lt;- 100</span></span>
<span id="cb21-725"><a href="#cb21-725"></a><span class="in">X &lt;- rtrbeta( n = m, shape1 = alpha, shape2 = gamma, shape3 = tau, scale = theta ) # simular una muestra de tamaño m</span></span>
<span id="cb21-726"><a href="#cb21-726"></a><span class="in">fx &lt;- dtrbeta( x = x, shape1 = alpha, shape2 = gamma, shape3 = tau, scale = theta ) # cálculo de la densidad f(x)</span></span>
<span id="cb21-727"><a href="#cb21-727"></a><span class="in">Fk &lt;- ptrbeta( q = x, shape1 = alpha, shape2 = gamma, shape3 = tau, scale = theta ) # cálculo de probabilidad F(x)</span></span>
<span id="cb21-728"><a href="#cb21-728"></a><span class="in">```</span></span>
<span id="cb21-729"><a href="#cb21-729"></a></span>
<span id="cb21-730"><a href="#cb21-730"></a>Dentro de la familia beta transformada se cuenta algunas distribuciones de probabilidad:</span>
<span id="cb21-731"><a href="#cb21-731"></a></span>
<span id="cb21-732"><a href="#cb21-732"></a><span class="ss">1. </span>La distribución de Burr para $\tau = 1$</span>
<span id="cb21-733"><a href="#cb21-733"></a></span>
<span id="cb21-734"><a href="#cb21-734"></a><span class="ss">2. </span>La distribución de log-logística para $\alpha = \tau = 1$</span>
<span id="cb21-735"><a href="#cb21-735"></a></span>
<span id="cb21-736"><a href="#cb21-736"></a><span class="ss">3. </span>La distribución de paralogística para $\alpha = \gamma, \tau = 1$</span>
<span id="cb21-737"><a href="#cb21-737"></a></span>
<span id="cb21-738"><a href="#cb21-738"></a><span class="ss">4. </span>La distribución de generalizada de Pareto para $\gamma = 1$</span>
<span id="cb21-739"><a href="#cb21-739"></a></span>
<span id="cb21-740"><a href="#cb21-740"></a><span class="ss">5. </span>La distribución de Pareto para $\gamma = \tau = 1$</span>
<span id="cb21-741"><a href="#cb21-741"></a></span>
<span id="cb21-742"><a href="#cb21-742"></a><span class="ss">6. </span>La distribución de inversa de Burr para $\alpha = 1$</span>
<span id="cb21-743"><a href="#cb21-743"></a></span>
<span id="cb21-744"><a href="#cb21-744"></a><span class="ss">7. </span>La distribución de inversa de Pareto para $\alpha = \gamma = 1$</span>
<span id="cb21-745"><a href="#cb21-745"></a></span>
<span id="cb21-746"><a href="#cb21-746"></a><span class="ss">8. </span>La distribución de inversa paralogística para $\alpha = 1, \gamma = \tau$</span>
<span id="cb21-747"><a href="#cb21-747"></a></span>
<span id="cb21-748"><a href="#cb21-748"></a>La distribución transformada gamma es un caso límite de la distribución transformada beta, cuando</span>
<span id="cb21-749"><a href="#cb21-749"></a>$\theta \rightarrow +\infty, \alpha \rightarrow +\infty$ y </span>
<span id="cb21-750"><a href="#cb21-750"></a>$\theta \alpha^{-\frac{1}{\gamma}} \rightarrow \xi$</span>
<span id="cb21-751"><a href="#cb21-751"></a></span>
<span id="cb21-752"><a href="#cb21-752"></a></span>
<span id="cb21-753"><a href="#cb21-753"></a><span class="co">&lt;!-------------------------------------------------------------------------------------------------&gt;</span></span>
<span id="cb21-754"><a href="#cb21-754"></a><span class="fu">## Estimación</span></span>
<span id="cb21-755"><a href="#cb21-755"></a></span>
<span id="cb21-756"><a href="#cb21-756"></a>En la práctica se observa la realización de una variable aleatoria $X$, es decir se tiene una </span>
<span id="cb21-757"><a href="#cb21-757"></a>muestra de la misma $X_1, \ldots, X_n$. Pero, no se dispone de la distribución $F$ o de la densidad </span>
<span id="cb21-758"><a href="#cb21-758"></a>$f$ que la describe. Como ya hemos mencionado, conociendo la distribución se puede inferir algunas</span>
<span id="cb21-759"><a href="#cb21-759"></a>propiedades sobre la variable. De ahí surge la necesidad de buscar la mejor distribución $F$ posible</span>
<span id="cb21-760"><a href="#cb21-760"></a>a partir de la muestra @est_math_bor, @KNight, @mathstat1, @funmathstat. </span>
<span id="cb21-761"><a href="#cb21-761"></a></span>
<span id="cb21-762"><a href="#cb21-762"></a>La estimación usualmente consiste en tratar de determinar la probabilidad $P_X$ asociada a $X$, o </span>
<span id="cb21-763"><a href="#cb21-763"></a>en su defecto su función de distribución acumulada. Bajo la consideración, usualmente, de alguna</span>
<span id="cb21-764"><a href="#cb21-764"></a>información adicional, se considera que para la medida de de probabilidad $P_X$ que caracteriza a la </span>
<span id="cb21-765"><a href="#cb21-765"></a>variable aleatoria $X$ está dentro de una familia probabilidades que dependen de un parámetro </span>
<span id="cb21-766"><a href="#cb21-766"></a>$\theta$ el cual pertenece a un conjunto $\Theta$, precisamente parametriza a la familia, es </span>
<span id="cb21-767"><a href="#cb21-767"></a>decir, para cada $\theta \in \Theta$, $P_{\theta}$ es una medida de probabilidad y para algún </span>
<span id="cb21-768"><a href="#cb21-768"></a>$\theta \in \Theta, P_X = P_{\theta}$. </span>
<span id="cb21-769"><a href="#cb21-769"></a></span>
<span id="cb21-770"><a href="#cb21-770"></a>Ya en la práctica se tiene una muestra, por lo regular finita, $X_1, \ldots, X_n$ de la variable</span>
<span id="cb21-771"><a href="#cb21-771"></a>aleatoria $X$ que se desea comprender. Es a partir de la muestra que se intenta crear un estimador </span>
<span id="cb21-772"><a href="#cb21-772"></a>$\tau$ el cual sea precisamente el "mejor" bajo un cierto criterio que aproxime a la probabilidad</span>
<span id="cb21-773"><a href="#cb21-773"></a>$P_X$, en términos algo más matemáticos $P_X \approx P_{\tau}$. Por lo regular, se propone un </span>
<span id="cb21-774"><a href="#cb21-774"></a>estimador $\tau$ como función de la muestra y su tamaño $\tau_n( X_1, \ldots, X_n ) = \tau( n, X_1, \ldots, X_n )$,</span>
<span id="cb21-775"><a href="#cb21-775"></a>en muchos casos se considera construir una función medible, que precisamente genere eventos </span>
<span id="cb21-776"><a href="#cb21-776"></a>observables. Es de notar que el estimador $\tau_n$ al depender de una muestra, la cual </span>
<span id="cb21-777"><a href="#cb21-777"></a>está constituida por variables aleatorias, como función de variables aleatorias se convierte también </span>
<span id="cb21-778"><a href="#cb21-778"></a>en una variable aleatoria al ser evaluada en estas.</span>
<span id="cb21-779"><a href="#cb21-779"></a></span>
<span id="cb21-780"><a href="#cb21-780"></a>Hay algunas propiedades deseables para una familia de estimadores $\mathcal{E}$, estas son:</span>
<span id="cb21-781"><a href="#cb21-781"></a></span>
<span id="cb21-782"><a href="#cb21-782"></a><span class="ss">1. </span>Consistencia</span>
<span id="cb21-783"><a href="#cb21-783"></a></span>
<span id="cb21-784"><a href="#cb21-784"></a><span class="ss">2. </span>Insesgamiento</span>
<span id="cb21-785"><a href="#cb21-785"></a></span>
<span id="cb21-786"><a href="#cb21-786"></a><span class="ss">3. </span>Eficiencia</span>
<span id="cb21-787"><a href="#cb21-787"></a></span>
<span id="cb21-788"><a href="#cb21-788"></a><span class="ss">4. </span>Suficiencia</span>
<span id="cb21-789"><a href="#cb21-789"></a></span>
<span id="cb21-790"><a href="#cb21-790"></a>Para, cada adjuntamos sus respectivas definiciones</span>
<span id="cb21-791"><a href="#cb21-791"></a></span>
<span id="cb21-792"><a href="#cb21-792"></a>:::: {.thmbox data-latex=""}</span>
<span id="cb21-793"><a href="#cb21-793"></a>::: {.definition #destconsis  name="Estimadores consistentes"}</span>
<span id="cb21-794"><a href="#cb21-794"></a>Consideramos una variable aleatoria $X$, cuya medida de probabilidad asociada $P_X$ pertenece a una </span>
<span id="cb21-795"><a href="#cb21-795"></a>familia de distribuciones de probabilidad $<span class="sc">\{</span>P_\theta<span class="sc">\}</span>_{\theta \in \Theta}$, esto quiere decir que</span>
<span id="cb21-796"><a href="#cb21-796"></a>$P_X$ está determinada por algún $\theta \in \Theta$, i.e. $P_X = P_{\theta}$. Entonces,</span>
<span id="cb21-797"><a href="#cb21-797"></a>la familia de estimadores $\mathcal{E} = <span class="sc">\{</span> \tau_n <span class="sc">\}</span>_n$ es **consistente** para la familia de </span>
<span id="cb21-798"><a href="#cb21-798"></a>probabilidades $<span class="sc">\{</span>P_\theta<span class="sc">\}</span>_{\theta \in \Theta}$ si para cualquier $n \in \N$ y muestra </span>
<span id="cb21-799"><a href="#cb21-799"></a>$X_1, \ldots, X_n$ de $X$ el estimador $\tau_n( X_1,\ldots,X_n)$ converge en probabilidad a </span>
<span id="cb21-800"><a href="#cb21-800"></a>$\theta$. Para todo $\varepsilon &gt; 0$</span>
<span id="cb21-801"><a href="#cb21-801"></a>$$</span>
<span id="cb21-802"><a href="#cb21-802"></a>\underset{n \rightarrow +\infty}{\lim} P_{\theta}\left( \left| \tau_n( X_1,\ldots,X_n) - \theta \right| &gt; \varepsilon\right) = 0</span>
<span id="cb21-803"><a href="#cb21-803"></a>$$</span>
<span id="cb21-804"><a href="#cb21-804"></a>también se dice que la familia de estimadores es convergente.</span>
<span id="cb21-805"><a href="#cb21-805"></a></span>
<span id="cb21-806"><a href="#cb21-806"></a>Decimos que **converge fuertemente** si la convergencia de la familia de estimadores </span>
<span id="cb21-807"><a href="#cb21-807"></a>$\mathcal{E} = <span class="sc">\{</span> \tau_n <span class="sc">\}</span>_n$ al parámetro $\theta$ se da **casi seguramente** o en </span>
<span id="cb21-808"><a href="#cb21-808"></a>**casi todas partes**. Esto en término de límites implica que:</span>
<span id="cb21-809"><a href="#cb21-809"></a>$$</span>
<span id="cb21-810"><a href="#cb21-810"></a>P_{\theta}\left( \underset{n \rightarrow +\infty}{\lim} </span>
<span id="cb21-811"><a href="#cb21-811"></a>\tau_n\left( X_1, \ldots, X_n \right) = \theta \right) = 1</span>
<span id="cb21-812"><a href="#cb21-812"></a>$$</span>
<span id="cb21-813"><a href="#cb21-813"></a>En otras palabras, la probabilidad de que el límite del estimador $\tau_n( X_1, \ldots, X_n )$ </span>
<span id="cb21-814"><a href="#cb21-814"></a>sea igual a $\theta$ es $1$, salvo un conjunto de medida nula para $P_{\theta}$. De ahí resulta </span>
<span id="cb21-815"><a href="#cb21-815"></a>la terminología de convergencia casi segura o convergencia en casi todas partes.</span>
<span id="cb21-816"><a href="#cb21-816"></a>:::</span>
<span id="cb21-817"><a href="#cb21-817"></a>::::</span>
<span id="cb21-818"><a href="#cb21-818"></a></span>
<span id="cb21-819"><a href="#cb21-819"></a>:::: {.thmbox data-latex=""}</span>
<span id="cb21-820"><a href="#cb21-820"></a>::: {.definition #destinsesg  name="Estimadores insesgados"}</span>
<span id="cb21-821"><a href="#cb21-821"></a>Bajo el mismo contexto de la definición anterior. Decimos que el estimador $\tau_n$ es </span>
<span id="cb21-822"><a href="#cb21-822"></a>insesgado si precisamente su esperanza es igual al parámetro a estimar $\theta \in \Theta$.</span>
<span id="cb21-823"><a href="#cb21-823"></a>$$</span>
<span id="cb21-824"><a href="#cb21-824"></a>\E\left<span class="co">[</span><span class="ot"> \tau_n( X_1,\ldots,X_n) \right</span><span class="co">]</span> = \theta</span>
<span id="cb21-825"><a href="#cb21-825"></a>$$</span>
<span id="cb21-826"><a href="#cb21-826"></a>La familia de estimadores $\mathcal{E} = <span class="sc">\{</span> \tau_n <span class="sc">\}</span>_n$ será insesgada si para cada $n$ se </span>
<span id="cb21-827"><a href="#cb21-827"></a>satisface lo anterior.</span>
<span id="cb21-828"><a href="#cb21-828"></a>:::</span>
<span id="cb21-829"><a href="#cb21-829"></a>::::</span>
<span id="cb21-830"><a href="#cb21-830"></a></span>
<span id="cb21-831"><a href="#cb21-831"></a>El siguiente teorema es de importancia para caracterizar una familia de estimadores consistentes.</span>
<span id="cb21-832"><a href="#cb21-832"></a></span>
<span id="cb21-833"><a href="#cb21-833"></a>:::: {.thmbox data-latex=""}</span>
<span id="cb21-834"><a href="#cb21-834"></a>::: {.theorem #sufestconsis }</span>
<span id="cb21-835"><a href="#cb21-835"></a>Una familia de estimadores $<span class="sc">\{</span> \tau_n <span class="sc">\}</span>_n$ para la cual se cumplen los siguientes límites</span>
<span id="cb21-836"><a href="#cb21-836"></a></span>
<span id="cb21-837"><a href="#cb21-837"></a><span class="ss">1. </span>$\E\left<span class="co">[</span><span class="ot"> \tau_n( X_1,\ldots,X_n) \right</span><span class="co">]</span> \rightarrow \theta$, conforme $n \rightarrow +\infty$,</span>
<span id="cb21-838"><a href="#cb21-838"></a></span>
<span id="cb21-839"><a href="#cb21-839"></a><span class="ss">2. </span>$\V\left<span class="co">[</span><span class="ot"> \tau_n( X_1,\ldots,X_n) \right</span><span class="co">]</span> \rightarrow 0$, conforme $n \rightarrow +\infty$.</span>
<span id="cb21-840"><a href="#cb21-840"></a></span>
<span id="cb21-841"><a href="#cb21-841"></a>Entonces, la familia $<span class="sc">\{</span> \tau_n <span class="sc">\}</span>_n$ es consistente como estimadores de $\theta$.</span>
<span id="cb21-842"><a href="#cb21-842"></a>:::</span>
<span id="cb21-843"><a href="#cb21-843"></a>::::</span>
<span id="cb21-844"><a href="#cb21-844"></a></span>
<span id="cb21-845"><a href="#cb21-845"></a>:::: {.thmbox data-latex=""}</span>
<span id="cb21-846"><a href="#cb21-846"></a>::: {.definition #destefi  name="Estimador más eficiente"}</span>
<span id="cb21-847"><a href="#cb21-847"></a>De igual forma en el contexto anterior. En una familia de estimadores consistentes para estimar</span>
<span id="cb21-848"><a href="#cb21-848"></a>un parámetro, si entre estos estimadores existe uno para el cual su varianza sea mínima, entonces</span>
<span id="cb21-849"><a href="#cb21-849"></a>si existe tal estimador se dice que este es el **estimador más eficiente**. Sin $\mathcal{E}$ es la</span>
<span id="cb21-850"><a href="#cb21-850"></a>familia de estimadores y $\xi$ es el estimador más eficiente, entonces para cualquier otro </span>
<span id="cb21-851"><a href="#cb21-851"></a>estimador $\tau \in \mathcal{E}$, se tiene la siguiente desigualdad</span>
<span id="cb21-852"><a href="#cb21-852"></a>$$</span>
<span id="cb21-853"><a href="#cb21-853"></a>\V\left<span class="co">[</span><span class="ot"> \xi \right</span><span class="co">]</span> \leq \V\left<span class="co">[</span><span class="ot"> \tau \right</span><span class="co">]</span></span>
<span id="cb21-854"><a href="#cb21-854"></a>$$</span>
<span id="cb21-855"><a href="#cb21-855"></a>la **eficiencia** $E$ de los estimadores en la familia $\mathcal{E}$ está dada por</span>
<span id="cb21-856"><a href="#cb21-856"></a>$$</span>
<span id="cb21-857"><a href="#cb21-857"></a>0 \leq E( \tau ) = \frac{\V\left<span class="co">[</span><span class="ot"> \xi \right</span><span class="co">]</span>}{\V\left<span class="co">[</span><span class="ot"> \tau \right</span><span class="co">]</span>} \leq 1</span>
<span id="cb21-858"><a href="#cb21-858"></a>$$</span>
<span id="cb21-859"><a href="#cb21-859"></a>Si además el estimador más eficiente $\xi$ es insesgado se dirá que este es un</span>
<span id="cb21-860"><a href="#cb21-860"></a>**estimador de varianza mínima insesgado**, o de forma corta **MVUE** por el término en inglés </span>
<span id="cb21-861"><a href="#cb21-861"></a>(minimum variance unbiased estimator).</span>
<span id="cb21-862"><a href="#cb21-862"></a>:::</span>
<span id="cb21-863"><a href="#cb21-863"></a>::::</span>
<span id="cb21-864"><a href="#cb21-864"></a></span>
<span id="cb21-865"><a href="#cb21-865"></a>:::: {.thmbox data-latex=""}</span>
<span id="cb21-866"><a href="#cb21-866"></a>::: {.theorem #thmvueunicity }</span>
<span id="cb21-867"><a href="#cb21-867"></a>Si en una familia de estimadores $\mathcal{E}$ tenemos dos estimadores MVUE para el mismo parámetro</span>
<span id="cb21-868"><a href="#cb21-868"></a>$\theta \in \Theta$, entonces estos dos estimadores son iguales en casi todas partes o lo que </span>
<span id="cb21-869"><a href="#cb21-869"></a>es lo mismo solo son diferentes en un conjunto de medida nula.</span>
<span id="cb21-870"><a href="#cb21-870"></a>:::</span>
<span id="cb21-871"><a href="#cb21-871"></a>::::</span>
<span id="cb21-872"><a href="#cb21-872"></a></span>
<span id="cb21-873"><a href="#cb21-873"></a>:::: {.thmbox data-latex=""}</span>
<span id="cb21-874"><a href="#cb21-874"></a>::: {.definition #destsufic  name="Estimador suficiente"}</span>
<span id="cb21-875"><a href="#cb21-875"></a>Dada una muestra $X_1, \ldots, X_n$ de la variable aleatoria $X$. Un estimador </span>
<span id="cb21-876"><a href="#cb21-876"></a>$\tau\left( X_1, \ldots, X_n \right)$ se dice un **estimador suficiente** para el parámetro </span>
<span id="cb21-877"><a href="#cb21-877"></a>$\theta \in \Theta$, si la distribución condicionada de la muestra $X_1, \ldots X_n$ dado el </span>
<span id="cb21-878"><a href="#cb21-878"></a>estimador $\tau$ es independiente del parámetro $\theta$.</span>
<span id="cb21-879"><a href="#cb21-879"></a>:::</span>
<span id="cb21-880"><a href="#cb21-880"></a>::::</span>
<span id="cb21-881"><a href="#cb21-881"></a></span>
<span id="cb21-882"><a href="#cb21-882"></a>El resultado a continuación es una caracterización del criterio de suficiencia, para una demostración</span>
<span id="cb21-883"><a href="#cb21-883"></a>se puede consultar @stat_theo_infer, @funmathstat.</span>
<span id="cb21-884"><a href="#cb21-884"></a></span>
<span id="cb21-885"><a href="#cb21-885"></a>:::: {.thmbox data-latex=""}</span>
<span id="cb21-886"><a href="#cb21-886"></a>::: {.theorem #thfactorsufic name="Teorema de factorización" }</span>
<span id="cb21-887"><a href="#cb21-887"></a>Un estimador $\tau$ es suficiente para el parámetro $\theta$ si y solo si la densidad conjunta</span>
<span id="cb21-888"><a href="#cb21-888"></a>de la muestra $X_1, \ldots, X_n$ puede ser expresada en la forma.</span>
<span id="cb21-889"><a href="#cb21-889"></a>$$</span>
<span id="cb21-890"><a href="#cb21-890"></a>f\left( x_1, \ldots, x_n \right) </span>
<span id="cb21-891"><a href="#cb21-891"></a>= g\left( \theta, \tau\left( x_1, \ldots, x_n \right) \right) h\left( x_1, \ldots, x_n \right)</span>
<span id="cb21-892"><a href="#cb21-892"></a>$$</span>
<span id="cb21-893"><a href="#cb21-893"></a>:::</span>
<span id="cb21-894"><a href="#cb21-894"></a>::::</span>
<span id="cb21-895"><a href="#cb21-895"></a></span>
<span id="cb21-896"><a href="#cb21-896"></a>:::: {.thmbox data-latex=""}</span>
<span id="cb21-897"><a href="#cb21-897"></a>::: {.definition #dfamicomp  name="Familia completa"}</span>
<span id="cb21-898"><a href="#cb21-898"></a>Data un estimador $\tau$ que depende de una muestra aleatoria $X_1, \ldots, X_n$ y una familia de </span>
<span id="cb21-899"><a href="#cb21-899"></a>medidas de probabilidad $<span class="sc">\{</span> P_\theta <span class="sc">\}</span>_{\theta \in \Theta}$, para cada $\theta \in \Theta$, se </span>
<span id="cb21-900"><a href="#cb21-900"></a>puede construir la medida $P_{\tau,\theta}$ asociada a la variable aleatoria $\tau(X_1, \ldots,X_n)$</span>
<span id="cb21-901"><a href="#cb21-901"></a>y al parámetro $\theta$ que determina la medida $P_\theta$. Para cualquier evento $A \in \F$.</span>
<span id="cb21-902"><a href="#cb21-902"></a>$$</span>
<span id="cb21-903"><a href="#cb21-903"></a>P_{\tau,\theta}( A ) = P_\theta\left( \tau\left(X_1, \ldots,X_n\right) \in A \right)</span>
<span id="cb21-904"><a href="#cb21-904"></a>$$</span>
<span id="cb21-905"><a href="#cb21-905"></a></span>
<span id="cb21-906"><a href="#cb21-906"></a>Así, la familia de medidas de probabilidad $<span class="sc">\{</span> P_{\tau,\theta} <span class="sc">\}</span>_{\theta \in \Theta}$ se dice </span>
<span id="cb21-907"><a href="#cb21-907"></a>**completa** si se satisface la siguiente implicación. Si una función $h : \R \longrightarrow \R$, </span>
<span id="cb21-908"><a href="#cb21-908"></a>tiene esperanza nula para cualquier esperanza tomada bajo la medida $P_{\tau, \theta}$, entonces la </span>
<span id="cb21-909"><a href="#cb21-909"></a>función $h$ es nula en casi todas partes para toda medida $P_\theta$. De forma más compacta.</span>
<span id="cb21-910"><a href="#cb21-910"></a>$$</span>
<span id="cb21-911"><a href="#cb21-911"></a>\forall P_{\tau,\theta}, \E_{P_{\tau,\theta}}\left<span class="co">[</span><span class="ot"> h\left( \tau\left( X_1,\ldots, X_n \right) \right) \right</span><span class="co">]</span> = 0 </span>
<span id="cb21-912"><a href="#cb21-912"></a>\Longrightarrow </span>
<span id="cb21-913"><a href="#cb21-913"></a>\forall P_\theta, P_\theta\left( h\left( \tau\left( X_1,\ldots, X_n \right) \right) = 0 \right) = 1</span>
<span id="cb21-914"><a href="#cb21-914"></a>$$</span>
<span id="cb21-915"><a href="#cb21-915"></a>Por extensión si sucede esta implicación, se dice que $\tau$ es un **estimador completo** respecto </span>
<span id="cb21-916"><a href="#cb21-916"></a>de la familia $<span class="sc">\{</span> P_\theta <span class="sc">\}</span>_{\theta \in \Theta}$.</span>
<span id="cb21-917"><a href="#cb21-917"></a>:::</span>
<span id="cb21-918"><a href="#cb21-918"></a>::::</span>
<span id="cb21-919"><a href="#cb21-919"></a></span>
<span id="cb21-920"><a href="#cb21-920"></a>Para ello se ha formulado diferentes aproximaciones, entre las cuales citamos las siguientes:</span>
<span id="cb21-921"><a href="#cb21-921"></a></span>
<span id="cb21-922"><a href="#cb21-922"></a><span class="ss">1. </span>Método de sustitución,</span>
<span id="cb21-923"><a href="#cb21-923"></a></span>
<span id="cb21-924"><a href="#cb21-924"></a><span class="ss">2. </span>Método de los momentos,</span>
<span id="cb21-925"><a href="#cb21-925"></a></span>
<span id="cb21-926"><a href="#cb21-926"></a><span class="ss">3. </span>Método de la distancia mínima,</span>
<span id="cb21-927"><a href="#cb21-927"></a></span>
<span id="cb21-928"><a href="#cb21-928"></a>    3.1 Método de mínimos cuadrados,</span>
<span id="cb21-929"><a href="#cb21-929"></a>  </span>
<span id="cb21-930"><a href="#cb21-930"></a>    3.2 Método de mínimo Chi-cuadrado,</span>
<span id="cb21-931"><a href="#cb21-931"></a>  </span>
<span id="cb21-932"><a href="#cb21-932"></a>    3.3 Método de varianza mínima,</span>
<span id="cb21-933"><a href="#cb21-933"></a>  </span>
<span id="cb21-934"><a href="#cb21-934"></a>    3.4 Método de minimización de la divergencia de Kullback–Leibler (máxima verosimilitud).</span>
<span id="cb21-935"><a href="#cb21-935"></a></span>
<span id="cb21-936"><a href="#cb21-936"></a><span class="ss">4. </span>Método de máxima verosimilitud,</span>
<span id="cb21-937"><a href="#cb21-937"></a></span>
<span id="cb21-938"><a href="#cb21-938"></a><span class="ss">5. </span>Método de Stein.</span>
<span id="cb21-939"><a href="#cb21-939"></a></span>
<span id="cb21-940"><a href="#cb21-940"></a><span class="co">&lt;!-------------------------------------------------------------------------------------------------&gt;</span></span>
<span id="cb21-941"><a href="#cb21-941"></a><span class="fu">### Método de sustitución</span></span>
<span id="cb21-942"><a href="#cb21-942"></a></span>
<span id="cb21-943"><a href="#cb21-943"></a>Entonces, se parte de suponer que existe funcional $G$ actuando sobre el conjunto de medidas de </span>
<span id="cb21-944"><a href="#cb21-944"></a>probabilidad $\mathcal{P}$ que contiene al conjunto</span>
<span id="cb21-945"><a href="#cb21-945"></a>$P( \Theta ) = <span class="sc">\{</span> P_{\theta} \mid \theta \in \Theta<span class="sc">\}</span> \subset \mathcal{P}$ y que toma valores </span>
<span id="cb21-946"><a href="#cb21-946"></a>en $\Theta$, i.e. $G: \mathcal{P} \longrightarrow \Theta$. De tal forma que $P_{\theta}$ es </span>
<span id="cb21-947"><a href="#cb21-947"></a>invariante, es decir:</span>
<span id="cb21-948"><a href="#cb21-948"></a>$$</span>
<span id="cb21-949"><a href="#cb21-949"></a>G( P_{\theta} ) = \theta,\quad \forall \theta \in \Theta</span>
<span id="cb21-950"><a href="#cb21-950"></a>$$</span>
<span id="cb21-951"><a href="#cb21-951"></a></span>
<span id="cb21-952"><a href="#cb21-952"></a>Así se construye un **estimador por el método de sustitución** si a partir de la medida de </span>
<span id="cb21-953"><a href="#cb21-953"></a>probabilidad empírica $P_n$ construida con una muestra $X_1, \ldots, X_n$ de la variable</span>
<span id="cb21-954"><a href="#cb21-954"></a>aleatoria $X$, se toma como parámetro el dado por:</span>
<span id="cb21-955"><a href="#cb21-955"></a>$$</span>
<span id="cb21-956"><a href="#cb21-956"></a>\widehat{\theta} = G\left( P_n \right)</span>
<span id="cb21-957"><a href="#cb21-957"></a>$$</span>
<span id="cb21-958"><a href="#cb21-958"></a></span>
<span id="cb21-959"><a href="#cb21-959"></a>En otras palabras se sustituye el parámetro $\theta$ por $\widehat{\theta}$. Esto implica que </span>
<span id="cb21-960"><a href="#cb21-960"></a>se aproxima la medida $P_X$ que caracteriza a $X$, con la aproximación</span>
<span id="cb21-961"><a href="#cb21-961"></a>$P_X \approx P_{\widehat{\theta}}$.</span>
<span id="cb21-962"><a href="#cb21-962"></a></span>
<span id="cb21-963"><a href="#cb21-963"></a>Es de notar que a priori no se hace establece ninguna medida de la calidad de la aproximación, </span>
<span id="cb21-964"><a href="#cb21-964"></a>para ello hay que adjuntar algunos otros criterios que caracterizan un buen tipo de estimador.</span>
<span id="cb21-965"><a href="#cb21-965"></a></span>
<span id="cb21-966"><a href="#cb21-966"></a>En otras ocasiones a partir de la muestra se define un estimador del parámetro $\theta \in \Theta$</span>
<span id="cb21-967"><a href="#cb21-967"></a>a partir de una familia de funciones medibles que dependen directamente de la </span>
<span id="cb21-968"><a href="#cb21-968"></a>muestra $X_1, \ldots, X_n$ y su tamaño $n$, i.e. una función $\theta_n( X_1, \ldots, X_n )$. Se </span>
<span id="cb21-969"><a href="#cb21-969"></a>puede considerar el caso anterior como un caso en particular de este tipo de funciones, ya que </span>
<span id="cb21-970"><a href="#cb21-970"></a>se puede definir $\theta_n( X_1, \ldots, X_n ) = G( P_n )$, pero hay que tener cuidado que $G$ es </span>
<span id="cb21-971"><a href="#cb21-971"></a>un funcional y como tal puede resultar una función que no es medible.</span>
<span id="cb21-972"><a href="#cb21-972"></a></span>
<span id="cb21-973"><a href="#cb21-973"></a><span class="co">&lt;!-------------------------------------------------------------------------------------------------&gt;</span></span>
<span id="cb21-974"><a href="#cb21-974"></a><span class="fu">### Método de momentos</span></span>
<span id="cb21-975"><a href="#cb21-975"></a></span>
<span id="cb21-976"><a href="#cb21-976"></a>Un estimador de momento se construye a partir de una relación función entre la media de una función</span>
<span id="cb21-977"><a href="#cb21-977"></a>medible $g$ y el parámetro $\theta \in \Theta$ que caracteriza a la distribución o medida de </span>
<span id="cb21-978"><a href="#cb21-978"></a>probabilidad de la variable aleatoria $X$. Es decir, existen funciones $g$ y $m$ a valores reales</span>
<span id="cb21-979"><a href="#cb21-979"></a>de tal forma que</span>
<span id="cb21-980"><a href="#cb21-980"></a>$$</span>
<span id="cb21-981"><a href="#cb21-981"></a>m( \theta ) </span>
<span id="cb21-982"><a href="#cb21-982"></a>= \E_{P_X} \left<span class="co">[</span><span class="ot"> g( X ) \right</span><span class="co">]</span> </span>
<span id="cb21-983"><a href="#cb21-983"></a>= \int\limits_{\R} g( x ) dP_X( x )</span>
<span id="cb21-984"><a href="#cb21-984"></a>= \int\limits_{\R} g( x ) dP_{\theta}( x )</span>
<span id="cb21-985"><a href="#cb21-985"></a>$$</span>
<span id="cb21-986"><a href="#cb21-986"></a></span>
<span id="cb21-987"><a href="#cb21-987"></a>Si de alguna forma se puede invertir $m$, de tal forma que se pueda determinar $\theta$, en tal </span>
<span id="cb21-988"><a href="#cb21-988"></a>caso se pude utilizar un **estimador por momentos** a partir de una muestra de la variable aleatoria</span>
<span id="cb21-989"><a href="#cb21-989"></a>$X_1, \ldots, X_n$</span>
<span id="cb21-990"><a href="#cb21-990"></a>$$</span>
<span id="cb21-991"><a href="#cb21-991"></a>\widehat{\theta} </span>
<span id="cb21-992"><a href="#cb21-992"></a>= \theta_n\left( X_1, \ldots, X_n \right)</span>
<span id="cb21-993"><a href="#cb21-993"></a>= m^{-1}\left( \overline{g} \right)</span>
<span id="cb21-994"><a href="#cb21-994"></a>$$</span>
<span id="cb21-995"><a href="#cb21-995"></a>donde</span>
<span id="cb21-996"><a href="#cb21-996"></a>$$</span>
<span id="cb21-997"><a href="#cb21-997"></a>\overline{g}</span>
<span id="cb21-998"><a href="#cb21-998"></a>= \int\limits_{\R} g( x ) dP_n( x )</span>
<span id="cb21-999"><a href="#cb21-999"></a>= \frac{1}{n}\sum\limits_{i=1}^n g\left( X_i \right)</span>
<span id="cb21-1000"><a href="#cb21-1000"></a>$$</span>
<span id="cb21-1001"><a href="#cb21-1001"></a></span>
<span id="cb21-1002"><a href="#cb21-1002"></a>Si en caso $\overline{g}$ está fuera de la imagen de $m$, $\overline{g} \notin m( \Theta )$, la </span>
<span id="cb21-1003"><a href="#cb21-1003"></a>inversión no sería posible. Para resolver este problema se puede recurrir a una distancia $d$ y </span>
<span id="cb21-1004"><a href="#cb21-1004"></a>buscar $\hat{g}$ tal que minimize la distancia a $m( \Theta )$, i.e. </span>
<span id="cb21-1005"><a href="#cb21-1005"></a>$\hat{g} = \underset{h \in m( \Theta )}{\arginf}\ d( \overline{g}, h)$. Una vez determinado </span>
<span id="cb21-1006"><a href="#cb21-1006"></a>$\hat{g}$ se toma como estimador de momentos su inversa con $m$, i.e. </span>
<span id="cb21-1007"><a href="#cb21-1007"></a>$\widehat{\theta} = m^{-1}( \hat{g} )$.</span>
<span id="cb21-1008"><a href="#cb21-1008"></a></span>
<span id="cb21-1009"><a href="#cb21-1009"></a>Es de notar que en el caso anterior, la selección de la mejor distancia $d$ no es para nada </span>
<span id="cb21-1010"><a href="#cb21-1010"></a>evidente y puede ser un problema tanto o más difícil que la misma estimación del parámetro </span>
<span id="cb21-1011"><a href="#cb21-1011"></a>$\theta$ o la inversión de $m$. </span>
<span id="cb21-1012"><a href="#cb21-1012"></a></span>
<span id="cb21-1013"><a href="#cb21-1013"></a>La estimación por momentos en los casos más elaborados lleva a buscar la solución de problemas no </span>
<span id="cb21-1014"><a href="#cb21-1014"></a>lineales, que no suelen ser estables y que deben estar bien definidos para proveer una solución </span>
<span id="cb21-1015"><a href="#cb21-1015"></a>única.</span>
<span id="cb21-1016"><a href="#cb21-1016"></a></span>
<span id="cb21-1017"><a href="#cb21-1017"></a><span class="co">&lt;!-------------------------------------------------------------------------------------------------&gt;</span></span>
<span id="cb21-1018"><a href="#cb21-1018"></a><span class="fu">### Método de la distancia mínima</span></span>
<span id="cb21-1019"><a href="#cb21-1019"></a></span>
<span id="cb21-1020"><a href="#cb21-1020"></a>Este método requiere de la definición de una distancia sobre el espacio de distribuciones de </span>
<span id="cb21-1021"><a href="#cb21-1021"></a>probabilidad $\mathcal{P}$, i.e. una función $d : \mathcal{P} \times \mathcal{P} \longrightarrow \R_+$</span>
<span id="cb21-1022"><a href="#cb21-1022"></a>que satisface las siguientes propiedades:</span>
<span id="cb21-1023"><a href="#cb21-1023"></a></span>
<span id="cb21-1024"><a href="#cb21-1024"></a><span class="ss">1. </span>Propiedad de simetría, para cualquier $P, Q \in \mathcal{P}$, $d( P, Q ) = d( Q, P )$,</span>
<span id="cb21-1025"><a href="#cb21-1025"></a></span>
<span id="cb21-1026"><a href="#cb21-1026"></a><span class="ss">2. </span>Para cualquier $P \in \mathcal{P}$, $d( P, P ) = 0$.</span>
<span id="cb21-1027"><a href="#cb21-1027"></a></span>
<span id="cb21-1028"><a href="#cb21-1028"></a><span class="ss">3. </span>Desigualdad triangular, para cualquier $P,Q,R \in \mathcal{P}$</span>
<span id="cb21-1029"><a href="#cb21-1029"></a>$$</span>
<span id="cb21-1030"><a href="#cb21-1030"></a>d( P, Q ) \leq d( P, R ) + d( R, Q )</span>
<span id="cb21-1031"><a href="#cb21-1031"></a>$$</span>
<span id="cb21-1032"><a href="#cb21-1032"></a></span>
<span id="cb21-1033"><a href="#cb21-1033"></a>A partir de la medida empírica de probabilidad $P_n$, se busca una probabilidad $P_{\theta}$ que </span>
<span id="cb21-1034"><a href="#cb21-1034"></a>minimize la distancia medida con $d$. Así resulta el estimador con el **método de distancia mínima**</span>
<span id="cb21-1035"><a href="#cb21-1035"></a>$$</span>
<span id="cb21-1036"><a href="#cb21-1036"></a>\widehat{\theta} = \underset{\theta \in \Theta}{\arginf} d\left( P_{\theta}, P_n \right)</span>
<span id="cb21-1037"><a href="#cb21-1037"></a>$$</span>
<span id="cb21-1038"><a href="#cb21-1038"></a></span>
<span id="cb21-1039"><a href="#cb21-1039"></a>Entra las distancias que se puede considerar tenemos las siguientes</span>
<span id="cb21-1040"><a href="#cb21-1040"></a></span>
<span id="cb21-1041"><a href="#cb21-1041"></a><span class="ss">- </span>Distancia del supremo entre las distribuciones acumuladas correspondientes</span>
<span id="cb21-1042"><a href="#cb21-1042"></a>$$</span>
<span id="cb21-1043"><a href="#cb21-1043"></a>d(P,Q) = \underset{x}{\sup} \left| F_P( x ) - F_Q( x ) \right|</span>
<span id="cb21-1044"><a href="#cb21-1044"></a>$$</span>
<span id="cb21-1045"><a href="#cb21-1045"></a></span>
<span id="cb21-1046"><a href="#cb21-1046"></a><span class="ss">- </span>Distancia cuadrática entre las distribuciones acumuladas correspondientes</span>
<span id="cb21-1047"><a href="#cb21-1047"></a>$$</span>
<span id="cb21-1048"><a href="#cb21-1048"></a>d( P, Q ) = \int\limits_{\R} \left( F_P( x ) - F_Q( x ) \right)^2\ dF_Q( x )</span>
<span id="cb21-1049"><a href="#cb21-1049"></a>$$</span>
<span id="cb21-1050"><a href="#cb21-1050"></a></span>
<span id="cb21-1051"><a href="#cb21-1051"></a><span class="ss">- </span>Distancia de Wasserstein, para $p &gt; 1$</span>
<span id="cb21-1052"><a href="#cb21-1052"></a>$$</span>
<span id="cb21-1053"><a href="#cb21-1053"></a>d( P, Q )</span>
<span id="cb21-1054"><a href="#cb21-1054"></a>= W_p( P, Q )</span>
<span id="cb21-1055"><a href="#cb21-1055"></a>= \underset{(X,Y) \rightsquigarrow \mu \in \Gamma( P, Q )}{\inf} \E_{\mu}\left<span class="co">[</span><span class="ot"> |X - Y|^p \right</span><span class="co">]</span>^{\frac{1}{p}}</span>
<span id="cb21-1056"><a href="#cb21-1056"></a>$$</span>
<span id="cb21-1057"><a href="#cb21-1057"></a>donde</span>
<span id="cb21-1058"><a href="#cb21-1058"></a>$$</span>
<span id="cb21-1059"><a href="#cb21-1059"></a>\Gamma( P, Q ) = \left<span class="sc">\{</span> \mu \middle| \text{$\mu$ es medida de probabilidad sobre $\Omega \times \Omega$, </span>
<span id="cb21-1060"><a href="#cb21-1060"></a>cuyas distribuciones marginales son $P$ y $Q$}\right<span class="sc">\}</span></span>
<span id="cb21-1061"><a href="#cb21-1061"></a>$$</span>
<span id="cb21-1062"><a href="#cb21-1062"></a></span>
<span id="cb21-1063"><a href="#cb21-1063"></a><span class="co">&lt;!-------------------------------------------------------------------------------------------------&gt;</span></span>
<span id="cb21-1064"><a href="#cb21-1064"></a><span class="fu">###  Método de maximización de la verosimilitud</span></span>
<span id="cb21-1065"><a href="#cb21-1065"></a></span>
<span id="cb21-1066"><a href="#cb21-1066"></a>La estimación de verosimilitud parte de asumir que se observan una cierta cantidad de eventos </span>
<span id="cb21-1067"><a href="#cb21-1067"></a>independientes $B_1, \ldots, B_n$, relacionados precisamente a la variable aleatoria en estudio $X$.</span>
<span id="cb21-1068"><a href="#cb21-1068"></a></span>
<span id="cb21-1069"><a href="#cb21-1069"></a>Se postula precisamente que la mejor medida de probabilidad es aquella que maximiza la probabilidad </span>
<span id="cb21-1070"><a href="#cb21-1070"></a>de observar estos eventos independientes. En el caso en particular de la estimación de una medida</span>
<span id="cb21-1071"><a href="#cb21-1071"></a>de probabilidad en una familia $<span class="sc">\{</span>P_{\theta}<span class="sc">\}</span>$ con $\theta \in \Theta$, se busca maximizar la </span>
<span id="cb21-1072"><a href="#cb21-1072"></a>probabilidad:</span>
<span id="cb21-1073"><a href="#cb21-1073"></a>$$</span>
<span id="cb21-1074"><a href="#cb21-1074"></a>\underset{\theta \in \Theta}{\sup} \prod\limits_{i=1}^n P_{\theta}\left( B_i \right)</span>
<span id="cb21-1075"><a href="#cb21-1075"></a>$$</span>
<span id="cb21-1076"><a href="#cb21-1076"></a></span>
<span id="cb21-1077"><a href="#cb21-1077"></a>o de forma equivalente, se puede utilizar una transformación con un logaritmo y tratar de maximizar</span>
<span id="cb21-1078"><a href="#cb21-1078"></a>lo que conocemos como **función de verosimilitud logarítmica** </span>
<span id="cb21-1079"><a href="#cb21-1079"></a>$$</span>
<span id="cb21-1080"><a href="#cb21-1080"></a>\ell = \sum\limits_{i=1}^n \log P_{\theta} \left( B_i \right)</span>
<span id="cb21-1081"><a href="#cb21-1081"></a>$$</span>
<span id="cb21-1082"><a href="#cb21-1082"></a></span>
<span id="cb21-1083"><a href="#cb21-1083"></a>el problema de estimación se reduce a la siguiente optimización</span>
<span id="cb21-1084"><a href="#cb21-1084"></a>$$</span>
<span id="cb21-1085"><a href="#cb21-1085"></a>\underset{\theta \in \Theta}{\sup} \ell( \theta )</span>
<span id="cb21-1086"><a href="#cb21-1086"></a>$$</span>
<span id="cb21-1087"><a href="#cb21-1087"></a></span>
<span id="cb21-1088"><a href="#cb21-1088"></a>Precisamente, el estimador de verosimilitud $\widehat{\theta}$, es el que resuelve el problema de </span>
<span id="cb21-1089"><a href="#cb21-1089"></a>optimización donde $\ell$ es la función objetivo y $\Theta$ representa el conjunto de restricciones.</span>
<span id="cb21-1090"><a href="#cb21-1090"></a>$$</span>
<span id="cb21-1091"><a href="#cb21-1091"></a>\widehat{\theta} </span>
<span id="cb21-1092"><a href="#cb21-1092"></a>= \underset{\theta \in \Theta}{\argsup} \ell( \theta )</span>
<span id="cb21-1093"><a href="#cb21-1093"></a>$$</span>
<span id="cb21-1094"><a href="#cb21-1094"></a></span>
<span id="cb21-1095"><a href="#cb21-1095"></a>En la práctica los eventos que se observa $B_i$ son puntuales y son precisamente los valores </span>
<span id="cb21-1096"><a href="#cb21-1096"></a>que toma la variable aleatoria $X$ para diferentes valores en el espacio muestra $\Omega$. Es decir, </span>
<span id="cb21-1097"><a href="#cb21-1097"></a>se observa una muestra $x_1 = X( \omega_1 ), \ldots, x_n = X( \omega_n )$. </span>
<span id="cb21-1098"><a href="#cb21-1098"></a></span>
<span id="cb21-1099"><a href="#cb21-1099"></a>Una forma de atacar el problema de estimación, cuando las observaciones son puntuales es trabar de </span>
<span id="cb21-1100"><a href="#cb21-1100"></a>encerrar cada una de las observaciones en intervalos lo suficientemente pequeños. Se puede tomar </span>
<span id="cb21-1101"><a href="#cb21-1101"></a>un valor $h &gt; 0$, y los eventos </span>
<span id="cb21-1102"><a href="#cb21-1102"></a>$B_i = \left<span class="sc">\{</span> \omega \in \Omega \middle| x_i - h \leq X( \omega ) \leq x_i + h \right<span class="sc">\}</span>$ y una </span>
<span id="cb21-1103"><a href="#cb21-1103"></a>aproximación de la función de verosimilitud en función de $h$, con la siguiente forma:</span>
<span id="cb21-1104"><a href="#cb21-1104"></a>$$</span>
<span id="cb21-1105"><a href="#cb21-1105"></a>\begin{split}</span>
<span id="cb21-1106"><a href="#cb21-1106"></a>\ell_h( \theta )</span>
<span id="cb21-1107"><a href="#cb21-1107"></a>&amp; = \sum\limits_{i=1}^n \log P_{\theta} \left( B_i \right) <span class="sc">\\</span></span>
<span id="cb21-1108"><a href="#cb21-1108"></a>&amp; = \sum\limits_{i=1}^n \log P_{\theta} \left( x_i - h \leq X \leq x_i + h \right) <span class="sc">\\</span></span>
<span id="cb21-1109"><a href="#cb21-1109"></a>&amp; = \sum\limits_{i=1}^n \log \left( F\left( x_i + h, \theta \right) - F\left( x_i - h, \theta \right) \right) <span class="sc">\\</span></span>
<span id="cb21-1110"><a href="#cb21-1110"></a>\end{split}</span>
<span id="cb21-1111"><a href="#cb21-1111"></a>$$</span>
<span id="cb21-1112"><a href="#cb21-1112"></a></span>
<span id="cb21-1113"><a href="#cb21-1113"></a>Sabemos de los diferentes resultados de análisis para funciones de variación acotada, como es el </span>
<span id="cb21-1114"><a href="#cb21-1114"></a>caso de las distribuciones de probabilidad, que tienen diferenciales en casi todo punto, es decir </span>
<span id="cb21-1115"><a href="#cb21-1115"></a>que las singularidades de estas son conjuntos con medida nula. Además la cantidad de singularidades,</span>
<span id="cb21-1116"><a href="#cb21-1116"></a>de saltos que puede presentar una función de variación acotada es a lo sumo numerable. Entonces, </span>
<span id="cb21-1117"><a href="#cb21-1117"></a>cada valor observado $x_i \in \Dif F$ es un punto de continuidad donde la función de distribución </span>
<span id="cb21-1118"><a href="#cb21-1118"></a>$F$ es diferenciable o es un punto de singularidad, así los valores observados en la muestra se pueden </span>
<span id="cb21-1119"><a href="#cb21-1119"></a>clasificar en estos dos tipos @KolmoFunAnalysis. De ello se puede dividir la suma anterior.</span>
<span id="cb21-1120"><a href="#cb21-1120"></a>$$</span>
<span id="cb21-1121"><a href="#cb21-1121"></a>\begin{split}</span>
<span id="cb21-1122"><a href="#cb21-1122"></a>\ell_h( \theta )</span>
<span id="cb21-1123"><a href="#cb21-1123"></a>&amp; = \sum\limits_{x_i \in \Dif F} \log \left( F\left( x_i + h, \theta \right) - F\left( x_i - h, \theta \right) \right) </span>
<span id="cb21-1124"><a href="#cb21-1124"></a><span class="ss">+ </span>\sum\limits_{x_i \notin \Dif F} \log \left( F\left( x_i + h, \theta \right) - F\left( x_i - h, \theta \right) \right) <span class="sc">\\</span></span>
<span id="cb21-1125"><a href="#cb21-1125"></a>\end{split}</span>
<span id="cb21-1126"><a href="#cb21-1126"></a>$$</span>
<span id="cb21-1127"><a href="#cb21-1127"></a>maximizar $\ell_h$ es equivalente a maximizar la siguiente función equivalente que notaremos con el</span>
<span id="cb21-1128"><a href="#cb21-1128"></a>mismo nombre</span>
<span id="cb21-1129"><a href="#cb21-1129"></a>$$</span>
<span id="cb21-1130"><a href="#cb21-1130"></a>\begin{split}</span>
<span id="cb21-1131"><a href="#cb21-1131"></a>\ell_h( \theta )</span>
<span id="cb21-1132"><a href="#cb21-1132"></a>&amp; = \frac{1}{h} \sum\limits_{x_i \in \Dif F} \log \left( F\left( x_i + h, \theta \right) - F\left( x_i - h, \theta \right) \right) </span>
<span id="cb21-1133"><a href="#cb21-1133"></a><span class="ss">+ </span>\sum\limits_{x_i \notin \Dif F} \log \left( F\left( x_i + h, \theta \right) - F\left( x_i - h, \theta \right) \right) <span class="sc">\\</span></span>
<span id="cb21-1134"><a href="#cb21-1134"></a>\end{split}</span>
<span id="cb21-1135"><a href="#cb21-1135"></a>$$</span>
<span id="cb21-1136"><a href="#cb21-1136"></a>la multiplicación de la primera suma por la constante $\frac{1}{h}$ tan solo escala la función, pero</span>
<span id="cb21-1137"><a href="#cb21-1137"></a>no cambia sus puntos críticos.</span>
<span id="cb21-1138"><a href="#cb21-1138"></a></span>
<span id="cb21-1139"><a href="#cb21-1139"></a>Tomando al límite, un valor $h$ cada vez más pequeño, para encerrar aún más en un intervalo los </span>
<span id="cb21-1140"><a href="#cb21-1140"></a>valores observados $<span class="sc">\{</span>x_i<span class="sc">\}</span>$, tenemos lo siguiente:</span>
<span id="cb21-1141"><a href="#cb21-1141"></a>$$</span>
<span id="cb21-1142"><a href="#cb21-1142"></a>\begin{split}</span>
<span id="cb21-1143"><a href="#cb21-1143"></a>\underset{h \searrow 0}{\lim} \ell_h( \theta )</span>
<span id="cb21-1144"><a href="#cb21-1144"></a>&amp; = \underset{h \searrow 0}{\lim} \frac{1}{h} \sum\limits_{x_i \in \Dif F} \log \left( F\left( x_i + h, \theta \right) - F\left( x_i - h, \theta \right) \right) </span>
<span id="cb21-1145"><a href="#cb21-1145"></a><span class="ss">+ </span>\underset{h \searrow 0}{\lim} \sum\limits_{x_i \notin \Dif F} \log \left( F\left( x_i + h, \theta \right) - F\left( x_i - h, \theta \right) \right) <span class="sc">\\</span></span>
<span id="cb21-1146"><a href="#cb21-1146"></a>&amp; = \sum\limits_{x_i \in \Dif F} \log \left( f(x_i,\theta) \right) </span>
<span id="cb21-1147"><a href="#cb21-1147"></a><span class="ss">+ </span>\sum\limits_{x_i \notin \Dif F} \log \left( F\left( x_i, \theta \right) - F\left( x_i-, \theta \right) \right),\quad</span>
<span id="cb21-1148"><a href="#cb21-1148"></a>\text{si existe el límite por izquierda de $F$}</span>
<span id="cb21-1149"><a href="#cb21-1149"></a>\end{split}</span>
<span id="cb21-1150"><a href="#cb21-1150"></a>$$</span>
<span id="cb21-1151"><a href="#cb21-1151"></a></span>
<span id="cb21-1152"><a href="#cb21-1152"></a>siendo $F\left( x_i-, \theta \right)$ el límite por izquierda de la función de distribución $F$ en </span>
<span id="cb21-1153"><a href="#cb21-1153"></a>el punto $x_i$. Además, recordamos que toda función de distribución es continua por derecha, esto </span>
<span id="cb21-1154"><a href="#cb21-1154"></a>implica que $F\left( x_i, \theta \right) = F\left( x_i+, \theta \right)$.</span>
<span id="cb21-1155"><a href="#cb21-1155"></a></span>
<span id="cb21-1156"><a href="#cb21-1156"></a>Para el caso puntual esta es precisamente la expresión de la función de verosimilitud logarítmica, </span>
<span id="cb21-1157"><a href="#cb21-1157"></a>la cual podemos utilizar para estimar el mejor parámetro $\theta \in \Theta$ que maximice el valor </span>
<span id="cb21-1158"><a href="#cb21-1158"></a>de verosimilitud.</span>
<span id="cb21-1159"><a href="#cb21-1159"></a></span>
<span id="cb21-1160"><a href="#cb21-1160"></a>:::: {.thmbox data-latex=""}</span>
<span id="cb21-1161"><a href="#cb21-1161"></a>$$</span>
<span id="cb21-1162"><a href="#cb21-1162"></a>\ell( \theta ) = </span>
<span id="cb21-1163"><a href="#cb21-1163"></a>\sum\limits_{x_i \in \Dif F} \log \left( f(x_i,\theta) \right) </span>
<span id="cb21-1164"><a href="#cb21-1164"></a><span class="ss">+ </span>\sum\limits_{x_i \notin \Dif F} \log \left( F\left( x_i, \theta \right) - F\left( x_i-, \theta \right) \right)</span>
<span id="cb21-1165"><a href="#cb21-1165"></a>(<span class="sc">\#</span>eq:loglik)</span>
<span id="cb21-1166"><a href="#cb21-1166"></a>$$</span>
<span id="cb21-1167"><a href="#cb21-1167"></a>::::</span>
<span id="cb21-1168"><a href="#cb21-1168"></a></span>
<span id="cb21-1169"><a href="#cb21-1169"></a>En los casos más sencillos de estimación por verosimilitud se suele cumplir una de las siguientes</span>
<span id="cb21-1170"><a href="#cb21-1170"></a>hipótesis: todos los puntos $x_i$ sobre los cuales se calcula son puntos de continuidad, o la </span>
<span id="cb21-1171"><a href="#cb21-1171"></a>función de distribución acumulada $F$ no tiene puntos singulares, siendo este último caso el más </span>
<span id="cb21-1172"><a href="#cb21-1172"></a>usual. Pero, en la práctica esto no sucede y la suma adicional a la derecha en la expresión anterior</span>
<span id="cb21-1173"><a href="#cb21-1173"></a>es precisamente necesaria, ya que la distribución $F$ si puede tener singularidades. Al utilizar </span>
<span id="cb21-1174"><a href="#cb21-1174"></a>solo la primera suma sobre los puntos donde hay diferenciabilidad, la estimación por verosimilitud </span>
<span id="cb21-1175"><a href="#cb21-1175"></a>para el parámetro $\theta$ no será la correcta.</span>
<span id="cb21-1176"><a href="#cb21-1176"></a></span>
<span id="cb21-1177"><a href="#cb21-1177"></a>Los estimadores que resulta de la maximización de verosimilitud satisfacen algunas propiedades</span>
<span id="cb21-1178"><a href="#cb21-1178"></a>deseables.</span>
<span id="cb21-1179"><a href="#cb21-1179"></a></span>
<span id="cb21-1180"><a href="#cb21-1180"></a>:::: {.thmbox data-latex=""}</span>
<span id="cb21-1181"><a href="#cb21-1181"></a>::: {.theorem #mleconsistent}</span>
<span id="cb21-1182"><a href="#cb21-1182"></a>Los estimadores por maximización de verosimilitud, si en caso existen, estos son consistentes.</span>
<span id="cb21-1183"><a href="#cb21-1183"></a>:::</span>
<span id="cb21-1184"><a href="#cb21-1184"></a>::::</span>
<span id="cb21-1185"><a href="#cb21-1185"></a></span>
<span id="cb21-1186"><a href="#cb21-1186"></a>:::: {.thmbox data-latex=""}</span>
<span id="cb21-1187"><a href="#cb21-1187"></a>::: {.theorem #mleefficient}</span>
<span id="cb21-1188"><a href="#cb21-1188"></a>Los estimadores por máximización de verosimilitud, si en caso existen, estos son eficientes.</span>
<span id="cb21-1189"><a href="#cb21-1189"></a>:::</span>
<span id="cb21-1190"><a href="#cb21-1190"></a>::::</span>
<span id="cb21-1191"><a href="#cb21-1191"></a></span>
<span id="cb21-1192"><a href="#cb21-1192"></a>:::: {.thmbox data-latex=""}</span>
<span id="cb21-1193"><a href="#cb21-1193"></a>::: {.theorem #mlesufficient}</span>
<span id="cb21-1194"><a href="#cb21-1194"></a>Si en caso existe un estimador suficiente, este es función del estimador de máxima verosimilitud.</span>
<span id="cb21-1195"><a href="#cb21-1195"></a>:::</span>
<span id="cb21-1196"><a href="#cb21-1196"></a>::::</span>
<span id="cb21-1197"><a href="#cb21-1197"></a></span>
<span id="cb21-1198"><a href="#cb21-1198"></a>Un estimador de máxima verosimilitud no necesariamente es insesgado.</span>
<span id="cb21-1199"><a href="#cb21-1199"></a></span>
<span id="cb21-1200"><a href="#cb21-1200"></a>:::: {.thmbox data-latex=""}</span>
<span id="cb21-1201"><a href="#cb21-1201"></a>::: {.theorem #mlenormality}</span>
<span id="cb21-1202"><a href="#cb21-1202"></a>Un estimador por maximización de verosimilitud se comporta normalmente de forma asintótica</span>
<span id="cb21-1203"><a href="#cb21-1203"></a>alrededor del verdadero parámetro $\theta \in \Theta$. Más claramente, si $\tau_n$ es el estimador</span>
<span id="cb21-1204"><a href="#cb21-1204"></a>por maximización de verosimilitud, se tiene que $\tau_n( X_1, \ldots, X_n ) \rightsquigarrow F_n$, </span>
<span id="cb21-1205"><a href="#cb21-1205"></a>entonces en el límite $n \rightarrow +\infty$, se tiene la convergencia en distribución</span>
<span id="cb21-1206"><a href="#cb21-1206"></a>$F_n \rightarrow N\left( \theta, I( \theta )^{-1} \right)$.</span>
<span id="cb21-1207"><a href="#cb21-1207"></a></span>
<span id="cb21-1208"><a href="#cb21-1208"></a>Donde $I(\theta)$, es un criterio de información, dado por la matriz</span>
<span id="cb21-1209"><a href="#cb21-1209"></a>$$</span>
<span id="cb21-1210"><a href="#cb21-1210"></a>\left<span class="co">[</span><span class="ot"> I(\theta) \right</span><span class="co">]</span>_{i,j}</span>
<span id="cb21-1211"><a href="#cb21-1211"></a>= \left<span class="co">[</span><span class="ot"> \E\left[ -\frac{\D^2}{\D \theta_i \D \theta_j}\log f( X, \theta )\, \middle|\, \theta \right] \right</span><span class="co">]</span>_{i,j}</span>
<span id="cb21-1212"><a href="#cb21-1212"></a>$$</span>
<span id="cb21-1213"><a href="#cb21-1213"></a>:::</span>
<span id="cb21-1214"><a href="#cb21-1214"></a>::::</span>
<span id="cb21-1215"><a href="#cb21-1215"></a></span>
<span id="cb21-1216"><a href="#cb21-1216"></a><span class="co">&lt;!-------------------------------------------------------------------------------------------------&gt;</span></span>
<span id="cb21-1217"><a href="#cb21-1217"></a><span class="fu">### Pruebas de hipótesis</span></span>
<span id="cb21-1218"><a href="#cb21-1218"></a></span>
<span id="cb21-1219"><a href="#cb21-1219"></a>Usualmente sobre un variable aleatoria $X$ a valores en $\R^n$, se suele establecer una decisión </span>
<span id="cb21-1220"><a href="#cb21-1220"></a>basada en el valores que toma la variable aleatoria, la selección se realiza entre diferentes </span>
<span id="cb21-1221"><a href="#cb21-1221"></a>hipótesis. De manera más formal, esta decisión puede ser representada por una función </span>
<span id="cb21-1222"><a href="#cb21-1222"></a>$\delta : \R^n \longrightarrow D$, donde $D = <span class="sc">\{</span>d_1,\ldots,d_n<span class="sc">\}</span>$ es un conjunto discreto que </span>
<span id="cb21-1223"><a href="#cb21-1223"></a>representa las hipótesis. Se conoce a $\delta$ como un **test estadístico de elección**.</span>
<span id="cb21-1224"><a href="#cb21-1224"></a></span>
<span id="cb21-1225"><a href="#cb21-1225"></a>En muchos casos uno nos interesa encontrar el test estadístico $\delta$ que tenga el menor error</span>
<span id="cb21-1226"><a href="#cb21-1226"></a>de cometer una selección errada, así para cada decisión en $d \in D$, el test $\delta$ tendrá</span>
<span id="cb21-1227"><a href="#cb21-1227"></a>probabilidades $P( \delta( X ) \neq d )$, es decir, para cualquier otro test $\eta$, se debe </span>
<span id="cb21-1228"><a href="#cb21-1228"></a>tener que</span>
<span id="cb21-1229"><a href="#cb21-1229"></a>$$</span>
<span id="cb21-1230"><a href="#cb21-1230"></a>P( \delta( X ) \neq d ) \leq P( \eta( X ) \neq d ),\qquad \forall d \in D</span>
<span id="cb21-1231"><a href="#cb21-1231"></a>$$</span>
<span id="cb21-1232"><a href="#cb21-1232"></a></span>
<span id="cb21-1233"><a href="#cb21-1233"></a>:::: {.thmbox data-latex=""}</span>
<span id="cb21-1234"><a href="#cb21-1234"></a>::: {.definition #dpowertest name="Test más potente"}</span>
<span id="cb21-1235"><a href="#cb21-1235"></a>En muchos casos uno no se interesa en cualquier tipo de test estadístico, sino en un cierto tipo </span>
<span id="cb21-1236"><a href="#cb21-1236"></a>de pruebas, más interesados en un subconjunto $K \subset <span class="sc">\{</span> \delta : \R^n \longrightarrow D <span class="sc">\}</span>$, </span>
<span id="cb21-1237"><a href="#cb21-1237"></a>a este se lo conoce como una **clase**. Así se dice que un test $\delta \in K$ es el </span>
<span id="cb21-1238"><a href="#cb21-1238"></a>**test más potente** en la clase $K$ para la hipótesis $d \in D$ si para cualquier otro test </span>
<span id="cb21-1239"><a href="#cb21-1239"></a>$\eta \in K$, se tiene la desigualdad.</span>
<span id="cb21-1240"><a href="#cb21-1240"></a>$$</span>
<span id="cb21-1241"><a href="#cb21-1241"></a>P( \delta( X ) \neq d ) \leq P( \eta( X ) \neq d )</span>
<span id="cb21-1242"><a href="#cb21-1242"></a>$$</span>
<span id="cb21-1243"><a href="#cb21-1243"></a>:::</span>
<span id="cb21-1244"><a href="#cb21-1244"></a>::::  </span>
<span id="cb21-1245"><a href="#cb21-1245"></a></span>
<span id="cb21-1246"><a href="#cb21-1246"></a></span>
<span id="cb21-1247"><a href="#cb21-1247"></a><span class="in">```{r l5end, echo = FALSE}</span></span>
<span id="cb21-1248"><a href="#cb21-1248"></a><span class="in">rm( list = ls()[ ls() != 'def.chunk.hook' ] )</span></span>
<span id="cb21-1249"><a href="#cb21-1249"></a><span class="in">```</span></span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/pedroguarderas/seguros_generales/edit/main/lectura_05.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/pedroguarderas/seguros_generales/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div></div></footer></body></html>