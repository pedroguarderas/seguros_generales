[
  {
    "objectID": "lectura_01.html",
    "href": "lectura_01.html",
    "title": "2  Materia",
    "section": "",
    "text": "Personal docente\nLas presentes notas de curso han sido creadas para la materia Matemática Actuarial de los Seguros Generales la cual es dictada en la Maestría de Ciencias Actuariales que realizan de forma conjunta la “Escuela Politécnica Nacional” (EPN) y la “Pontificia Universidad Católica del Ecuador” (PUCE).\nHemos creado estas notas con la motivación de brindar una referencia de la materia y facilitar el aprendizaje a los estudiantes, tratando de conjugar la parte teórica del curso con ejemplos prácticos implementados utilizando el lenguaje R.\nEste trabajo se ha realizado en colaboración entre Leonardo Vélez y Pedro Guarderas. La motivación y objetivo para haber creado las presentes notas es llegar a generar un buen material de aprendizaje y referencia.\nSin embargo, será un placer para nosotros que las lean, critiquen y en especial nos hagan llegar sus observaciones y posibles mejoras.\nEl profesor a cargo de la materia es el actuario Leonardo Vélez Aguirre.\nComo docente de apoyo, especialmente en la parte informática, también colaborará el matemático Pedro Guarderas.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Materia</span>"
    ]
  },
  {
    "objectID": "lectura_01.html#resultados-de-aprendizaje",
    "href": "lectura_01.html#resultados-de-aprendizaje",
    "title": "2  Materia",
    "section": "Resultados de aprendizaje",
    "text": "Resultados de aprendizaje\n\nContrastar los criterios de valoración actuarial en los seguros generales, así como elaborar y aplicar las bases técnicas.\nAprender a obtener la distribución y momentos correspondientes de la siniestralidad total pagada por el asegurador y por el reasegurador en presencia de franquicias y reaseguro.\nUtilizar textos informativos y de divulgación científica para el desarrollo de trabajos académicos y el desempeño profesional actuarial.\nAplicar adecuadamente las fases y procedimientos de la investigación científica a fin de encauzarla de manera eficiente y tendiente a la excelencia.\nDesarrollar la capacidad de trabajar en equipo y el sentido de responsabilidad en el cumplimiento de sus responsabilidades.\nDesarrollar un espíritu crítico y creativo, y respetuoso de su docente y compañeros.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Materia</span>"
    ]
  },
  {
    "objectID": "lectura_01.html#contenidos-propuestos",
    "href": "lectura_01.html#contenidos-propuestos",
    "title": "2  Materia",
    "section": "Contenidos propuestos",
    "text": "Contenidos propuestos\n\nDistribuciones compuestas\nSeries de tiempo\nEl Proceso de Riesgo: Distribución clase (a,b) y algoritmo de Panjer, Distribución de siniestralidad agregada\nModelos Lineales Generalizados (GLMs): para datos binarios y recuentos\nProceso de Tarificación\nTarificación a priori: cálculo de la prima pura\nTarificación a posteriori: sistemas Bonus-Malus\nTeoría de la Ruina",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Materia</span>"
    ]
  },
  {
    "objectID": "lectura_01.html#referencias-bibliográficas",
    "href": "lectura_01.html#referencias-bibliográficas",
    "title": "2  Materia",
    "section": "Referencias bibliográficas",
    "text": "Referencias bibliográficas\n\nPrincipales libros guía\n[1], [2], [10], [5], [6], https://nonlifemaths.github.io/.\n\n\n\nOtras referencias de soporte\nDe utilidad para quien desee profundizar con más detalle en algunos conceptos\n[8], [12], [11], [9], [3], [4].\nGIL FANA: Elementos de Matemáticas para las Ciencias del Seguro. Fundación Mapfre Estudios. 1991",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Materia</span>"
    ]
  },
  {
    "objectID": "lectura_01.html#requerimientos-informáticos",
    "href": "lectura_01.html#requerimientos-informáticos",
    "title": "2  Materia",
    "section": "Requerimientos informáticos",
    "text": "Requerimientos informáticos\n\nLenguaje R\nEditor de código: RStudio, VScode, …\nPaquete R actuar [7].\n\n\n\n\n\n\n\n1. A. Klugman S, H. Panjer H, E. Willmot G (2012) Loss Models, From Data to Decisions, 4th ed. John Wiley & Sons, Inc, Hoboken, New Jersey, United States\n\n\n2. A. Klugman S, H. Panjer H, E. Willmot G (2013) Loss Models, Further Topics, 1st ed. John Wiley & Sons, Inc, Hoboken, New Jersey, United States\n\n\n3. Bickel PJ, Doksum KA (2015) Mathematical statistics: Basic ideas and selected topics. CRC Press\n\n\n4. Bickel PJ, Doksum KA (2016) Mathematical statistics: Basic ideas and selected topics. CRC Press\n\n\n5. Denuit M, Charpentier A (2005) Mathématiques de l’assurance non-vie. Economica, Paris\n\n\n6. Denuit M, Charpentier A (2005) Mathématiques de l’assurance non-vie. Economica, Paris\n\n\n7. Dutang C, Goulet V, Pigeon M (2008) Actuar: An R package for actuarial science\n\n\n8. Gupta SG, Kapoor VK (2020) Fundamental of Mathematical Statistics: A modern approach, 12th ed. Sultan Chand & Sons\n\n\n9. Knight K (1999) Mathematical statistics. Chapman & Hall/CRC\n\n\n10. Marceau É (2013) Modélisation et évaluation quantitative des risques en actuariat: Modèles sur une période. Springer\n\n\n11. McNeil A, Frey R, Embrechts P (2015) Quantitative Risk Management: Concepts, Techniques and Tools. Princeton University Press\n\n\n12. Mildenhall SJ, Major JA (2022) Pricing Insurance Risk. Wiley",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Materia</span>"
    ]
  },
  {
    "objectID": "lectura_02.html",
    "href": "lectura_02.html",
    "title": "3  Preliminares",
    "section": "",
    "text": "3.1 Notación matemática",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Preliminares</span>"
    ]
  },
  {
    "objectID": "lectura_02.html#notación-matemática",
    "href": "lectura_02.html#notación-matemática",
    "title": "3  Preliminares",
    "section": "",
    "text": "\\(\\N\\) el conjunto de los números naturales \\(\\N = \\{0,1,2,\\ldots,\\}\\)\n\\(\\N^*\\) el conjunto de los números naturales positivos \\(\\N^* = \\N \\setminus \\{0\\} = \\{1,2,3,\\ldots,\\}\\)\n\\(\\Z\\) el conjunto de los números enteros, \\(\\Z = \\{\\ldots,-2,-1,0,1,2, \\ldots \\}\\)\n\\(\\R\\) el conjunto de los números reales\n\\(\\R_+\\) el conjunto de los números reales no negativos \\(\\R_+ = \\{ x \\in \\R \\mid x \\geq 0 \\}\\)\n\\(\\C\\) el conjunto de los números complejos, \\(\\C = \\R \\oplus i \\R\\)\n\\(\\overline{\\R} = \\R \\cup \\{ +\\infty \\} \\cup \\{ -\\infty \\}\\) el conjunto de los reales extendido incluyendo los infinitos\n\\(x \\approx y\\) indica que el valor \\(y\\) es aproximado al valor \\(x\\)\n\\(f: X \\longrightarrow Y\\) la función que toma valores en \\(X\\) y entrega valores en \\(Y\\)\n\\(\\sum\\limits_{k=0}^n x_k\\) es la suma de los elementos \\(x_0,\\ldots,x_n\\) de una lista\n\\([ a, b ] = \\left\\{ x \\in \\R \\mid x \\geq a \\land x \\leq b \\right\\}\\) intervalo cerrado para cualquier \\(a,b \\in \\overline{\\R}\\)\n\\((a,b) = \\left\\{ x \\in \\R \\mid x &gt; a \\land x &lt; b \\right\\}\\) intervalo abierto para cualquier \\(a,b \\in \\overline{\\R}\\)\n\\(\\{x_n\\}_{n\\in\\N} = \\left\\{ x_n \\in X \\mid n \\in \\N \\right\\}\\) secuencia en el conjunto \\(X\\), no es más que una función de \\(\\N\\) con valores en \\(X\\)\n\\(\\underset{x \\rightarrow y}{\\lim} f(x) = a\\) límite de la función \\(f(x)\\) cuando \\(x\\) tiende a \\(y\\).\n\\(\\inf A\\) es la mayor de las cotas inferiores de \\(A\\).\n\\(\\sup A\\) es la menor de las cotas superiores de \\(A\\).\n\\(\\Dif f\\) conjunto de puntos donde la función \\(f\\) es diferenciable.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Preliminares</span>"
    ]
  },
  {
    "objectID": "lectura_02.html#consideraciones-de-programación-en-r",
    "href": "lectura_02.html#consideraciones-de-programación-en-r",
    "title": "3  Preliminares",
    "section": "3.2 Consideraciones de programación en R",
    "text": "3.2 Consideraciones de programación en R\n\n3.2.1 Opciones usuales\n\n\nCode\noptions( scipen = 9999 )\noptions( stringsAsFactors = FALSE )\n\n\n\n\n\n3.2.2 Paquetes usuales\n\n\nCode\nlibrary( actuar )\nlibrary( data.table )\nlibrary( fExtremes )\nlibrary( fitdistrplus )\nlibrary( ggplot2 )\nlibrary( googledrive )\nlibrary( kableExtra )\nlibrary( knitr )\nlibrary( latex2exp )\nlibrary( lubridate )\nlibrary( openxlsx )\nlibrary( readxl )\nlibrary( rmarkdown )\nlibrary( shiny )\nlibrary( wesanderson )\nlibrary( dplyr )\n\n\n\n\n\n3.2.3 Estructuras básicas\nPara definir una variable utilizamos el operador de asignación &lt;-, también se puede utilizar =.\nUn vector se define con la función de concatenación c\n\n\nCode\nx &lt;- c( 1, 2, 3 )\nprint( x )\n\n\n[1] 1 2 3\n\n\nTambién, se puede guardar los elementos en una lista\n\n\nCode\nx &lt;- list( 1, 2, 3 )\nprint( x )\n\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 2\n\n[[3]]\n[1] 3\n\n\nLos elementos de las listas se pueden guardar con nombres.\n\n\nCode\nx &lt;- list( 'a' = 1, 'b' = 2, 'c' = 3 )\nprint( x )\n\n\n$a\n[1] 1\n\n$b\n[1] 2\n\n$c\n[1] 3\n\n\nCode\nx[[ 'b' ]]\n\n\n[1] 2\n\n\nUna función se define con la sentencia function\n\n\nCode\nf &lt;- function( t ) {\n  return( t^2 )\n}\n\n\nSi buscamos aplicar una función sobre un vector, podemos utilizar sapply\n\n\nCode\ny &lt;- sapply( x, FUN = f )\nprint( y )\n\n\na b c \n1 4 9 \n\n\n\n\nCode\nz &lt;- lapply( x, FUN = f )\nprint( z )\n\n\n$a\n[1] 1\n\n$b\n[1] 4\n\n$c\n[1] 9\n\n\nPara muchas tareas que están relacionadas con el manejo de datos, podemos utilizar las funcionalidades del paquete de R, data.table. Para trabajar con fechas recomendamos utilizar el paquete lubridate. Para trabajar con distribuciones de probabilidad que se utilizan para modelar valores extremos utilizamos fExtremes. Para varias funcionalidades asociadas a la estimación de modelos de pérdida utilizamos actuar.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Preliminares</span>"
    ]
  },
  {
    "objectID": "lectura_03.html",
    "href": "lectura_03.html",
    "title": "4  Introducción a las operaciones de seguros",
    "section": "",
    "text": "4.1 El sistema financiero\nEn todos lo países del mundo existe un sistema financiero con el objetivo de ofrecer facilidades para poner en práctica las decisiones financieras de:\nEn general en un sistema financiero encontramos:\nPor medio de las instituciones del sistema, se espera facilitar algunas operaciones:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducción a las operaciones de seguros</span>"
    ]
  },
  {
    "objectID": "lectura_03.html#el-sistema-financiero",
    "href": "lectura_03.html#el-sistema-financiero",
    "title": "4  Introducción a las operaciones de seguros",
    "section": "",
    "text": "Personas\nEmpresas\nGobierno.\n\n\n\nMercados de activos financieros\n\nDeuda, Derivados, Títulos…\n\nIntermediarios (públicos y privados)\n\nBancos\nEmpresas de seguros\nFondos de pensiones\n\nEmpresas de servicios\n\nDifusión de información\nTarjetas de crédito, brokers…\n\n\n\n\nTransferencia de recursos\nA través del tiempo, regiones, entre, industrias,…\nAdministración del riesgo\nMedios para administrar el riesgo financiero\nCompensación y establecimiento de pagos\nFacilitar el intercambio de bienes,servicios, activos…\nConcentración de recursos\nFinanciamiento de grandes empresas y proyectos\nSuministro de información\nToma de decisiones en iguales condiciones",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducción a las operaciones de seguros</span>"
    ]
  },
  {
    "objectID": "lectura_03.html#riesgos-financieros",
    "href": "lectura_03.html#riesgos-financieros",
    "title": "4  Introducción a las operaciones de seguros",
    "section": "4.2 Riesgos financieros",
    "text": "4.2 Riesgos financieros\nLos riegos financieros son aquellos riesgos relacionados con las posibles pérdidas económicas que pueden enfrentar los actores del sistema (personas, empresas, gobierno), causadas por hechos contingentes que amenazan sus operaciones en el mercado financiero, sus actividades de negocio o sus pertenencias (activos financieros).\nDe manera natural los individuos presentan un comportamiento que traduce su “miedo” a las pérdidas cuando enfrentan situaciones de riesgo: la aversión al riesgo.\nLa aversión al riesgo se produce hacia la parte negativa del riesgo, esto es ante la posibilidad de pérdidas financieras.\nEn estas situaciones es necesario administrar el riesgo, para lo cual co-existen cuatro formas principales:\n\nEvitar el riesgo (no siempre posible)\nPrevenir y controlar el riesgo\nAceptar el riesgo\nTransferir el riesgo\n\nVale recordar que históricamente los grandes avances de la humanidad han sido posibles gracias a a la aceptación y correcta gestión de de riesgos muy importantes.\nPor lo tanto, en lugar de evitar los riesgos debemos tratar de conocerlos profundamente para aceptarlos y poder aprovechar las oportunidades que brinda su correcta gestión.\n\nPara pensar:\nEl riesgo existe gracias a nuestra humana incapacidad de conocer el futuro, a nuestras limitaciones para identificar sus verdaderas causas. Y mientras así sea, debemos esforzarnos por comprenderlo, aceptarlo y gestionarlo inteligentemente y con humildad.\n– L. Vélez, 2003",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducción a las operaciones de seguros</span>"
    ]
  },
  {
    "objectID": "lectura_03.html#los-seguros-y-la-transferencia-de-riesgos-financieros",
    "href": "lectura_03.html#los-seguros-y-la-transferencia-de-riesgos-financieros",
    "title": "4  Introducción a las operaciones de seguros",
    "section": "4.3 Los seguros y la transferencia de riesgos financieros",
    "text": "4.3 Los seguros y la transferencia de riesgos financieros\nTransferir el riesgo consiste en trasladar el riesgo a otros a un costo razonable. Esto es posible en un sistema financiero gracias a mecanismos como:\n\nProtección: reducir una pérdida potencial a costa de renunciar a una ganancia.\nDiversificación: distribuir una pérdida potencial en varias pérdidas menores con distintas exposiciones al riesgo.\nAseguramiento: pagar una prima para compensar una pérdida potencial mayor.\n\nEl aseguramiento consiste en transferir a una compañía de seguros, la totalidad o parte de las posibles pérdidas financieras que enfrenta un asegurado, bajo las condiciones estipuladas en un contrato: la póliza de seguro.\nEn la práctica, el seguro permite al asegurado reducir significativamente el impacto del riesgo en su patrimonio, pero no eliminarlo por completo. Entonces, una operación de seguro consiste en sustituir un riesgo inicial por otro más ventajoso para el asegurado, a cambio del pago de una prima.\nAdemás es necesario tomar en cuenta que el asegurado también asume el riesgo de quiebra de la aseguradora, en cuyo caso la compañía no podrá brindar la cobertura ofrecida por los daños sufridos por el asegurado.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducción a las operaciones de seguros</span>"
    ]
  },
  {
    "objectID": "lectura_03.html#la-mutualización-de-riesgos",
    "href": "lectura_03.html#la-mutualización-de-riesgos",
    "title": "4  Introducción a las operaciones de seguros",
    "section": "4.4 La mutualización de riesgos",
    "text": "4.4 La mutualización de riesgos\nLa técnica fundamental de la industria de seguros es la mutualización o compensación de riesgos: muchos peligros (accidentes, robos, enfermedades, etc.) amenazan a un gran número de personas, pero a la final afectan solo a un número reducido de personas.\nPor lo tanto, el seguro aprovecha esta técnica para financiar el pago de pocos siniestros por medio de las primas recolectadas de muchos asegurados a un mismo riesgo y para hacer de esto un negocio lucrativo.\nEn este contexto, para que la mutualización de riesgos funciones es necesario que los riesgos estén suficientemente dispersos y guarden entre ellos un cierto grado de independencia.\nIlustremos estos conceptos con un simple análisis:\n\nSi la compañía emitiera una sola póliza (que cubriera incendios, por ejemplo), obtendría un beneficio modesto con una alta probabilidad (si el incendio no destruye la propiedad asegurada), pero estaría expuesta a una pérdida considerable si se produjera un siniestro. En este caso, la situación de la compañía es idéntica a la del asegurado!.\nSi, por el contrario, la compañía suscribe un gran número de pólizas contra incendios, que cubren edificaciones similares, y si un siniestro que afecta a una de las pólizas no influye en los demás riesgos de la cartera, intuitivamente vemos que que la compañía podría mutualizar o compensar los riesgos, es decir, financiar el pago de las pérdidas que afectan a un pequeño número de pólizas, utilizando las primas recolectadas de todas las pólizas de la cartera.\nEn resumen, hablamos de mutualización de riesgos cuando la aseguradora logra agrupar en su cartera un gran número de riesgos similares (dispersión) y sin influencia mutua (independencia), y de esta forma puede utilizar las primas de todos los contratos para compensar y financiar los reclamos por los siniestros sufridos por los asegurados desafortunados…\n… y además obtiene una ganancia!\n\nPero ¿hasta qué punto podemos mutualizar los riesgos?\nExisten casos como los riesgos catastróficos o de desastres naturales que no pueden ser mutualizados.\nEn efecto, los riesgos catastróficos o de desastres naturales (ej. terremotos), son riesgos que no están suficientemente dispersos y afectan a un gran número de asegurados al mismo tiempo, por lo cual escapan al principio de mutualización o compensación.\nTradicionalmente, los riesgos catastróficos estaban excluidos del seguro, pero hoy en día, su asegurabilidad tiene una creciente aceptación gracias a las operaciones de coaseguro y reaseguro que se utilizan para garantizar una mayor dispersión de los riesgos, incluso a nivel mundial, y hacen posible la cobertura de riesgos catastróficos.\n\nPerspectivas\nEn mercados más desarrollados, existen otras técnicas poco utilizadas en nuestro mercado local, que ofrecen soluciones en algunos casos donde la técnica de mutualización podría fallar:\n\nLa titularización de riesgos: permite a las compañías de seguros recurrir al mercado financiero para cubrir los riesgos que han suscrito.\nDe manera más general, las técnicas denominadas transferencia alternativa de riesgos podrían permitir realizar importantes avances en este ámbito, ampliando los límites de la asegurabilidad, pero demandan cambios significativos en el plano legal.\nLas empresas de retrocesión intervienen también a nivel global como aseguradores de los reaseguradores.\nEmpresas captivas de seguros: son empresas de seguros o reaseguros que son propiedad de otra organización generalmente no aseguradora ni reaseguradora, creadas con el objetivo de asegurar total o parcialmente los riesgos de su empresa matriz o grupo empresarial. Son una forma de autoseguro o transferencia de riesgos, que permiten a la empresa matriz, gestionar sus riesgos de manera más eficiente y personalizada y con costos más bajos.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducción a las operaciones de seguros</span>"
    ]
  },
  {
    "objectID": "lectura_03.html#fundamentos-técnicos-de-los-seguros",
    "href": "lectura_03.html#fundamentos-técnicos-de-los-seguros",
    "title": "4  Introducción a las operaciones de seguros",
    "section": "4.5 Fundamentos técnicos de los seguros",
    "text": "4.5 Fundamentos técnicos de los seguros\nDesde su concepción, la idea de la mutualización de riesgos que fundementa el seguro, despertó la necesidad de medir ¿cuántos son y cuánto costarán los ‘pocos siniestros’ que deberá pagar un seguro?, y por ende, cuantificar ‘cuál es el monto de la prima’ que debería recolectar de la cartera de asegurados.\nExisten dos disciplinas que se ocupan del estudio de estos problemas: la estadística y la teoría de probabilidades.\nAmbas ramas estudian la forma de medir o cuantificar la incertidumbre, en base de una nueva magnitud: la probabilidad, que es un número que expresa, a priori, el nivel de las posibilidades de que un hecho ocurra.\nLa estadística busca asignar una medida de probabilidad a los eventos aleatorios sobre la base de la observación o experimentación de esos eventos.\nEn cambio, la teoría de probabilidades supone que la medida de la probabilidad asociada a los eventos aleatorios o es conocida o se puede calcular de forma deductiva por medio del razonamiento lógico.\nEl objetivo principal del cálculo de probabilidades es proporcionar un método científico para cuantificar la probabilidad de que ocurran ciertos eventos. En este contexto, surge de forma natural el concepto de variable aleatoria, que para el actuario principalmente puede representar:\n\nel coste de un siniestro: severidad\no el número de siniestros: frecuencia.\n\nLas variables aleatorias constituyen la herramienta esencial para modelar la transferencia y mutualización de riesgos entre asegurados y aseguradoras.\n\n4.5.1 Un poco de historia…\nEl sustento científico de los fundamentos del seguro comenzó con los grandes desarrollos de la estadística y cálculo de probabilidades:\n\nCorrespondencia entre Pascal, Caballero de Méré y Fermat sobre problemas de apuestas\n\nse consideran los documentos que desencadenaron el desarrollo de la estadística y la teoría de probabilidades\n\nLey de los grandes números (Teorema de Bernoulli, s.XVIII y Poisson s.XIX) y\nDesarrollo del cálculo de probabilidades (Kolmogorov, s.XX).\n\nEl seguro comienza entonces a fundamentarse en las probabilidades y leyes estadísticas (en sus inicios el seguro fue confundido con un juego de apuestas) que posteriormente dieron nacimiento a una nueva disciplina: el cálculo actuarial, que demanda el análisis de información histórica para medir los riesgos y determinar los niveles de primas que permiten financiar las operaciones de seguros, considerando todos los aspectos del negocio: leyes, contabilidad, finanzas, demografía, etc.\nRetomando el tema de los límites de la mutualización de riesgos, en la actualidad, las compañías de seguros ofrecen coberturas que presentan ciertas limitaciones técnicas, pues no se basan en análisis de información histórica, porque ésta no existe o es limitada:\n\nSeguros “especiales” o “exóticos”\n\nSeguro de las piernas de futbolistas, bailarinas…\nSeguro de satélites\n\nSeguros nuevos\n\nEj. SPPAT (antiguo SOAT) al inicio!\n\n\nEn estos casos el fundamento técnico resulta subjetivo, basado en análisis de grupos de expertos que estudian los riesgos potenciales o experiencias relacionadas que permiten determinar primas: todo depende del nivel de la aversión al riesgo de los accionistas para aceptar los riesgos y de la capacidad económica de la empresa para aceptar esos negocios.\nEn estos casos es imprescindible el proceso de monitoreo de la evolución del riesgo.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducción a las operaciones de seguros</span>"
    ]
  },
  {
    "objectID": "lectura_03.html#ley-de-los-grandes-números",
    "href": "lectura_03.html#ley-de-los-grandes-números",
    "title": "4  Introducción a las operaciones de seguros",
    "section": "4.6 Ley de los grandes números",
    "text": "4.6 Ley de los grandes números\nComo hemos dicho, la mutualización de riesgos no funciona con pocos asegurados, y funciona mejor cuanto más grande sea el número de asegurados.\nPara medir este hecho el seguro se sustenta en una de las leyes más importantes de la teoría de probabilidades:\n\nLey de los grandes números\nLas frecuencias relativas (% de ocurrencia) del resultado de un experimento aleatorio, tienden a estabilizarse en un valor cuando el número de experimentos es grande. Ese número es una aproximación de la probabilidad de ese resultado.\n\nMás adelante volveremos a revisar este tema desde un punto de vista más formal.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducción a las operaciones de seguros</span>"
    ]
  },
  {
    "objectID": "lectura_03.html#microeconomía-del-negocio-de-seguros",
    "href": "lectura_03.html#microeconomía-del-negocio-de-seguros",
    "title": "4  Introducción a las operaciones de seguros",
    "section": "4.7 Microeconomía del negocio de seguros",
    "text": "4.7 Microeconomía del negocio de seguros\nEl seguro es un negocio de libre competencia y con fines de lucro.\n\n4.7.1 Ciclo de producción\nEn el negocio del seguro se produce el fenómeno conocido como inversión del ciclo de producción:\n\nEl asegurador fija el precio antes de conocer el costo del producto y por lo tanto asume obligaciones frente a los asegurados.\nComo consecuencias las compañías de seguros captan dinero del público y por esta razón:\nDeben conformar reservas técnicas\nSon controladas por organismos de control estatal.\n\n\n\n4.7.2 Reservas técnicas\nEl asegurador no puede disponer inmediatamente de las primas que recibe.\nDebe constituir reservas o provisiones técnicas con la finalidad de mantener niveles mínimos de solvencia al cierre de cada ejercicio y garantizar el cumplimiento de las obligaciones financieras que deberá honrar en el futuro por la cobertura de siniestros.\n– Ref. Norma de reservas técnicas\nUna gran parte de las primas que percibe la compañía de seguros se registran en el pasivo del balance, en calidad de reservas y representan las obligaciones futuras del asegurador con los asegurados:\n\nContablemente un gran parte de las primas se registran en el pasivo y conforme se devengan pasan a constituir los ingresos financieros dela compañía.\nLas reservas se respaldan por las inversiones que obligatoriamente debe realiza la compañía y registrar en el activo del balance.\n\nEl principal riesgo del asegurador está en su pasivo.\n\n\n4.7.3 Control estatal\nLa inversión del ciclo de producción implica que los asegurados corren el riesgo de quiebra asegurador y éste no pueda cumplir con sus obligaciones de cubrir los siniestros en el futuro:\n\nEsto se conoce como riesgo de insolvencia del asegurador\n\nPor esta razón los gobiernos han instituido la supervisión de las compañías de seguros a través de entidades de control cuyo objetivo es proteger a los asegurados, garantizando la solvencia del mercado asegurador en el largo plazo:\n\nSuperintendencia de Compañías, Valores y Seguros (SCVS)\n\nLeyes\nNormas de solvencia, nacionales e internacionales\nEj.: resoluciones de la SCVS, Solvencia I y II\n\nOtras instituciones de control\n\nJunta de Política y Regulación Financiera (JPRF)\n\n\nEn consecuencia las compañías de seguros son compañías especializadas que ofrecen garantías suficientes y están legalmente autorizadas para poder ejercer esta actividad.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducción a las operaciones de seguros</span>"
    ]
  },
  {
    "objectID": "lectura_03.html#el-seguro-privado",
    "href": "lectura_03.html#el-seguro-privado",
    "title": "4  Introducción a las operaciones de seguros",
    "section": "4.8 El seguro privado",
    "text": "4.8 El seguro privado\nEl seguro (privado) es un contrato mediante el cual una de las partes, el asegurador, se obliga, a cambio del pago de una prima, a indemnizar a la otra parte, dentro de los límites convenidos, de una pérdida o un daño producido por un acontecimiento incierto; o a pagar un capital o una renta, si ocurre la eventualidad prevista en el contrato.\n– Ref. Art. 1, Decreto Supremo 1147 del 29/Nov/1963\n\nActores\n\nEl asegurado\n\nEl tomador del seguro\nEl portador del riesgo\nEl beneficiario\n\nLa empresa de seguros\nLas empresas de reaseguros o retrocesiones",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducción a las operaciones de seguros</span>"
    ]
  },
  {
    "objectID": "lectura_03.html#clasificación-según-la-materia-asegurada",
    "href": "lectura_03.html#clasificación-según-la-materia-asegurada",
    "title": "4  Introducción a las operaciones de seguros",
    "section": "4.9 Clasificación según la materia asegurada",
    "text": "4.9 Clasificación según la materia asegurada\n\nSeguros de personas\n\n\nDestinados a asegurar los riesgos que afecten la vida, la integridad física o la situación familiar de las personas.\n\n\nSeguros generales o de daños\n\n\nDestinados a cubrir los riesgos que pueden amenazar al patrimonio de las personas o instituciones.\n\n– Ref. Decreto 1147\n\nSeguros de personas\n\nSeguros de vida\nSeguros de salud o asistencia médica\nSeguros de accidentes\nSeguros de natalidad\n\n\n\nSeguros generales o de daños\n\nSeguro de cosas\nEl objeto amenazado es un bien del patrimonio del asegurado\nEj: incendio, robo, inundación,…\nSeguro de responsabilidad\nEl objeto amenazado es el patrimonio global del asegurado\nEj: responsabilidad civil, responsabilidad médica",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducción a las operaciones de seguros</span>"
    ]
  },
  {
    "objectID": "lectura_03.html#clasificación-según-el-beneficio",
    "href": "lectura_03.html#clasificación-según-el-beneficio",
    "title": "4  Introducción a las operaciones de seguros",
    "section": "4.10 Clasificación según el beneficio",
    "text": "4.10 Clasificación según el beneficio\n\nSeguros de compensación o de sumas fijas\n\n\nEl beneficio es fijado de antemano sin referencia a la magnitud del siniestro\nEj: 50 USD diarios por hospitalización\nEj: 20,000 USD en caso de muerte.\n\n\nSeguros de indemnización\n\n\nCubren el perjuicio real del siniestro, según las condiciones del contrato\nEj: incendio de un inmueble.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducción a las operaciones de seguros</span>"
    ]
  },
  {
    "objectID": "lectura_03.html#clasificación-según-la-regulación",
    "href": "lectura_03.html#clasificación-según-la-regulación",
    "title": "4  Introducción a las operaciones de seguros",
    "section": "4.11 Clasificación según la regulación",
    "text": "4.11 Clasificación según la regulación\n\nSeguros de vida\n\n\nCubren los riesgos de las personas o garantizan a éstas, dentro o al término de un plazo, un capital o una renta periódica\n\n\nSeguros generales (de daños)\n\n\nCubren los riesgos causados por afecciones, pérdidas o daños de la salud, de los bienes o del patrimonio y fianzas o garantías.\n\n– Ref. Art. 3, Ley General de Seguros.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducción a las operaciones de seguros</span>"
    ]
  },
  {
    "objectID": "lectura_03.html#visita-web-de-la-scvs-y-jprf",
    "href": "lectura_03.html#visita-web-de-la-scvs-y-jprf",
    "title": "4  Introducción a las operaciones de seguros",
    "section": "4.12 Visita web de la SCVS y JPRF",
    "text": "4.12 Visita web de la SCVS y JPRF",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducción a las operaciones de seguros</span>"
    ]
  },
  {
    "objectID": "lectura_03.html#principios-básicos-del-seguro",
    "href": "lectura_03.html#principios-básicos-del-seguro",
    "title": "4  Introducción a las operaciones de seguros",
    "section": "4.13 Principios básicos del seguro",
    "text": "4.13 Principios básicos del seguro\n\nBuena fe\nInterés asegurable\nIndemnización\nSubrogación\n\n\nPrincipio de buena fe\nTanto el asegurador como el asegurado deben actuar con máxima honestidad, moral y buena fe.\nEl asegurado debe declarar todas las características del riesgo que se asegura.\nEl asegurador debe explicar con claridad y detalle las condiciones de la cobertura que ofrece.\n\n\nPrincipio del interés asegurable\nEl asegurado debe sufrir una pérdida económica si el riesgo asegurado se realiza.\nNo se puede asegurar legalmente nada que no tenga interés asegurable.\nEl interés debe ser lícito.\n\nEn seguros de personas, toda persona tiene interés asegurable:\nEn su propia vida;\nEn la de las personas a quienes pueda reclamar alimentos, y\nEn la de aquellas cuya muerte pueda aparejarle un perjuicio económico.\nEn los amparos accesorios de gastos que tengan carácter de daño patrimonial, como gastos médicos, clínicos, quirúrgicos o farmacéuticos\nSon susceptibles de indemnización y se regulan por las normas relativas a los seguros de daños.\nEn seguros de daños puede ser objeto de contrato de seguro todo interés económico en que una persona tenga en que no se produzca un siniestro.\n\n– Ref. Decreto 1147\n\n\nPrincipio de indemnización\nLa indemnización no puede exceder el valor real del interés asegurado ni el monto efectivo del prejuicio patrimonial ni el límite de la suma asegurada.\nEl seguro no puede constituir fuente de enriquecimiento del asegurado.\nA opción del asegurador la indemnización es pagadera en dinero mediante la reposición, reparación o reconstrucción de la cosa asegurada.\nEn los seguros de personas el valor del interés asegurable no tiene otro límite que el que libremente le asignen las partes contratantes.\n– Ref. Decreto 1147\n\n\nPrincipio de subrogación\n\nEl asegurador sustituye al asegurado en el ejercicio de las acciones o derechos que tendría este, a fin de recuperar la indemnización. – Diccionario MAPFRE de seguros\nEl asegurador que haya pagado una indemnización de seguro se subroga, por Ministerio de la Ley, hasta el monto de dicha indemnización, en los derechos del asegurado contra terceros responsables del siniestro.\n\n– Ref. Decreto 1147\n\nConsecuencia -Bienes recuperables pasan a ser propiedad de la compañía -Ej. carros robados y recuperados.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducción a las operaciones de seguros</span>"
    ]
  },
  {
    "objectID": "lectura_03.html#seguro-o-apuesta",
    "href": "lectura_03.html#seguro-o-apuesta",
    "title": "4  Introducción a las operaciones de seguros",
    "section": "4.14 Seguro o apuesta?",
    "text": "4.14 Seguro o apuesta?\n\n\nCode\ntbd &lt;- data.table(\nmatrix( c(\n    'Respaldo legal estricto',              'Puede ser ilegal',\n    'Interés asegurable indispensable',     'Carece de interés asegurable',\n    'Máxima buena fe',                      'No necesariamente de buena fe',\n    'Nunca es fuente de enriquecimiento',   'Puede ser fuente de enriquecimiento' \n  ),\n  nrow = 4, ncol = 2, byrow = TRUE )\n)\ntbd %&gt;%\n  kable(\n    label = NA,\n    caption = 'Seguros vs Apuestas',\n    row.names = FALSE,\n    col.names = c( 'Seguros', 'Apuestas' ),\n    align = 'll',\n    digits = c( 0, 0 ),\n    format.args = list( big.mark = ',', decimal.mark = '.', scientific = FALSE ),\n    escape = FALSE,\n    centering = TRUE, \n    booktabs = TRUE ) %&gt;%\n  kable_classic( full_width = FALSE, html_font = \"Cambria\", position = \"center\" )\n\n\n\nSeguros vs Apuestas\n\n\nSeguros\nApuestas\n\n\n\n\nRespaldo legal estricto\nPuede ser ilegal\n\n\nInterés asegurable indispensable\nCarece de interés asegurable\n\n\nMáxima buena fe\nNo necesariamente de buena fe\n\n\nNunca es fuente de enriquecimiento\nPuede ser fuente de enriquecimiento\n\n\n\n\n\n\n\nEstas características permiten distinguir claramente las operaciones de seguro de las apuestas o la especulación financiera.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducción a las operaciones de seguros</span>"
    ]
  },
  {
    "objectID": "lectura_03.html#fuentes-del-riesgo-en-seguros",
    "href": "lectura_03.html#fuentes-del-riesgo-en-seguros",
    "title": "4  Introducción a las operaciones de seguros",
    "section": "4.15 Fuentes del riesgo en seguros",
    "text": "4.15 Fuentes del riesgo en seguros\n\nNúmero de reclamos: frecuencia\nCosto de los reclamos: severidad\nPeríodo de cobertura\nPeríodo de pago de primas\nPeríodo de pago de siniestros\nDesde la fecha del reclamo hasta el cierre legal.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducción a las operaciones de seguros</span>"
    ]
  },
  {
    "objectID": "lectura_03.html#problemas-que-enfrentan-los-seguros",
    "href": "lectura_03.html#problemas-que-enfrentan-los-seguros",
    "title": "4  Introducción a las operaciones de seguros",
    "section": "4.16 Problemas que enfrentan los seguros",
    "text": "4.16 Problemas que enfrentan los seguros\nIdentificamos tres problemas principales:\n\nAsegurabilidad de riesgos\nSelección adversa (anti-selección)\nAzar moral\n\n\n4.16.1 Asegurabilidad de riesgos\nRecordemos que el seguro se fundamenta en la compensación o mutualización de riesgos que no siempre es posible aplicar, y además, el seguro enfrenta algunas limitaciones técnicas. Por lo tanto existen riesgos que no son siempre asegurables y su transferencia dependerá de la capacidad financiera y aversión al riesgo de la compañía de seguros.\n\n\nRiesgos asegurables (deseables)\n\nRiesgos comparables y de naturaleza homogénea\nEs necesario que se puedan mutualizar\n\nPara que se cumpla la ley de los grandes números\n\nRiesgos medibles\n\nresponden a una ley de probabilidad\npermitan estimar la pérdida máxima\n\nRiesgos no potestativos: que no dependen de la voluntad del asegurado\nRiesgos inciertos\n\nQue no hayan ocurrido o que su ocurrencia sea desconocida.\n\n\n\n\n4.16.2 Selección adversa\nEs un fenómeno causado porque la información del asegurador es incompleta y asimétrica:\n\nEl asegurado es quien conoce mejor el riesgo.\n\nEntonces el seguro atrae con más fuerza a quienes tienen mayor exposición al riesgo y el portafolio puede resultar desequilibrado, asegurando demasiados “malos riesgos” que causan el desequilibrio financiero\n\n\nMedidas contra la selección adversa\nCon la finalidad de equilibrar el portafolio el mercado a puesto en práctica algunas medidas:\n\nPersonalización de la prima para asegurados siniestrosos\n\nEj. bonus malus\n\nProceso de selección de asegurados en base a declaración del riesgo\nPotestad de la compañía para rechazar un seguro\nObligación de asegurar?\nPeríodo de carencia\nLímites de cobertura\nCopago, franquicia, deducible, suma máxima asegurada\n\n\n\n4.16.3 Azar moral\nEs el riesgo de que el asegurado se desinterese de prevenir la ocurrencia de siniestros:\n\nEj. Seguro automotriz\n\no se interese en que ocurra el riesgo:\n\nEj. Por publicidad\n\no incremente la frecuencia de reclamos\n\nEj. Seguro de salud\n\n\n\nMedidas contra el azar moral\nCon el objetivo de incentivar la prevención, el mercado ha desarrollado algunas estrategias:\n\nCopago y/o deducible\nObligaciones de prevención previstas en el contrato\nCobertura máxima\n\nPor siniestro\nPor período\n\nBono no-claims.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducción a las operaciones de seguros</span>"
    ]
  },
  {
    "objectID": "lectura_03.html#primas-de-seguro-art.-53-rlgs",
    "href": "lectura_03.html#primas-de-seguro-art.-53-rlgs",
    "title": "4  Introducción a las operaciones de seguros",
    "section": "4.17 Primas de seguro: Art. 53 RLGS",
    "text": "4.17 Primas de seguro: Art. 53 RLGS\n\nPrima pura o prima de riesgo: es el costo del riesgo\nPrima neta: prima pura más un recargo de seguridad\nPrima de tarifa: prima neta más recargos de operación:\n\ngastos de adquisición,\ngastos de administración,\nutilidad razonable\notros gastos justificados (legales, inspección, …)\n\nPrima comercial: prima de tarifa más impuestos, contribuciones, etc.\n\n\nRelación fundamental\n\\[ P_{tar} = \\frac{P}{1-GA-GB-U} \\] \\(P\\): prima pura\n\\(P_{tar}\\): prima de tarifa\n\\(GA\\): porcentaje de gastos de administración\n\\(GB\\): porcentaje de gastos de adquisición (comisiones de brokers)\n\\(U\\): porcentaje de utilidad esperada por los accionistas\nLos porcentajes se calculan sobre la prima de tarifa.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducción a las operaciones de seguros</span>"
    ]
  },
  {
    "objectID": "lectura_03.html#caso-nessie",
    "href": "lectura_03.html#caso-nessie",
    "title": "4  Introducción a las operaciones de seguros",
    "section": "4.18 Caso Nessie",
    "text": "4.18 Caso Nessie\n\nEstudio de caso.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducción a las operaciones de seguros</span>"
    ]
  },
  {
    "objectID": "lectura_04.html",
    "href": "lectura_04.html",
    "title": "5  Probabilidad y Estadística",
    "section": "",
    "text": "5.1 Probabilidad y conceptos asociados\nAntes de introducir algunos conceptos necesarios para nuestro estudio, necesitamos de algunos conceptos de base.\nEmpezamos con algunas definiciones que serán de utilidad siempre a lo largo de toda esta exposición. Son ideas fundamentales que se utilizan continuamente en la elaboración de diferentes modelos actuariales y en general estadísticos.\nEn relación a lo anterior, también tenemos estas definiciones ya más formales.\nMuchas propiedades en la teoría de probabilidades se cumplen en un sentido más débil, bajo la consideración de la medida de probabilidad \\(P\\) salvo cualquier conjunto de medida nula. Esto se debe que a que existen eventos en un experimento que tienen probabilidad nula de presentarse.\nEn muchas ocasiones se observa los resultados a través de la realización de un experimento, y es usual que a cada experimente se le asocie un único resultado. Esta noción permite la definición de una variable aleatoria.\nDe la idea de variable aleatoria a valores reales se puede extender fácilmente al caso de varias variables aleatorias, un vector aleatorio que toma valores en \\(\\R^n\\), i.e. una función medible \\(X : \\Omega \\longrightarrow \\R^n\\).\nCuando se trata de un variable aleatoria con varias componentes reales \\(X = ( X_1, \\ldots, X_n )\\), o lo que es lo mismo una secuencia finita de varias variables aleatorias a valores reales. Se puede extender la definición de distribución acumulada para la variable \\(X\\), si consideramos valores reales \\(x = ( x_1, \\ldots, x_n ) \\in \\R^n\\), así definimos la forma más general de la distribución acumulada a varias variables. \\[\nF\\left( x_1, \\ldots, x_n \\right)\n= P\\left( X_1 \\leq x_1 \\land \\cdots \\land X_n \\leq x_n \\right)\n= P\\left( \\bigcap_{i=1}^n X_i^{-1}\\left( (-\\infty, x_i ] \\right) \\right)\n\\] Es decir, \\(F\\) es tan solo la probabilidad de la intersección de los eventos de la forma \\(X_i^{-1}\\left( (-\\infty, x_i ] \\right)\\).\nHay un caso bastante conocido, el mismo es el más presentado en el curso introductorio de teoría de probabilidades, precisamente se hace uso del teorema fundamental del cálculo para poder establecer la siguiente definición.\nPara cuando tratamos con dos variables aleatorias \\(X\\) y \\(Y\\), si en caso existe y está bien definida las derivadas hasta el segundo orden de la función de distribución acumulada conjunta \\(F_{X,Y}\\). En tal caso se puede definir la respectiva densidad de probabilidad \\[\nf_{X,Y}( x, y ) = \\frac{\\D^2 F_{X,Y}}{\\D x \\D y}( x, y )\n\\]\nAsí mismo, para la densidad de probabilidad conjunta, si en caso las variables aleatoria \\(X\\) y \\(Y\\) son independientes, también se puede factorar la densidad de probabilidad. \\[\nf_{X,Y}( x, y ) = f_X( x ) f_Y( y )\n\\]\nEn el desarrollo a continuación haremos bastante uso de familias de variables aleatorias \\(X_1, \\ldots, X_n\\) las cuales muchas de las veces se consideraran que son independientes entre si e idénticamente distribuidas, es usual designarlas con las siglas i.i.d.. Esta situación si las \\(X_1, \\ldots, X_n\\) siguen la misma distribución, su distribución conjunta tiene la siguiente forma: \\[\n\\begin{split}\nF( x_1, \\ldots, x_n )\n& = P( X_1 \\leq x_1 \\land \\cdots \\land X_n \\leq x_n ), \\qquad \\text{por definición} \\\\\n& = \\prod\\limits_{i=1}^n P( X_i \\leq x_i ),\\qquad \\text{si las variables $X_i$ son independientes} \\\\\n& = \\prod\\limits_{i=1}^n F_{X_i}( x_i ) \\\\\n& = \\prod\\limits_{i=1}^n F_{X}( x_i ), \\qquad \\text{si las variables $X_i$ son idénticamente distribuidas}\n\\end{split}\n\\]\nLa siguiente función es utilidad para comprender algunos resultados en teoría de probabilidades. También, es bastante útil para realizar de forma más clara y rápida algunos cálculos.\nHay un caso atípico que suele ser útil, esto sucede cuando la variable aleatoria \\(X : \\Omega \\longrightarrow \\R\\), es constante, quiere decir que tenemos un \\(a \\in \\R\\), tal que \\(X( \\omega ) = a\\) para todo \\(\\omega \\in \\Omega\\). En este caso la distribución de probabilidad acumulada \\(F\\) de \\(X\\), tiene la siguiente forma particular. \\[\nF( x ) = P( X \\leq x ) = \\mathbf{1}_{[a, +\\infty)}( x )\n\\]\nPara muchos resultados asociados a la estimación de estadística la variantes empíricas \\(P_n\\) y \\(F_n\\) serán de mucha de utilidad.\nDe esto resulta que la esperanza jamás puede ser mayor que cualquiera de los valores que toma la variable aleatoria \\(X\\).\nLa función indicatriz \\(\\mathbf{1}_A\\) sobre un evento \\(A\\) del espacio muestral \\(\\Omega\\), también se puede interpretar como una variable aleatoria, que tan solo tomo los valores \\(0\\) o \\(1\\). Es más, su esperanza es precisamente la probabilidad del evento \\(A\\). \\[\n\\E\\left[ \\mathbf{1}_A \\right]\n= \\int\\limits_\\Omega \\mathbf{1}_A( \\omega )\\ dP( \\omega )\n= \\int\\limits_A dP( \\omega )\n= P( A )\n\\] Con esta propiedad, podemos obtener la siguiente representación para la función de distribución acumulada de una variable aleatoria a valores reales \\(X\\). \\[\nF( X \\leq x )\n= P( X \\leq x )\n= \\E\\left[ \\mathbf{1}_{(-\\infty,x]}( X ) \\right]\n\\]\nSi \\(X_1, \\ldots, X_n\\) son variables aleatorias y \\(Y = X_1 + \\cdots + X_n\\), entonces \\[\n\\begin{split}\nM_Y( t )\n& = \\E\\left[ \\exp\\left( t Y \\right) \\right] \\\\\n& = \\E\\left[ \\exp\\left( t \\sum\\limits_{i=1}^n X_i \\right) \\right] \\\\\n& = \\prod\\limits_{i=1}^n \\E\\left[ \\exp\\left( t X_i \\right) \\right],\\qquad\n\\text{si las variables $X_i$ son independientes} \\\\\n& = \\left( \\E\\left[ \\exp\\left( t X \\right) \\right] \\right)^n,\\qquad\n\\text{si las variables $X_i$ son identicamente distribuidas} \\\\\n& = \\left( M_X( t ) \\right)^n\n\\end{split}\n\\]\nPrecisamente se conoce a \\(\\varphi_X\\) como función característica de una variable aleatoria \\(X\\), debido al siguiente resultado.\nCon el resultado anterior sabemos que si conocemos de alguna manera la función característica \\(\\varphi_X\\) de una variable aleatoria \\(X\\), entonces podemos aproximar su distribución acumulada con una evaluación numérica de la integral anterior.\nPara el estudio de los variaciones que puede presentar una variable aleatoria \\(X\\) se puede considerar otra medida de tendencia la cual precisamente está enfocada a medir cuanto se aleja de la media una variable aleatoria \\(X\\). Esta medida, en las aplicaciones financieras está fuertemente asociada a lo que se conoce como volatilidad.\nEn más de una ocasión se tendrá la necesidad de agregar variables aleatorias, sumarlas entre si, en este caso se tiene la necesidad de determinar su distribución acumulada en función de las distribuciones acumuladas que se consideraron en la suma.\nPara cuando se realiza la convolución de varias veces la misma función, se opta por una notación más compacta \\(f^{\\star k}\\), para el producto de convolución \\(k\\)-veces la misma función \\(f \\star f \\star \\cdots \\star f\\).\nCuando se tiene dos variables aleatorias independientes \\(X\\) y \\(Y\\), muchas de las veces nos interesamos a trabajar con la variable aleatoria dada por el mínimo entre estas variables, i.e. \\(Z = \\min( X, Y )\\). De ello surge la necesidad de determinar la distribución de probabilidad acumulada \\(F_Z\\) de \\(Z\\), a partir de las distribuciones de \\(F_X\\) de \\(X\\) y \\(F_Y\\) de \\(Y\\). \\[\n\\begin{split}\nF_Z( z )\n& = P( Z \\leq z ) \\\\\n& = 1 - P( Z &gt; z ) \\\\\n& = 1 - P( \\min(X,Y) &gt; z ) \\\\\n& = 1 - P( X &gt; z \\land Y &gt; z ) \\\\\n& = 1 - P( X &gt; z ) P( Y &gt; z ) \\\\\n& = 1 - \\left( 1 - F_X( z ) \\right) \\left( 1 - F_Y( z ) \\right) \\\\\n& = F_X( z ) + F_Y( z ) - F_X( z ) F_Y( z )\n\\end{split}\n\\]\nEn particular cuando \\(Y = a\\) es constante tenemos la siguiente distribución de probabilidad para \\(Z = \\min( X, Y ) = \\min( X, a )\\) \\[\nF_Z( z ) = F_X( z ) + \\mathbf{1}_{[a,+\\infty)}( z ) - \\mathbf{1}_{[a,+\\infty)}( z ) F_X( z )\n= \\mathbf{1}_{(-\\infty,a)}( z ) F_X( z ) + \\mathbf{1}_{[a,+\\infty)}( z )\n\\] Esta función tiene las siguientes propiedades.\nDe forma similar nos podemos también interesar a la variable aleatoria que expresa el máximo entre otras dos variables, i.e. \\(Z = \\max( X, Y )\\), por un razonamiento similar podemos obtener la distribución de probabilidad \\(F_Z\\) de \\(Z\\). \\[\n\\begin{split}\nF_Z( z )\n& = P( Z \\leq z ) \\\\\n& = P( \\max(X,Y) \\leq z ) \\\\\n& = P( X \\leq z \\land Y \\leq z ) \\\\\n& = P( X \\leq z ) P( Y \\leq z ) \\\\\n& = F_X( z ) F_Y( z )\n\\end{split}\n\\]\nDe forma análoga el caso cuando \\(Y = a\\) es una constante se reduce a la siguiente distribución de probabilidad para \\(Z\\). \\[\nF_Z( z ) = \\mathbf{1}_{[a,+\\infty)}( z ) F_X( z )\n\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probabilidad y Estadística</span>"
    ]
  },
  {
    "objectID": "lectura_04.html#probabilidad-y-conceptos-asociados",
    "href": "lectura_04.html#probabilidad-y-conceptos-asociados",
    "title": "5  Probabilidad y Estadística",
    "section": "",
    "text": "Definition 5.1: Conceptos de base\nEn breves términos, utilizaremos los siguientes conceptos\n\nUn fenómeno es un hecho que puede ser observado.\nUn fenómeno estocástico es un fenómeno sobre el que se realiza un experimento que puede arrojar más de un resultado posible.\nUn experimento es la observación de un fenómeno bajo condiciones específicas.\nUn resultado es la información obtenida de un experimento.\n\n\n\n\n\n\nDefinition 5.2: Espacio muestral y eventos\nEl conjunto de todos los resultados lo denominamos espacio muestral y usualmente lo notamos por \\(\\Omega\\).\nUn evento es un conjunto de uno o más resultados posibles. En pocas palabras un evento \\(A\\) es un subcojunto de \\(\\Omega\\), i.e. \\(A \\subset \\Omega\\).\nEn algunas ocasiones, para dar mayor precisión a la naturaleza de los eventos, si este está asociado a un fenómeno estocástico se denomina evento contingente.\nNo todos los subconjuntos de \\(\\Omega\\) necesariamente son un evento, usualmente hay subconjuntos que no pueden resultar de un experimento. Así, si agrupamos todos los eventos usualmente tenemos un conjunto menor a las partes \\(\\mathcal{P}(\\Omega)\\) de \\(\\Omega\\). El conjunto de todos los eventos se conoce como una álgebra de eventos, \\(\\sigma\\)-álgebra o tribu. Usualmente se la nota por \\(\\F\\), es claro que \\(\\F \\subset \\mathcal{P}(\\Omega)\\).\nEl álgebra de eventos, se denomina así, ya que usualmente es cerrada para algunas operaciones de conjuntos. Tiene las siguientes propiedades:\n\nTodos los resultados de espacio muestral están en \\(\\F\\), es decir \\(\\Omega \\in \\F\\)\nEl complemento de un evento también es parte de \\(\\F\\), es decir si \\(A \\in \\F\\), entonces \\(A^c = \\Omega \\setminus A \\in \\F\\).\nLa unión de eventos es un evento, si \\(A, B \\in \\F\\), entonces \\(A \\cup B \\in \\F\\),\nAunque redunde, la intersección de eventos es un evento, si \\(A, B \\in \\F\\), entonces \\(A \\cap B \\in \\F\\).\n\n\n\n\n\nDefinition 5.3: Medida de probabilidad\nLa probabilidad es una medida de las posibilidades de que ocurra un evento contingente que toma valores en \\([0,1]\\). Formalmente, la probabilidad se la define como una medida, esto es una función de conjuntos \\[\nP: \\F \\longrightarrow [0,1]\n\\] sobre los eventos \\(\\F \\subset \\mathcal{P}( \\Omega )\\) del espacio muestral \\(\\Omega\\).\n\nSi \\(A, B \\in \\F\\) son disjuntos \\(A \\cap B = \\emptyset\\), entonces \\(P( A \\cup B ) = P( A ) + P( B )\\)\n\\(P( \\Omega ) = 1\\)\n\\(P( \\emptyset ) = 0\\)\n\n\n\n\n\nDefinition 5.4: Casi en todas partesUna propiedad sobre sobre \\(\\Omega\\) viene caracterizada por una función de verificación \\(V : \\Omega \\longrightarrow \\{0,1\\}\\) de tal forma, se dice que un evento \\(A \\in \\F\\) satisface la propiedad dada por \\(V\\) si \\(V(\\omega) = 1\\), para todo \\(\\omega \\in A\\). Entonces decimos que se satisface la propiedad en \\(V\\) en casi en todas partes o casi seguramente si para todo evento \\(N \\in \\F\\) que no satisfaga \\(V\\) mantiene una medida nula, es decir \\(P( N ) = 0\\) y para cualquier \\(\\omega \\in \\Omega \\setminus N\\) se satisface \\(V\\), \\(V( \\omega ) = 1\\). En otras palabras en ciertos experimentos hay resultados que se pueden presentar, están dentro de las opciones, sin embargo en caso de presentarse no hay forma que se pueda medirlos.\n\n\n\n\n\nDefinition 5.5: Variable aleatoriaUna variable aleatoria es una función que asigna un valor numérico a todo evento contingente.\nUna variable aleatoria \\(X: \\Omega \\longrightarrow D\\) que parte del espacio muestral \\(\\Omega\\) y toma valores en el conjunto de los números reales \\(\\R\\). Como la variable aleatoria es el resultado de un experimento, el cual debe ser medible, es natural esperar que todo intervalo observado sea el resultado de un evento en el espacio muestra \\(\\Omega\\). De forma más clara, la imagen recíproca de un cualquier intervalo \\(A \\subset \\R\\) es un evento en \\(\\F\\), i.e. \\(X^{-1}( A ) \\in \\F\\) esto precisamente se designa como una función medible.\nDe \\(X\\) se puede construir o heredar otra medida a partir de \\(P\\), esta se representa \\(P_X\\) y tan solo mide los eventos que son imagen de \\(X\\). Es decir para cualquier intervalo \\(A \\in \\R\\) \\[\n\\begin{split}\nP_X( A )\n& = P( X^{-1}( A ) )\\qquad \\text{definición de la notación} \\\\\n& = P(\\{ \\omega \\in \\Omega \\mid X( \\omega ) \\in A \\})\\qquad \\text{definición de imagen recíproca} \\\\\n& = \\int\\limits_{X^{-1}( A )} dP( \\omega )\\qquad \\text{representación en forma integral}\n\\end{split}\n\\] a partir de la medida \\(P_X\\) precisamente se puede determinar algunas funciones que se conocen como distribuciones o densidades de probabilidad.\n\n\n\n\n\nDefinition 5.6: Variable aleatoria discretaUna variable aleatoria \\(K: \\Omega \\longrightarrow D\\) que parte del espacio muestral \\(\\Omega\\) y toma valores en un conjunto discreto \\(D = \\left\\{k_i \\in \\R | i \\in \\mathbb{N} \\right\\}\\), sigue una probabilidad discreta dada por las probabilidades \\(p_i \\in [0,1]\\) , \\(i \\in \\mathbb{N}\\), si \\[\nP\\left( K = k_i \\right) = p_i,\\qquad \\forall i \\in \\mathbb{N}\n\\] Además se cumple la condición de normalización que es muy importante. \\[\n\\sum\\limits_{i = 0}^{\\infty} p_i = 1\n\\] las probabilidades ¡Nunca son negativas! y !Suman siempre 1!.\n\n\n\n\nDefinition 5.7: Función de distribución acumuladaConsideramos una variable aleatoria a valores reales \\(X\\), la función de distribución acumulada \\(F\\) asociada a la variable aleatoria \\(X\\), está dada por la siguiente relación: \\[\nF( x )\n= P( X \\leq x )\n= P_X( (-\\infty, x] )\n= P( X \\in (-\\infty, x] )\n= P\\left( X^{-1}\\left( (-\\infty, x ] \\right) \\right)\n\\]\nLa función de distribución acumulada, tiene las siguientes propiedades:\n\nPara cualquier \\(x \\in \\R\\), \\(0 \\leq F( x ) \\leq 1\\),\nLa función \\(F\\) es no decreciente,\nLa función \\(F\\) es continua por derecha,\nSe satisfacen los siguientes límites:\n\n\\[\n\\underset{x \\rightarrow -\\infty}{\\lim} F( x ) = 0\\qquad\n\\underset{x \\rightarrow +\\infty}{\\lim} F( x ) = 1\n\\]\n\nLa función \\(F\\) es a variación acotada [1]. Esto quiere decir que existe un real \\(C &gt; 0\\), para cualquier partición \\(\\{x_i\\}_{i\\in\\Z}\\), tal que:\n\n\\[\n\\sum\\limits_{i\\in\\Z} \\left| F( x_{i+1} ) - F( x_i ) \\right| &lt; C\n\\]\n\n\n\n\n\nDefinition 5.8: Función supervivenciaLa función de supervivencia \\(S : \\R \\longrightarrow \\R\\) está asociada a una variable aleatoria \\(X\\), está dada por la siguiente: \\[\nS( x ) = 1 - F( x ) = 1 - P( X \\leq x ) = P( X &gt; x )\n\\]\n\n\n\n\n\nDefinition 5.9: Función de densidad de probabilidadLa densidad de probabilidad o también la ley de probabilidad de una variable aleatoria a valores reales \\(X\\), es una función \\(f: \\R \\longrightarrow \\R\\), tal que \\[\nP( a \\leq X \\leq b ) = F( b ) - F( a ) = \\int\\limits_a^b f( x )\\ dx\n\\]\nAsí se satisface la siguiente igualdad \\[\nF( x )\n= \\int\\limits_{(-\\infty,x]} f( x )\\ dx\n= \\int\\limits_{-\\infty}^x f( x )\\ dx\n\\]\nEsto implica que \\(f\\) es la derivada de \\(F\\), i.e. \\(\\frac{dF}{dx} = f\\), por tal razón \\(f\\) estará bien definida siempre y cuando la derivada de \\(F\\) esté bien definida.\n\n\n\nEl siguiente ejemplo muestra una forma típica de una distribución acumulada continua. En este caso la distribución acumulada de la normal estándar, la cual no admite una forma analítica. \\[\n\\Phi( x ) = \\frac{1}{\\sqrt{2\\pi}} \\int\\limits_{-\\infty}^x \\exp\\left( -\\frac{u^2}{2} \\right)\\ du\n\\]\n\n\nCode\nn &lt;- 1e3\nx &lt;- seq( -6, 6, length = n )\n\nxbrk &lt;- seq( -6, 6, 1 )\nxlbl &lt;- formatC( xbrk, digits = 0, format = 'f' )\nxlim &lt;- c( -6, 6 )\n\nybrk &lt;- seq( 0, 1, length = 11 )\nylbl &lt;- formatC( ybrk, digits = 2, format = 'f' )\nylim &lt;- c( 0, 1 )\n\nFx &lt;- sapply( x, FUN = function( x ) pnorm( x ) )\n\nplt &lt;- ggplot( ) +\n  geom_step( aes( x = x, y = Fx, colour = 'a' ), linewidth = 1 ) +\n  geom_vline( xintercept = c( 0 ), colour = 'orange', linewidth = 0.7 ) +\n  scale_colour_manual( breaks = c( 'a' ),\n                       values = c( 'dodgerblue4' ) ) +\n  scale_x_continuous( breaks = xbrk,\n                      labels = xlbl, \n                      limits = xlim, \n                      expand = c( 0.008, 0.008 ) ) +\n  scale_y_continuous( breaks = ybrk,\n                      labels = ylbl, \n                      limits = ylim, \n                      expand = c( 0.005, 0.005 ) ) +\n  xlab( TeX( \"$x$\" ) ) + \n  ylab( TeX( \"$\\\\Phi$\" ) ) + \n  theme_bw() +\n  theme( legend.position = \"none\" )\nplot( plt )\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition 5.10: Independencia de variables aleatoriasDos variables aleatorias a valores reales \\(X\\) y \\(Y\\), se dicen independientes si para cualquier par de eventos \\(A\\) y \\(B\\), sucede la siguiente factorización de probabilidades \\[\nP( X \\in A \\land Y \\in B ) = P( X \\in A ) P( Y \\in B)\n\\]\nLa propiedad anterior, en particular para la función de distribución conjunta, toma la siguiente forma: \\[\nF_{X,Y}( x, y ) = P( X \\leq x \\land Y \\leq y )\n= P( X \\leq x ) P( Y \\leq y )\n= F_{X}( x ) F_{Y}( y )\n\\]\n\n\n\n\n\n\n\n\nDefinition 5.11: Función indicatriz\nLa función indicatriz de un conjunto \\(A \\subset \\Omega\\), es la función \\(\\mathbf{1}_A : \\Omega \\longrightarrow \\{0,1\\}\\), que toma los valores \\(\\mathbf{1}_A( \\omega ) = 0\\), si \\(\\omega \\notin A\\) y \\(\\mathbf{1}_A( \\omega ) = 1\\), si \\(\\omega \\in A\\).\nAdemás esta función indicatriz tiene las siguientes propiedades\n\nSi \\(A, B \\subset \\Omega\\), entonces \\(\\mathbf{1}_{A \\cap B} = \\mathbf{1}_A \\mathbf{1}_B\\)\nSi \\(A \\subset \\Omega\\), entonces \\(\\mathbf{1}_{A^c} = 1 - \\mathbf{1}_A\\)\nSi \\(A, B \\subset \\Omega\\), entonces \\(\\mathbf{1}_{A\\cup B} = \\mathbf{1}_A + \\mathbf{1}_B - \\mathbf{1}_A \\mathbf{1}_B\\)\n\\(\\mathbf{1}_\\Omega = 1\\)\n\n\n\n\n\nLas constantes o variables aleatorias constantes son independientes de cualquier otra variable aleatoria.\n\n\nProof. Consideremos \\(X\\) una constante o variable aleatoria constante, tal que existe un \\(a \\in \\R\\), tal que \\(X( \\omega ) = a\\) para cualquier \\(\\omega \\in \\Omega\\) y \\(Y\\) una variable aleatoria cualquiera a valores reales. Para cualesquier intervalos \\(A, B \\subset \\R\\). Entonces tenemos dos casos, primero \\(X \\notin A\\), entonces \\(\\emptyset = \\{ \\omega \\in \\Omega \\mid X( \\omega ) \\in A \\}\\), por otra parte si \\(y \\in A\\), entonces \\(\\Omega = \\{ \\omega \\in \\Omega \\mid X( \\omega ) \\in A \\}\\), como consecuencia \\[\nP( X \\in A ) = P\\left( X^{-1}(A) \\right)\n= P\\left( \\{ \\omega \\in \\Omega \\mid X( \\omega ) \\in A \\} \\right)\n= \\mathbf{1}_{A}( a )\n\\]\nAhora consideramos los casos anteriores para la probabilidad conjunta, primero el caso \\(X \\notin A\\) \\[\n\\begin{split}\nP\\left( X \\in A \\land Y \\in B \\right)\n& = P\\left( X^{-1}(A) \\cap Y^{-1}(B) \\right) \\\\\n& = P\\left( \\left\\{ \\omega \\in \\Omega \\middle| X( \\omega ) \\in A \\right\\} \\cap\n\\left\\{ \\omega \\in \\Omega \\middle| Y( \\omega ) \\in B \\right\\} \\right) \\\\\n& = P\\left( \\emptyset \\cap \\left\\{ \\omega \\in \\Omega \\middle| Y( \\omega ) \\in B \\right\\} \\right) \\\\\n& = P( \\emptyset ) \\\\\n& = 0\n\\end{split}\n\\]\nEl segundo caso \\(X \\in A\\) \\[\n\\begin{split}\nP\\left( X \\in A \\land Y \\in B \\right)\n& = P\\left( X^{-1}(A) \\cap Y^{-1}(B) \\right) \\\\\n& = P\\left( \\left\\{ \\omega \\in \\Omega \\middle| X( \\omega ) \\in A \\right\\} \\cap\n\\left\\{ \\omega \\in \\Omega \\middle| Y( \\omega ) \\in B \\right\\} \\right) \\\\\n& = P\\left( \\Omega \\cap \\left\\{ \\omega \\in \\Omega \\middle| Y( \\omega ) \\in B \\right\\} \\right) \\\\\n& = P( Y \\in B )\n\\end{split}\n\\]\nEsto quiere decir en conclusión que \\[\nP( X \\in A \\land Y \\in B ) = \\mathbf{1}_{A}( a ) P( Y \\in B ) = P( X \\in A ) P( Y \\in B )\n\\] por tanto \\(X\\) y \\(Y\\) son independientes\n\n\n\nDefinition 5.12: Medida de probabilidad empíricaA partir de una muestra de \\(X_1, \\ldots, X_n\\) de una variable aleatoria \\(X\\), podemos definir también la medida de probabilidad empírica asociada a la muestra \\(X_1, \\ldots, X_n\\). Así para cualquier evento del espacio muestral \\(\\Omega\\), \\(A \\subset \\Omega\\) \\[\nP_n( B ) = \\frac{1}{n} \\sum\\limits_{i=1}^n \\mathbf{1}_{B}\\left( X_i \\right)\n\\]\nA partir de la medida empírica de probabilidad, podemos extraer la de para la distribución acumulada empírica \\(F_n\\), para ello consideremos eventos de la forma \\((-\\infty,x]\\) para cualquier \\(x \\in \\R\\). \\[\nF_n( x ) = P_n( (-\\infty,x])\n= \\frac{1}{n} \\sum\\limits_{i=1}^n \\mathbf{1}_{(-\\infty,x]}\\left( X_i \\right)\n\\]\n\n\n\n\n\nDefinition 5.13: Esperanza de una variable aleatoria\nConsiderando una variable aleatoria discreta \\(K : \\Omega \\longrightarrow D \\subset \\R\\), donde \\(D\\) es un conjunto discreto, es decir sus elementos se pueden contar y poner en correspondencia con \\(\\mathbb{N}\\). Entonces, la esperanza se define como: \\[\n\\E[ K ]\n= \\E_P[ K ]\n= \\sum\\limits_{i=0}^{\\infty} k_i P\\left( K = k_i \\right)\n= \\sum\\limits_{i=0}^{\\infty} k_i p_i\n\\]\nPara el caso de una variable aleatoria a valores reales, es decir una función medible \\(X : \\Omega \\longrightarrow \\R\\), la esperanza matemática está dada por \\[\n\\E[ X ]\n= \\E_P[ K ]\n= \\int\\limits_{\\mathbb{\\Omega}} X( \\omega )\\ dP( \\omega )\n= \\int\\limits_{\\R} x\\ dF( x )\n\\] donde la última integral de tipo Riemann-Stieltjes tiene sentido, ya que \\(F\\) es una función de variación acotada [1].\nCuando la función de densidad de probabilidad está bien definida es posible expresar y calcular la esperanza matemática como la siguiente expresión: \\[\n\\E[ X ]\n= \\int\\limits_{\\R} x f( x )\\ dx\n\\]\nLa esperanza matemática goza de las siguientes propiedades:\n\nLinealidad, \\(a \\in \\R\\) \\[\n\\E[ aX + Y ] = a \\E[ X ] + \\E[ Y ]\n\\]\nMonotonía, si \\(X \\leq Y\\), entonces \\[\n\\E[X] \\leq \\E[Y]\n\\]\nLa esperanza de una constante \\(a \\in \\R\\) es la misma constante. \\[\n\\E[a] = \\int\\limits_{\\R} a\\ dF_X( x ) = a\\int\\limits_{\\R} dF_X( x ) = a\n\\]\n\n\n\n\n\n\n\nDefinition 5.14: Función generadora de momentosLa función generadora de momentos de una variable aleatoria a valores reales \\(X\\), es la función: \\[\nM_X( t ) = \\E\\left[ \\exp( t X ) \\right]\n\\]\n\n\n\n\n\nDefinition 5.15: Función característicaLa función característica de una variable aleatoria a valores reales \\(X\\), es la función: \\[\n\\varphi_X( t ) = M_X( it ) = \\E\\left[ \\exp( i t X ) \\right]\n\\]\n\n\n\n\nTheorem 5.16: InversiónSea \\(\\varphi\\) la función característica de la distribución de probabilidad \\(F\\) de una variable aleatoria \\(X\\). Si \\(x,y \\in \\R\\) son dos puntos de continuidad de la distribución \\(F\\), entonces: \\[\nF( y ) - F( x ) = \\frac{1}{2\\pi} \\underset{c \\rightarrow +\\infty}{\\lim} \\int\\limits_{-c}^c\n\\frac{\\exp(-itx) - \\exp(-ity)}{it} \\varphi( t )\\ dt\n\\]\n\n\n\n\n\n\nDefinition 5.17: Varianza de una variable aleatoriaAsí mismo su varianza es la dada por: \\[\n\\V[ X ]\n= \\E\\left[ \\left( X - \\E[ X ] \\right)^2 \\right]\n= \\E\\left[ X^2 \\right] - \\E\\left[ X \\right]^2\n\\]\n\n\n\n\nDefinition 5.18: CovarianzaLa covarianza de dos variables aleatorias, está dada por la siguiente expresión: \\[\n\\mathbb{C}[ X, Y ]\n= \\E\\left[ \\left( X - \\E[ X ] \\right)\\left( Y - \\E[ Y ] \\right) \\right]\n= \\E\\left[ X Y \\right] - \\E\\left[ X \\right]\\E\\left[ Y \\right]\n\\]\nDe forma integral esta se la puede expresar como: \\[\n\\begin{split}\n\\mathbb{C}[ X, Y ]\n& = \\int\\limits_{\\R^2} xy\\ dP( x, y )\n- \\int\\limits_{\\R^2} x\\ dP( x, y )\\int\\limits_{\\R^2} y\\ dP( x, y ) \\\\\n& = \\int\\limits_{\\R^2} xy f_{X,Y}( x, y )\\ dx dy\n- \\int\\limits_{\\R^2} x f_{X,Y}( x, y )\\ dx dy\\int\\limits_{\\R^2} y f_{X,Y}( x, y )\\ dx dy\n\\end{split}\n\\]\nNo está por demás notar que \\(\\mathbb{C}[ X, X ] = \\V[ X ]\\)\n\n\n\n\n\nDefinition 5.19: Distribución de la suma de variables aleatoriasDadas dos variables aleatorias a valores reales \\(X\\) y \\(Y\\), con funciones de distribución acumulada \\(F_X\\) y \\(F_Y\\) respectivamente, la distribución acumulada \\(F_Z\\) de la variable aleatoria \\(Z = X + Y\\) está dada por la siguiente expresión: \\[\nF_Z( z )\n= P( Z \\leq z )\n= F_X \\star F_Y ( z )\n= \\int\\limits_{\\R} F_X( x - y )dF_Y( y )\n\\]\nsi en caso se puede definir las densidades de probabilidad \\(f_X\\) y \\(f_Y\\) para las variables aleatorias \\(X\\) y \\(Y\\), entonces: \\[\nf_Z( z ) = f_X \\star f_Y ( z )\n= \\int\\limits_{\\R} f_X( x - y ) f_Y( y )\\ dy\n\\]\nEl producto \\(\\star\\) se conoce como convolución de funciones, el mismo es simétrico.\n\n\n\n\n\n\nsi \\(z &gt; a\\), entonces \\(F_Z( z ) = 1\\)\nsi \\(z &lt; a\\), entonces \\(F_Z( z ) = F_X( z )\\)\nsi \\(z = a\\), entonces \\(F_Z( a ) = 1\\)\nla función \\(F\\) es discontinua en \\(a\\) \\[\n\\underset{z \\nearrow a}{\\lim} F_Z( z ) = \\underset{z \\nearrow a}{\\lim} F_X( z ) = F_X( a )\n\\] por otra parte \\[\n\\underset{z \\searrow a}{\\lim} F_Z( z ) = \\underset{z \\searrow a}{\\lim} \\mathbf{1}_{[a,+\\infty)}( z )\n= \\underset{z \\searrow a}{\\lim} 1 = 1\n\\] donde no necesariamente \\(F_X( a )\\) es igual \\(1\\).\nel evento puntual \\(\\{Z = a\\}\\) no tiene necesariamente probabilidad nula, hay un fenómeno de concentración de la probabilidad en el punto \\(a\\). \\[\n\\begin{split}\nP( Z = a )\n& = P\\left( \\min( X, a ) = a \\right) \\\\\n& = P\\left( X \\geq a \\right) \\\\\n& = 1 - F_X( a )\n\\end{split}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probabilidad y Estadística</span>"
    ]
  },
  {
    "objectID": "lectura_04.html#probabilidad-condicional",
    "href": "lectura_04.html#probabilidad-condicional",
    "title": "5  Probabilidad y Estadística",
    "section": "5.2 Probabilidad condicional",
    "text": "5.2 Probabilidad condicional\nEn varias situaciones nos interesa estudiar la probabilidad de ciertos eventos dado que otro evento \\(A \\in \\F\\) ya se ha dado. Esto nos permite convertir al evento \\(A\\) en el nuevo espacio muestral y todos los experimentos se restringen al mismo.\n\n\nDefinition 5.20: Probabilidad condicionalPara cualquier par de eventos \\(A, B \\in \\F\\), la probabilidad condicional de \\(A\\) dado \\(B\\), está dada por: \\[\nP\\left( B \\middle| A \\right) = \\frac{P( A \\cap B)}{P(A)}\n\\]\n\n\nLa medida condicionada \\(P( \\cdot | A )\\) satisface todas las condiciones de una medida de probabilidad, y mide sobre los eventos de la forma \\(A \\cap B\\), con \\(B \\in \\F\\). Es de notar que si \\(A \\cap B = \\emptyset\\), \\(P( B | A ) = 0\\).\nEsta idea de probabilidad condicional se puede extender fácilmente a las funciones de distribución acumulada.\n\n\nDefinition 5.21: Distribución acumulada condicionadaSi consideramos un evento cualquiera \\(A \\in \\F\\), \\(X\\) una variable aleatoria a valores reales y \\(x \\in \\R\\). Surge el interés de estudiar la distribución acumulada de \\(X\\) pero considerando dado que se produce el evento \\(A\\), esto resulta en estudiar: \\[\nP\\left( X \\leq x \\middle| A \\right)\n= \\frac{P(X \\leq x \\land A )}{P(A)}\n= \\frac{P( \\{X \\leq x \\} \\cap A )}{P(A)}\n\\] Esto precisamente da paso a la definición de la distribución acumulada de \\(X\\) condicionada a \\(A\\) \\[\nF( x | A ) = P\\left( X \\leq x \\middle| A \\right)\n\\]\n\n\nAdemás, la distribución acumulada condicionada satisface algunas propiedades:\n\nSi \\(A, B \\in \\F\\) son dos eventos disjuntos, entonces:\n\n\\[\n\\begin{split}\nF\\left( x \\middle| A \\cup B \\right)\n& = P\\left( X \\leq x \\middle| A \\right) \\\\\n& = \\frac{P\\left( X \\leq x \\cap ( A \\cup B ) \\right)}{P\\left( A \\cup B \\right)} \\\\\n& = \\frac{P\\left( X \\leq x \\cap A \\right) + P\\left( X \\leq x \\cap B \\right)}{P\\left( A \\cup B \\right)} \\\\\n& = \\frac{P\\left( X \\leq x \\cap A \\right)P( A )}{P(A)P\\left( A \\cup B \\right)}\n+ \\frac{P\\left( X \\leq x \\cap B \\right)P( B )}{P( B )P\\left( A \\cup B \\right)} \\\\\n& = P\\left( X \\leq x \\middle| A \\right) \\frac{P( A )}{P\\left( A \\cup B \\right)}\n+ P\\left( X \\leq x \\middle| B \\right) \\frac{P( B )}{P\\left( A \\cup B \\right)} \\\\\n& = P\\left( X \\leq x \\middle| A \\right) P\\left( A \\middle| A \\cup B \\right)\n+ P\\left( X \\leq x \\middle| B \\right) P\\left( B \\middle| A \\cup B \\right) \\\\\n& = F\\left( x \\middle| A \\right) P\\left( A \\middle| A \\cup B \\right)\n+ F\\left( x \\middle| B \\right) P\\left( B \\middle| A \\cup B \\right)\n\\end{split}\n\\] 2. La distribución acumulada condicionada con respecto a todo el espacio muestral \\(\\Omega\\), es la distribución acumulada.\n\\[\n\\begin{split}\nF\\left( x \\middle| \\Omega \\right)\n& = P\\left( X \\leq x \\middle| \\Omega \\right) \\\\\n& = \\frac{P\\left( X \\leq x \\cap \\Omega \\right)}{P(\\Omega)} \\\\\n& = \\frac{P\\left( X \\leq x \\right)}{P(\\Omega)} \\\\\n& = P\\left( X \\leq x \\right) \\\\\n& = F( x )\n\\end{split}\n\\]\n\nPara cualquier \\(A \\in \\F\\), el evento \\(\\{ X \\leq x \\} \\cap A \\subset A\\) y por tanto \\(P\\left( X \\leq x \\middle| A \\right) \\leq P( A )\\), entonces \\(0 \\leq F(x | A) \\leq 1\\),\nPara cualquier \\(A \\in \\F\\), la función de distribución acumulada condicionada \\(F( \\cdot | A )\\) es no decreciente,\nPara cualquier \\(A \\in \\F\\), la función de distribución acumulada condicionada \\(F( \\cdot | A )\\) es continua por derecha,\n\nEn algunos casos la distribución acumulada de la variable aleatoria \\(X\\) se nota por \\(F_X\\) y la distribución acumulada de \\(X\\) condicionada por el evento \\(A\\), se nota por \\(F_{X|A}\\).\nComo hemos visto la función de distribución acumulada satisface todas las propiedades de una función de distribución acumulada y por tal razón se puede calcular una esperanza, la cual vendrá condicionada.\nGracias a las propiedades 1 y 2 antes señaladas, tenemos fácilmente la siguiente igualdad para cualquier evento \\(A \\in \\F\\). \\[\nF( x )\n= F\\left( x \\middle| \\Omega \\right)\n= F\\left( x \\middle| A \\cup A^c \\right)\n= F\\left( x \\middle| A \\right) P( A ) + F\\left( x \\middle| A^c \\right) P( A^c )\n\\] de donde surge la idea de generalizar la interpretación de la distribución acumulada de \\(X\\) condicionada por un evento \\(A\\). No solo puede ser interpretada como una función, sino como una variable aleatoria. Para ello, observemos que la distribución acumulada condicionada admite también la siguiente representación con el uso de la esperanza y las funciones indicatrices. \\[\nF\\left( x \\middle| A \\right)\n= \\frac{P( X \\leq x \\land A )}{P( A )}\n= \\frac{\\E\\left[ \\mathbf{1}_{(-\\infty,x]}( X ) \\mathbf{1}_{A} \\right]}{\\E\\left[ \\mathbf{1}_{A} \\right]}\n\\] Esto sugiere la siguiente extensión de la distribución acumulada condicional a ser una variable aleatoria, ya no tan solo condicionada por eventos sino condicionada por variables aleatorias, en este primer caso por variables aleatorias de la forma \\(\\mathbf{1}_A\\). Así damos sentido a la variable aleatoria \\(F\\left( x \\middle| \\mathbf{1}_A \\right) : \\Omega \\longrightarrow \\R_+\\), para cualquier \\(\\omega \\in \\Omega\\), definimos \\[\nF\\left( x \\middle| \\mathbf{1}_A( \\omega ) \\right)\n=\n\\left\\{\n\\begin{array}{ll}\nF\\left( x \\middle| A \\right) & \\text{si}\\ \\mathbf{1}_A( \\omega ) = 1 \\\\\nF\\left( x \\middle| A^c \\right) & \\text{si}\\ \\mathbf{1}_A( \\omega ) = 0\n\\end{array}\n\\right.\n\\] de la definición anterior se sigue de forma inmediata la igualdad \\[\n\\begin{split}\n\\E\\left[ F\\left( x \\middle| \\mathbf{1}_A \\right) \\right]\n& = F\\left( x \\middle| A \\right) P( A ) + F\\left( x \\middle| A^c \\right) P( A^c ) \\\\\n& = \\frac{P\\left( X \\leq x \\land A \\right)}{P(A)} P( A )\n+ \\frac{P\\left( X \\leq x \\land A^c \\right)}{P(A^c)} P( A^c ) \\\\\n& = P\\left( X \\leq x \\land A \\right) + P\\left( X \\leq x \\land A^c \\right) \\\\\n& = P\\left( X \\leq x \\right) \\\\\n& = F( x )\n\\end{split}\n\\] Las variables aleatorias también generan eventos, ya que son medibles, por tal razón si tomamos en cuenta la definición anterior, podemos ciertamente extender la definición anterior de distribución acumulada condicionada, pero considerando variables aleatorias en su condicionamiento.\nAsí si consideramos dos variables aleatorias \\(X\\) y \\(Y\\) a valores reales, a partir de la definición podemos tranquilamente considerar la distribución acumulada de \\(X\\) condicionada al evento \\(Y \\leq y\\) \\[\nF\\left( x \\middle| Y \\leq y \\right)\n= \\frac{P( X \\leq x \\land Y \\leq y )}{P(Y \\leq y)}\n= \\frac{F_{X,Y}( x, y )}{F_Y( y )}\n\\] está bien definida para \\(F_Y( y ) \\neq 0\\)\n\n\nDefinition 5.22: Esperanza condicionadaPara una variable aleatoria \\(X\\) y un evento cualquiera \\(A \\in \\F\\), se define la esperanza condicionada respecto del evento \\(A\\), como: \\[\n\\E\\left[ X \\middle| A \\right]\n= \\E_{P|A}\\left[ X \\right]\n= \\E_P\\left[ X \\middle| A \\right]\n= \\int\\limits_{A} X( \\omega )\\ dP \\left( \\omega \\middle| A \\right)\n= \\int\\limits_{\\R} x\\ dF\\left( x \\middle| A \\right)\n\\] cuando \\(F(x | A)\\) es derivable, con densidad de probabilidad \\(f(x|A)\\), la esperanza condicionada también se puede escribir como: \\[\n\\E\\left[ X \\middle| A \\right]\n= \\int\\limits_{\\R} x\\ f( x | A )\\ dx\n\\] La distribución acumulada condicionada satisface las mismas propiedades de una distribución acumulada, y por tanto la esperanza que resulta de esta también satisface las mismas propiedades que cualquier esperanza.\nAl igual que lo realizamos con la distribución acumulada condicionada, podemos reinterpretar a la esperanza condicionada como una variable aleatoria. \\[\n\\E\\left[ X \\middle| \\mathbf{1}_A \\right]\n= \\int\\limits_{\\R} x\\ dF\\left( x \\middle| \\mathbf{1}_A \\right)\n\\]\n\n\nPor otra parte, para cualquier evento \\(A \\in \\F\\), las distribuciones \\(F( \\cdot ), F( \\cdot |  A )\\) y \\(F( x | A^c)\\) son funciones de variación acotada y por tanto sus integrales de Riemann-Stieltjes están bien definidas [1], de donde resulta la siguiente igualdad. \\[\n\\begin{split}\n\\E\\left[ X \\right]\n& = \\int\\limits_{\\R} x dF( x ) \\\\\n& = \\int\\limits_{\\R} x dF( x | A ) P( A ) + \\int\\limits_{\\R} x dF( x | A^c ) P( A^c ) \\\\\n& = \\E\\left[ X \\middle| A \\right] P( A ) + \\E\\left[ X \\middle| A^c \\right] P( A^c ) \\\\\n& = \\E\\left[ \\E\\left[ X \\middle| \\mathbf{1}_A \\right] \\right]\n\\end{split}\n\\] por abuso de notación se suele utilizar la misma notación \\(\\E\\left[ X \\middle| A \\right]\\) para indicar también la variable aleatoria \\(\\E\\left[ X \\middle| \\mathbf{1}_A \\right]\\). Así la siguiente expresión tienen sentido. \\[\n\\E\\left[ \\E\\left[ X \\middle| A \\right] \\right]\n= \\E\\left[ \\E\\left[ X \\middle| \\mathbf{1}_A \\right] \\right]\n\\]\nLa siguiente definición de distribución de exceso condicionada es útil para el estudio de los valores extremos que se pueden presentar en el estudio de los valores de siniestros que se presentan en un seguro.\n\n\nDefinition 5.23La distribución de exceso condicionada asociada a una variable aleatoria \\(X\\) con distribución de probabilidad acumulada \\(F\\) y a un umbral de condicionamiento \\(u &gt; 0\\), está dada por: \\[\nF_u( y ) = P\\left( X - u \\leq y \\middle| X &gt; u \\right)\n= \\frac{P\\left( u &lt; X \\leq y + u\\right)}{P(X &gt; u)}\n= \\frac{F( u + y ) - F( u )}{ 1 - F( u )}\n\\]\n\n\n\n\nDefinition 5.24: Varianza condicionadaLa varianza condicionada se puede definir de forma directa a partir de los anterior. Para una variable aleatoria a valores reales \\(X\\) y un evento \\(A \\in \\F\\). \\[\n\\V[X|A] = \\E[Y^2|A] - \\E[Y|A]^2\n\\]\n\n\n\n\nProposition 5.25: Ley de la varianza totalConsideremos dos variables aleatorias \\(X, Y\\), ambas a valores reales, entonces se satisface la siguiente igualdad: \\[\n\\V[ Y ] = \\E\\left[ \\V[ Y| X ] \\right] + \\V\\left[ \\E[ Y | X ] \\right]\n\\]\n\n\n\nProof. \\[\n\\begin{split}\n\\E\\left[ \\V[ Y| X ] \\right]\n& = \\E\\left[ \\E[ Y^2 | X ] - \\E[ Y | X ]^2 \\right] \\\\\n& = \\E\\left[ \\E[ Y^2 | X ] \\right] - \\E\\left[ \\E[ Y | X ]^2 \\right] \\\\\n& = \\E\\left[ Y^2 \\right] - \\E\\left[ \\E[ Y | X ]^2 \\right] + \\E\\left[ \\E[ Y | X ] \\right]^2\n- \\E\\left[ \\E[ Y | X ] \\right]^2 \\\\\n& = \\E\\left[ Y^2 \\right] - \\V\\left[ \\E[ Y | X ] \\right] - \\E\\left[ Y \\right]^2 \\\\\n& = \\V[ Y ] - \\V\\left[ \\E[ Y | X ] \\right]\n\\end{split}\n\\] Basta con ordenar los términos y se tiene la igualdad que buscamos.\n\n\n\nDefinition 5.26: Mixtura de distribucionesUn caso de especial interés para estudiar algunos problemas actuariales se da cuando se dispone de dos variables aleatorias, \\(N\\) que toma solo valores discretos (numerables) los cuales pueden ser finitos como infinitos, por ejemplo: \\(N\\) toma valores en los números naturales \\(\\mathbb{N}\\) y otra variable aleatoria \\(X\\) que toma valores continuos reales en \\(\\R\\). La distribución conjunta puede generarse de la siguiente forma \\[\n\\begin{split}\nF( n, x )\n& = P( N = n \\land X \\leq x ) \\\\\n& = P\\left( X \\leq x \\middle| N = n \\right) P( N = n )\\qquad \\text{propiedades de la probabilidad condicional} \\\\\n& = F( x | n ) p_n\\qquad \\text{simplificando notación} \\\\\n\\end{split}\n\\]\n\\(F( x | n )\\) es la ley condicionada de \\(X\\) dado que \\(N = n\\) y \\(p_n = P( N = n )\\)\n\n\nAdemás, es de notar que: \\[\n\\begin{split}\nF_X( x )\n& = P( X \\leq x ) \\\\\n& = P( X \\leq x \\land N \\in \\mathbb{N} ) \\\\\n& = P\\left( X \\leq x \\land N \\in \\bigcup_{n \\in \\mathbb{N}} \\{n\\} \\right) \\\\\n& = P\\left( \\bigcup_{n \\in \\mathbb{N}} \\left\\{ X \\leq x \\land N \\in \\{n\\} \\right\\} \\right) \\\\\n& = \\sum\\limits_{n \\in \\mathbb{N}} P\\left( X \\leq x \\land N \\in \\{n\\} \\right) \\\\\n& = \\sum\\limits_{n \\in \\mathbb{N}} P\\left( X \\leq x \\land N = n \\right) \\\\\n& = \\sum\\limits_{n \\in \\mathbb{N}} P\\left( X \\leq x\\ \\middle|\\ N = n \\right) P( N = n ) \\\\\n& = \\sum\\limits_{n \\in \\mathbb{N}} F( x | n ) p_n\n\\end{split}\n\\] la distribución de \\(F_X\\) de \\(X\\) no es más que una mixtura de las distribuciones condicionales de \\(X\\) para cada \\(n \\in \\mathbb{N}\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probabilidad y Estadística</span>"
    ]
  },
  {
    "objectID": "lectura_04.html#resultados-de-convergencia",
    "href": "lectura_04.html#resultados-de-convergencia",
    "title": "5  Probabilidad y Estadística",
    "section": "5.3 Resultados de convergencia",
    "text": "5.3 Resultados de convergencia\nDe las ramas de las Matemáticas la Estadística ciertamente es la más subestimada, en muchos casos menospreciada. Sin embargo, no sin mucha pretención, sino más bien honestidad, se puede decir que la Estadística es una de las ramas más complicadas de las Ciencias en general, ya que busca en muchos casos comprender, explicar y predecir fenómenos reales. En su fundamentación, al profundizar en ella, uno encontrará un sin número de conceptos, métodos y teorías con un amplio espectro de complejidad, que incluso se sustentan en ideas filosóficas bastante elaboradas y poco comprendidas.\nNo olvidar, la Estadística busca de frente y sin rodeos extraer conocimiento de la realidad y no hay algo más complejo y duro que la realidad misma.\nMuchos de las herramientas de las estadística se resumen en algunas recetas o aplicaciones de software, sin embargo, no se debe olvidar que en muchos casos estas herramientas hacen uso de muchos métodos bastante avanzados y complejos en lo que respecta al conocimiento Matemático.\n\nTheorem 5.27: Ley débil de los grandes númerosConsideramos la secuencia de variables aleatorias \\(\\{X_i\\}_{i\\in \\mathbb{N}}\\) las cuales consideraremos que son i.i.d. y con media común finita \\(\\E[X_i] = \\mu &lt; +\\infty\\). Entonces, se satisface el siguiente límite en probabilidad \\[\n\\frac{1}{n} \\sum\\limits_{i=1}^n X_i \\overset{p}{\\rightarrow} \\mu\n\\] esto quiere decir que para cualquier \\(\\varepsilon &gt; 0\\) se satisface el siguiente límite \\[\n\\underset{n \\rightarrow +\\infty}{\\lim} P\\left( \\left| \\frac{1}{n}\\sum\\limits_{i=1}^n X_i - \\mu \\right| &gt; \\varepsilon\\right) = 0\n\\]\n\n\nEl comportamiento de la suma de variables aleatorias, que posean además los dos primeros momentos finitos. Al estandarizar esta suma, la misma tiende a comportarse en el límite, conforme se incluye más términos, como una variable aleatoria con distribución normal. En muchos casos este método puede proveer una primera aproximación.\n\nTheorem 5.28: Teorema del límite centralConsideramos las variables aleatorias \\(X_1,\\ldots,X_n\\) i.i.d. con media común finita \\(\\E[X_i] = \\mu &lt; +\\infty\\) y varianza común finita \\(\\V[ X_i ] = \\sigma^2 &lt; +\\infty\\), para todo \\(i \\in \\{1, \\ldots, n \\}\\). Si consideramos la variable aleatoria de la suma total \\(S_n = \\sum\\limits_{i=1}^n X_i\\), entonces para la variable aleatoria estandarizada \\[\n\\frac{S_n - \\E[S_n]}{\\V[ S_n ]} \\overset{d}{\\longrightarrow} Z\n\\] se tiene la convergencia en distribución, cuando \\(n \\rightarrow +\\infty\\), donde \\(Z \\rightsquigarrow N( 0, 1 )\\). Más aún, de manera más clara la convergencia en distribución se puede expresar la convergencia puntual dada por el siguiente límite. \\[\n\\underset{n \\rightarrow +\\infty}{\\lim} P\\left( \\frac{S_n\n- \\E[S_n]}{\\V[ S_n ]} \\leq z \\right)\n= \\Phi( z ), \\qquad \\forall z \\in \\R\n\\]\n\n\nEl teorema del límite central en su forma usual no proporciona una tasa de convergencia es decir, la variable aleatoria \\(\\frac{S_n - \\E[S_n]}{\\V[ S_n ]}\\) tiende a tener un comportamiento de una variable aleatoria normal estándar conforme aumenta \\(n\\), pero no estamos seguros que tamaño debe tomar \\(n\\) para que esto se cumpla con una alta certeza. Para ello adicional al 5.28 se debe considerar otros resultados asociados a desigualdades de concentración.\nEl siguiente teorema también es gran ayuda y nos asegura la convergencia de la distribución acumulada empírica a la distribución acumulada teórica.\n\nTheorem 5.29: Teorema de Glivenko-CantelliConsideramos las variables aleatorias \\(\\{X_n\\}_{n\\in\\N}\\) i.i.d., con función de distribución acumulada \\(F\\). Entonces, la distribución acumula empírica \\(F_n\\) generada a partir de las variables \\(\\{X_n\\}_{n\\in\\N}\\), converge en casi todo punto de forma uniforme a la distribución \\(F\\), más precisamente, para la norma de la convergencia uniforme \\(\\left\\| F_n - F \\right\\|_{\\infty} = \\underset{x \\in \\R}{\\sup} \\left| F_n( x ) - F( x ) \\right|\\) tenemos la siguiente igualdad que precisamente expresa la convergencia en casi todo punto. \\[\nP\\left( \\underset{n \\rightarrow +\\infty}{\\lim} \\left\\| F_n - F \\right\\|_{\\infty} = 0 \\right) = 1\n\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probabilidad y Estadística</span>"
    ]
  },
  {
    "objectID": "lectura_04.html#desigualdades-de-concentración",
    "href": "lectura_04.html#desigualdades-de-concentración",
    "title": "5  Probabilidad y Estadística",
    "section": "5.4 Desigualdades de concentración",
    "text": "5.4 Desigualdades de concentración\nPara muchos fines prácticos es importante encontrar una buena estimación de donde se encuentran concentrados los valores de una distribución de probabilidad, para ello existen varios resultados que caracterizan precisamente ello, estos se conocen como desigualdades de concentración.\n\nProposition 5.30: Desigualdad de ChebyshevDada una variable aleatoria \\(X\\) con esperanza y varianza finitas \\(\\E[X] &lt; +\\infty\\) y \\(\\V[X] &lt; +\\infty\\), tenemos que se satisface la siguiente desigualdad para cualquier \\(\\varepsilon &gt; 0\\) \\[\nP\\left( \\left| X - \\E[X] \\right| &gt; \\varepsilon \\sqrt{\\V[ X ]} \\right) &lt; \\frac{1}{\\varepsilon^2}\n\\]\n\n\nEn la práctica la desigualdad de Chebyshev nos asegura que sin importar la variable aleatoria a valores reales, si tienen sus dos primeros momentos finitos, la probabilidad que la variable aleatoria \\(X\\) se aleje de su esperanza \\(\\E[X]\\) en más de \\(\\varepsilon\\) veces la desviación típica \\(\\sqrt{\\V[ X ]}\\) viene acotada por el término \\(\\frac{1}{\\varepsilon^2}\\) que decrece cuadráticamente en \\(\\varepsilon\\). Entonces, por más pesada que sea la cola de la distribución de la variable aleatoria \\(X\\), a lo mucho decrecerá cuadráticamente.\n\nTheorem 5.31: Desigualdad de Berry-EsséenSean \\(X_1, \\ldots X_n\\) variables aleatorias i.i.d., con media y varianza finitas, i.e \\(\\E[X] &lt; +\\infty\\) y \\(\\V[X] &lt; +\\infty\\) y además con tercer momento absoluto finito \\(\\E\\left[\\left|X - \\E[X]\\right|^3\\right] &lt; +\\infty\\). Entonces, para la variable aleatoria \\[\nU_n = \\frac{S_n - \\E[ S_n ]}{\\sqrt{\\V[ S_n ]}}\n\\] con \\(S_n = \\sum\\limits_{i=1}^n X_i\\). Se satisface la siguiente desigualdad de concentración con respecto a la distribución acumulada \\(\\Phi\\) de la ley normal estándar. \\[\n\\underset{u \\in \\R}{\\sup}\\left( 1 + |u|^3 \\right)\\left| P\\left( U_n \\leq u\\right) - \\Phi( u ) \\right|\n\\leq \\frac{C}{\\sqrt{n}} \\frac{\\E\\left[\\left|X - \\E[X]\\right|^3\\right]}{\\sqrt{\\V[X]}^3}\n\\] Con \\(C = 0.7655+8( 1 + e ) &gt; 0\\) una constante universal, independiente de \\(N\\), \\(X\\) o \\(S\\).\n\n\nEl teorema del límite central 5.28 no proporciona un criterio para determinar cuan rápido se realiza la convergencia hacia la distribución normal. El resultado de la desigualdad anterior 5.31 si proporciona un criterio de la convergencia eso sí, bajo la condición adicional de tener \\(\\E\\left[\\left|X - \\E[X]\\right|^3\\right] &lt; +\\infty\\). Como podemos observar la convergencia es sub-lineal, va en orden de la raíz cuadrada del número de términos \\(n\\).\nEsto quiere decir que si deseamos un error de aproximación \\(\\varepsilon &gt; 0\\), requerimos que \\[\n\\begin{split}\n\\frac{C}{\\sqrt{n}} \\frac{\\E\\left[\\left|X - \\E[X]\\right|^3\\right]}{\\sqrt{\\V[X]}^3}\n& \\leq \\varepsilon \\\\\n\\frac{C^2}{\\varepsilon^2} \\frac{\\E\\left[\\left|X - \\E[X]\\right|^3\\right]^2}{\\V[X]^3}\n& \\leq n\n\\end{split}\n\\] lo cual muestra que el número de variables aleatorias \\(n\\) crece en orden cuadrático inverso al error de aproximación deseado \\(\\varepsilon\\).\nAsí mismo, el teorema 5.29 asegura un convergencia, sin embargo no da un estimativo de la rapidez de dicha convergencia conforme aumenta el valor \\(n\\), el siguiente resultado es más claro al respecto y proporciona un criterio de convergencia.\n\nTheorem 5.32: Desigualdad Dvoretzky–Kiefer–WolfowitzDada una serie de variables aleatorias a valores reales \\(X_1, \\ldots, X_n\\), i.i.d., con distribución acumulada \\(F\\), tenemos la siguiente desigualdad asociada a la distribución acumulada empírica \\[\nF_n( x ) = \\frac{1}{n}\\sum\\limits_{i=1}^n \\mathbf{1}_{(-\\infty,x]}( X_i )\n\\] y su aproximación a \\(F\\). \\[\nP\\left( \\left\\| F_n( x ) - F( x ) \\right\\|_{\\infty} &gt; \\varepsilon \\right)\n= P\\left( \\underset{x \\in \\R}{\\sup} \\left| F_n( x ) - F( x ) \\right| &gt; \\varepsilon \\right)\n\\leq 2 e^{-2n \\varepsilon^2 }\\qquad \\forall \\varepsilon &gt; 0\n\\]\n\n\nComo podemos notar el orden de convergencia del teorema es \\(\\sqrt{n}\\) en el tamaño de observaciones, esto quiere decir que la convergencia es menos que el orden lineal.\nAcorde a la desigualdad 5.32, para tener un probabilidad baja de aproximación \\(\\delta &gt; 0\\) en un error de discrepancia \\(\\varepsilon &gt;0\\), necesitamos satisfacer la desigualdad. \\[\n\\begin{split}\n2 e^{-2n \\varepsilon^2 }\n& \\leq \\delta \\\\\n2n \\varepsilon^2\n& \\geq -\\ln\\left( \\frac{\\delta}{2} \\right) \\\\\nn & \\geq \\left\\lceil -\\frac{1}{2\\varepsilon^2} \\ln\\left( \\frac{\\delta}{2} \\right) \\right\\rceil\n\\end{split}\n\\] así se observa que para tener una aproximación de orden \\(\\delta\\) y con un error de discrepancia \\(\\varepsilon\\), se requiere como mínimo realizar un número de simulaciones \\(n\\) de orden logarítmico en \\(\\delta\\) y cuadrático en \\(\\varepsilon\\).\n\n\nCode\ndelta &lt;- 0.01\ne &lt;- unlist( lapply( seq( 1, 9, 1 ), FUN = function( n ) seq( 9, 1, -1 ) * 10^{-n} ) )\nn &lt;- ceiling( -log( delta / 2 ) / ( 2 * e^2 ) )\n\n\n\n\nCode\ndt &lt;- data.table( delta = delta, e, n, d = 8 * n / 1024^3 )\ndt %&gt;%\n  kable(\n    label = NA,\n    caption = 'Error versus número de simulaciones',\n    row.names = FALSE,\n    col.names = c( \"$\\\\delta$\", \"$\\\\varepsilon$\", \"$n$\", \"GB\" ),\n    align = 'llrr',\n    digits = c( 10, 20, 0, 10 ),\n    format.args = list( big.mark = ',', decimal.mark = '.', scientific = FALSE ),\n    escape = FALSE,\n    centering = TRUE, \n    booktabs = TRUE,\n    longtable = TRUE ) %&gt;%\n  kable_classic( \n    full_width = FALSE, \n    html_font = \"Cambria\", \n    position = 'center', \n    latex_options = c( 'hold_position', 'repeat_header' ) ) %&gt;%\n  scroll_box( width = \"100%\", height = \"500px\" )\n\n\n\n\nError versus número de simulaciones\n\n\n$\\delta$\n$\\varepsilon$\n$n$\nGB\n\n\n\n\n0.01\n0.900000000\n4\n0.0000000298\n\n\n0.01\n0.800000000\n5\n0.0000000373\n\n\n0.01\n0.700000000\n6\n0.0000000447\n\n\n0.01\n0.600000000\n8\n0.0000000596\n\n\n0.01\n0.500000000\n11\n0.0000000820\n\n\n0.01\n0.400000000\n17\n0.0000001267\n\n\n0.01\n0.300000000\n30\n0.0000002235\n\n\n0.01\n0.200000000\n67\n0.0000004992\n\n\n0.01\n0.100000000\n265\n0.0000019744\n\n\n0.01\n0.090000000\n328\n0.0000024438\n\n\n0.01\n0.080000000\n414\n0.0000030845\n\n\n0.01\n0.070000000\n541\n0.0000040308\n\n\n0.01\n0.060000000\n736\n0.0000054836\n\n\n0.01\n0.050000000\n1,060\n0.0000078976\n\n\n0.01\n0.040000000\n1,656\n0.0000123382\n\n\n0.01\n0.030000000\n2,944\n0.0000219345\n\n\n0.01\n0.020000000\n6,623\n0.0000493452\n\n\n0.01\n0.010000000\n26,492\n0.0001973808\n\n\n0.01\n0.009000000\n32,706\n0.0002436787\n\n\n0.01\n0.008000000\n41,394\n0.0003084093\n\n\n0.01\n0.007000000\n54,065\n0.0004028156\n\n\n0.01\n0.006000000\n73,588\n0.0005482733\n\n\n0.01\n0.005000000\n105,967\n0.0007895157\n\n\n0.01\n0.004000000\n165,573\n0.0012336150\n\n\n0.01\n0.003000000\n294,351\n0.0021930858\n\n\n0.01\n0.002000000\n662,290\n0.0049344450\n\n\n0.01\n0.001000000\n2,649,159\n0.0197377726\n\n\n0.01\n0.000900000\n3,270,567\n0.0243676230\n\n\n0.01\n0.000800000\n4,139,311\n0.0308402702\n\n\n0.01\n0.000700000\n5,406,447\n0.0402811691\n\n\n0.01\n0.000600000\n7,358,775\n0.0548271462\n\n\n0.01\n0.000500000\n10,596,635\n0.0789510831\n\n\n0.01\n0.000400000\n16,557,242\n0.1233610660\n\n\n0.01\n0.000300000\n29,435,097\n0.2193085626\n\n\n0.01\n0.000200000\n66,228,968\n0.4934442639\n\n\n0.01\n0.000100000\n264,915,869\n1.9737770334\n\n\n0.01\n0.000090000\n327,056,628\n2.4367617667\n\n\n0.01\n0.000080000\n413,931,045\n3.0840266123\n\n\n0.01\n0.000070000\n540,644,630\n4.0281163901\n\n\n0.01\n0.000060000\n735,877,413\n5.4827139750\n\n\n0.01\n0.000050000\n1,059,663,474\n7.8951081187\n\n\n0.01\n0.000040000\n1,655,724,178\n12.3361064345\n\n\n0.01\n0.000030000\n2,943,509,649\n21.9308558777\n\n\n0.01\n0.000020000\n6,622,896,709\n49.3444257155\n\n\n0.01\n0.000010000\n26,491,586,833\n197.3777028397\n\n\n0.01\n0.000009000\n32,705,662,757\n243.6761763468\n\n\n0.01\n0.000008000\n41,393,104,427\n308.4026606902\n\n\n0.01\n0.000007000\n54,064,462,924\n402.8116384447\n\n\n0.01\n0.000006000\n73,587,741,203\n548.2713967785\n\n\n0.01\n0.000005000\n105,966,347,331\n789.5108113512\n\n\n0.01\n0.000004000\n165,572,417,705\n1,233.6106427386\n\n\n0.01\n0.000003000\n294,350,964,809\n2,193.0855870917\n\n\n0.01\n0.000002000\n662,289,670,819\n4,934.4425709471\n\n\n0.01\n0.000001000\n2,649,158,683,275\n19,737.7702837810\n\n\n0.01\n0.000000900\n3,270,566,275,647\n24,367.6176342890\n\n\n0.01\n0.000000800\n4,139,310,442,616\n30,840.2660683990\n\n\n0.01\n0.000000700\n5,406,446,292,396\n40,281.1638444364\n\n\n0.01\n0.000000600\n7,358,774,120,206\n54,827.1396771520\n\n\n0.01\n0.000000500\n10,596,634,733,097\n78,951.0811351016\n\n\n0.01\n0.000000400\n16,557,241,770,463\n123,361.0642735884\n\n\n0.01\n0.000000300\n29,435,096,480,823\n219,308.5587086007\n\n\n0.01\n0.000000200\n66,228,967,081,851\n493,444.2570943460\n\n\n0.01\n0.000000100\n264,915,868,327,402\n1,973,777.0283773690\n\n\n0.01\n0.000000090\n327,056,627,564,694\n2,436,761.7634288520\n\n\n0.01\n0.000000080\n413,931,044,261,566\n3,084,026.6068396419\n\n\n0.01\n0.000000070\n540,644,629,239,596\n4,028,116.3844436109\n\n\n0.01\n0.000000060\n735,877,412,020,561\n5,482,713.9677149132\n\n\n0.01\n0.000000050\n1,059,663,473,309,608\n7,895,108.1135094762\n\n\n0.01\n0.000000040\n1,655,724,177,046,262\n12,336,106.4273585528\n\n\n0.01\n0.000000030\n2,943,509,648,082,242\n21,930,855.8708596379\n\n\n0.01\n0.000000020\n6,622,896,708,185,045\n49,344,425.7094341889\n\n\n0.01\n0.000000010\n26,491,586,832,740,180\n197,377,702.8377367556\n\n\n0.01\n0.000000009\n32,705,662,756,469,352\n243,676,176.3428848386\n\n\n0.01\n0.000000008\n41,393,104,426,156,528\n308,402,660.6839636564\n\n\n0.01\n0.000000007\n54,064,462,923,959,544\n402,811,638.4443606734\n\n\n0.01\n0.000000006\n73,587,741,202,056,032\n548,271,396.7714908123\n\n\n0.01\n0.000000005\n105,966,347,330,960,720\n789,510,811.3509470224\n\n\n0.01\n0.000000004\n165,572,417,704,626,112\n1,233,610,642.7358546257\n\n\n0.01\n0.000000003\n294,350,964,808,224,128\n2,193,085,587.0859632492\n\n\n0.01\n0.000000002\n662,289,670,818,504,448\n4,934,442,570.9434185028\n\n\n0.01\n0.000000001\n2,649,158,683,274,017,792\n19,737,770,283.7736740112\n\n\n\n\n\n\n\n\nEn este caso en particular estudiaremos la velocidad de convergencia del método resultante del teorema del límite central 5.28. Generaremos una simulación aleatoria de la suma agregada \\(S_n\\) y mostrar\n\n\nCode\nm &lt;- 600\nn &lt;- 3000\nu &lt;- 4\ns &lt;- 2\n\nset.seed( 5143829 )\nX &lt;- lapply( 1:m, FUN = function( j ) rlnorm( n, meanlog = u, sdlog = s ) )\nS &lt;- lapply( X, FUN = function( x ) cumsum( x ) )\nES &lt;- sapply( 1:n, FUN = function( i ) mean( sapply( 1:m, FUN = function( j ) S[[ j ]][ i ] ) ) )\nVS &lt;- sapply( 1:n, FUN = function( i ) var( sapply( 1:m, FUN = function( j ) S[[ j ]][ i ] ) ) )\nNS &lt;- lapply( S, FUN = function( s ) ( s - ES ) / sqrt( abs( VS ) ) )\n\nz &lt;- seq( -5, 5, length.out = 200 )\nFSn &lt;- lapply( 1:n, FUN = function( i ) ecdf( sapply( 1:m, FUN = function( j ) NS[[ j ]][ i ] ) )( z ) )\nphi &lt;- pnorm( z )\nD &lt;- sapply( FSn, FUN = function( Fn ) max( abs( Fn - phi ) ) )\n\n\n\n\nCode\nplt &lt;- ggplot() +\n  geom_line( aes( x = 1:n, y = D ), colour = 'darkred', linewidth = 0.5 ) + \n  scale_x_continuous( breaks = seq( 0, n, 500 ), limits = c( 1, n ), expand = c( 0, 0 ) ) +\n  scale_y_continuous( breaks = seq( 0, 0.5, length = 11 ), limits = c( 0, 0.5 ), expand = c( 0, 0 ) ) +\n  xlab( TeX( \"$n$\" ) ) + \n  ylab( TeX( \"$D_n$\" ) ) + \n  theme_bw()\nplot( plt )\n\n\n\n\n\n\n\n\n\n\n\nCode\ndat &lt;- NULL\nfor ( k in 1:n ) {\n  dat &lt;- rbindlist( list( dat, data.table( z = z, n = as.factor( k ), FSn = FSn[[ k ]] ) ) )\n}\ncols &lt;- wes_palette( n, name = \"BottleRocket2\", type = \"continuous\" )\n\nplt &lt;- ggplot( ) +\n  geom_step( data = dat, aes( x = z, y = FSn, group = n, colour = n ), linewidth = 0.07 ) +\n  geom_line( aes( x = z, y = phi, colour = 'olivedrab3' ), linewidth = 1 ) +\n  scale_color_manual( values = c( cols, 'olivedrab3' ) ) +\n  scale_x_continuous( breaks = seq( -5, 5, 1 ),\n                      labels = formatC( seq( -5, 5, 1 ), digits = 1, format = 'f' ),\n                      limits = c( -5, 5 ), \n                      expand = c( 0, 0 ) ) +\n  scale_y_continuous( breaks = seq( 0, 1, length = 11 ), \n                      labels = formatC( seq( 0, 1, length = 11 ), digits = 2, format = 'f' ), \n                      limits = c( 0, 1 ), \n                      expand = c( 0, 0 ) ) +\n  xlab( TeX( \"$z$\" ) ) + \n  ylab( TeX( \"$F_n$\" ) ) + \n  theme_bw() +\n  theme( legend.position = \"none\" )\nplot( plt )\n\n\n\n\n\n\n\n\n\n\n\n\nTheorem 5.33: Desigualdad de ChernoffDada una variable aleatoria \\(X\\) a valores reales, para la cual si existe y está bien definida su función generadora de momentos \\(M\\), entonces se satisface la siguientes desigualdades para cualquier \\(\\varepsilon \\in \\R\\). \\[\nP( X \\geq \\varepsilon ) \\leq \\underset{t &gt; 0}{\\inf} \\exp( -t \\varepsilon ) M( t )\n\\] así mismo \\[\nP( X \\leq \\varepsilon ) \\leq \\underset{t &lt; 0}{\\inf} \\exp( -t \\varepsilon ) M( t )\n\\]\n\n\n\n\nTheorem 5.34: Desigualdad de Paley-ZygmundDada una variable aleatoria \\(X\\) a valores reales, que solo toma valores no negativos \\(X \\geq 0\\) y que además tiene varianza finita \\(\\V[X] &lt; +\\infty\\), entonces si tomamos un valor \\(\\rho \\in [0,1]\\), se satisface la siguiente desigualdad: \\[\nP( X \\geq \\rho \\E[ X ] ) \\geq ( 1 - \\rho )^2 \\frac{\\E[ X ]^2}{\\E[ X^2 ]}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probabilidad y Estadística</span>"
    ]
  },
  {
    "objectID": "lectura_04.html#transformada-de-fourier-contínua-y-discreta",
    "href": "lectura_04.html#transformada-de-fourier-contínua-y-discreta",
    "title": "5  Probabilidad y Estadística",
    "section": "5.5 Transformada de Fourier, contínua y discreta",
    "text": "5.5 Transformada de Fourier, contínua y discreta\n\n\nDefinition 5.35: Tansformada de FourierLa transformada de Fourier para una función integrable \\(f : \\R^n \\longrightarrow \\R\\), está definida como un funcional que toma funciones acotadas y general una función usualmente integrable a valores complejos en general. De forma más formal, se puede definir la transformada de Fourier como una función \\(\\Fo : L^1( \\R^n ) \\longrightarrow L^{\\infty} \\left( \\R^n, \\mathbb{C} \\right)\\)\n\\[\n\\Fo( f )( \\omega ) =\n\\int\\limits_{\\R^n} f( x ) \\exp( -2 \\pi i \\omega \\cdot x )\\ dx\n\\] donde \\(i\\) es la cantidad compleja \\(i = \\sqrt{-1}\\).\nla inversa de la transformada de Fourier, para una función \\(g : \\R^n \\longrightarrow \\R\\), está dada por: \\[\n\\Fo^{-1}( g ) =\n\\int\\limits_{\\R^n} g( x ) \\exp( 2 \\pi i \\omega \\cdot x )\\ dx\n\\]\nse satisface la siguiente igualdad \\[\n\\Fo^{-1}\\left( \\Fo( f ) \\right) = f\n\\]\n\n\nAdemás, la transformada de Fourier satisface las siguientes propiedades:\n\nLa transformada de Fourier es lineal, si \\(a,b \\in \\R\\) y \\(f :\\R^n \\longrightarrow \\R\\) y \\(g :\\R^n \\longrightarrow \\R\\), funciones integrables. \\[\n\\Fo\\left( a f + b g \\right) = a \\Fo( f ) + b \\Fo( g )\n\\]\nLa transformada de Fourier de la convolución de funciones es el producto de las transformadas de Fourier. \\[\n\\Fo\\left( f \\star g \\right) = \\Fo( f ) \\Fo( g )\n\\]\n\nPara realizar algunas aproximaciones numéricas, se hace de una discretización de la transformada de Fourier, la misma se evalúa sobre un número finito de valores. Para ello necesitas considerar secuencias de números finitas.\n\nDefinition 5.36: Familia de secuencias finitasEn lo que sigue trabajaremos con secuencias finitas a valores reales \\(x = \\{x_k\\} = (x_0, \\ldots, x_{N-1}) \\in \\R^{N}\\), tan solo iniciamos los índices desde \\(0\\). Toda el álgebra lineal del espacio \\(\\R^N\\) se puede utilizar en la secuencias. Además, podemos medir la distancia entre una secuencia \\(x = \\{x_k\\}\\) y otra \\(y = \\{y_k\\}\\), ambas de igual dimensión, tan solo utilizando la norma cuadrática. \\[\n\\left\\| x - y \\right\\|_2\n= \\left( \\sum\\limits_{k=0}^{N-1} \\left| x_k - y_k \\right|^2 \\right)^{\\frac{1}{2}}\n\\] Las secuencias se pueden igual considerar a valores complejos en \\(\\C^N\\), tan solo en la definición anterior se reemplaza el valor absoluto por la norma compleja de cada coordenada. También hay que tomar en cuenta que las secuencias de números reales \\(\\R^N\\) pueden ser vistas como un subconjunto de \\(\\C^N\\).\n\n\nCon la definición anterior estamos en la capacidad de definir la transformada de Fourier discreta que actúa sobre la secuencias de números.\n\nDefinition 5.37: Tansformada de Fourier discretaDada una secuencia finita de números \\(x = \\{x_k\\} = (x_0, \\ldots, x_{N-1})\\). Entonces, la transformada de Fourier discreta de la secuencia \\(\\{x_k\\}\\) es la función \\(\\DFT : \\C^n \\longrightarrow \\C^n\\), definida por: \\[\n\\DFT\\left[ \\{x_k\\} \\right]\n= \\left\\{ \\sum\\limits_{k=0}^{N-1} x_k \\exp\\left( -2\\pi i \\frac{jk}{N} \\right) \\right\\}_j\n\\] con \\(j \\in \\{0, \\ldots, N-1 \\}\\).\nla inversa de la transformada de Fourier discreta, está simplemente dada por: \\[\n\\DFT^{-1}\\left[ \\{x_k\\} \\right]\n= \\left\\{ \\frac{1}{N} \\sum\\limits_{k=0}^{N-1} x_k \\exp\\left( 2\\pi i \\frac{jk}{N} \\right) \\right\\}_j\n\\]\n\n\n\n\n5.5.1 Aproximación numérica en una dimensión\nPara el caso de 1-dimensional podemos tener la siguiente aproximación a la transformada de Fourier. Consideramos el caso donde la función \\(f\\) está concentrada en su mayoría en un intervalo \\([a,b]\\), la mayor parte de su integral está ahí. Luego para aproximar la integral consideramos una discretización uniforme del intervalo \\([a,b]\\), seleccionando un tamaño \\(N \\in \\mathbb{N}\\) y tomando una secuencia de valores discretos \\(x_k = a + k h\\), con \\(h = \\frac{b-a}{N}\\) y \\(k \\in \\{0, \\ldots, N\\}\\). Así, tenemos la siguiente aproximación a la transformada de Fourier. \\[\n\\begin{split}\n\\Fo( f )( \\omega )\n& = \\int\\limits_{\\R} f( x ) \\exp( -2\\pi i \\omega x )\\ dx \\\\\n& \\approx \\int\\limits_{a}^b f( x ) \\exp( -2\\pi i \\omega x )\\ dx\n\\quad \\text{dominio finito $[ a, b ]$ que concentra la integral} \\\\\n& \\approx \\sum\\limits_{k=0}^{N-1} f( x_k ) \\exp( -2 \\pi i \\omega x_k ) h\n\\quad \\text{discretización de la integral} \\\\\n& = h \\sum\\limits_{k=0}^{N-1} f( x_k ) \\exp( -2\\pi i \\omega ( a + k h ) ) \\\\\n& = h \\exp( -i 2\\pi \\omega a ) \\sum\\limits_{k=0}^{N-1} f( x_k ) \\exp\\left( -2\\pi i \\omega k \\frac{b - a}{N} \\right)\n\\end{split}\n\\]\ndonde \\(\\{f_k\\}\\) es la secuencia finita de números \\(f_k = f( x_k )\\) y \\(\\DFT\\) es la transformada de Fourier Discreta. La anterior relación es una aproximación para todo \\(\\omega\\). En particular se puede considerar \\(\\omega_j = \\frac{j}{b-a}\\), para \\(j \\in \\{0,\\ldots,N-1\\}\\) \\[\n\\begin{split}\n\\Fo( f )\\left( \\omega_j \\right)\n& \\approx h \\exp\\left( -2\\pi i \\frac{j}{b-a} a \\right)\n\\sum\\limits_{k=0}^{N-1} f_k \\exp\\left( -2\\pi i \\frac{jk}{N} \\right) \\\\\n\\hat{f}_j & \\approx h \\exp\\left( -2\\pi i \\frac{j}{b-a} a \\right) \\left( \\DFT\\left[ \\{f_k\\} \\right] \\right)_j\n\\quad \\text{por definición de la $\\DFT$}\n\\end{split}\n\\] entonces, lo anterior implica que podemos recuperar los valores aproximados a \\(f(x_k)\\) utilizando la transformada de Fourier discreta y su inversión. \\[\n\\{f( x_k )\\}\n\\approx \\DFT^{-1}\\left[ \\DFT\\left[ \\{f_k\\} \\right] \\right]\n= \\DFT^{-1}\\left[ \\left\\{ \\frac{1}{h} \\exp\\left( 2\\pi i \\frac{j}{b-a} a \\right) \\hat{f}_j \\right\\} \\right]\n\\]\nes de notar que numérica por la aritmética en coma flotante la expresión \\(\\DFT^{-1}\\left[ \\left\\{ \\frac{1}{h} \\exp\\left( 2\\pi i \\omega_j a \\right) \\hat{f}_j \\right\\} \\right]\\) puede tener parte compleja muy pequeña, cercana a \\(0\\). Para superar este posible problema numérico tomamos solo la parte real. \\[\n\\{f( x_k )\\}\n\\approx \\Re\\left( \\DFT^{-1}\\left[ \\left\\{ \\frac{1}{h} \\exp\\left( 2\\pi i \\frac{j}{b-a} a \\right)\n\\hat{f}_j \\right\\} \\right] \\right)\n\\]\n\n\nCode\nN &lt;- 3000\nalpha &lt;- 2\ntheta &lt;- 3\nb &lt;- qgamma( 0.9999, shape = alpha, scale = theta )\na &lt;- 0\nh &lt;- ( b - a ) / N\nn &lt;- 0:N\nx &lt;- a + n * h\nw &lt;- n / ( b - a )\neta &lt;- h * exp( -2 * pi * 1i * w * a )\nf &lt;- sapply( x, FUN = function( x ) dgamma( x, shape = alpha, scale = theta ) )\nFf &lt;- eta * fft( f )\nIFf &lt;- fft( eta^(-1) * Ff, inverse = TRUE ) / ( N + 1 )\nIFf &lt;- Re( IFf )\nerr &lt;- norm( f - IFf, type = '2' )\n\n\nEl error cuadrático de esta aproximación para el caso de la distribución \\(Gamma( \\alpha, \\theta )\\), como es de esperar es bastante pequeño. \\[\n\\left\\| \\{f( x_k ) \\} -\n\\DFT^{-1}\\left[ \\left\\{ \\frac{1}{h} \\exp\\left( 2\\pi i \\omega_j a \\right) \\hat{f}_j \\right\\} \\right] \\right\\|_2\n= 0.00000000000035173800\n\\]\nComo se puede observar se superponen cada una de las distribuciones la discretización \\(f_k\\) y la calculada con inversión de la transformada de Fourier discreta.\n\n\nCode\nxbrk &lt;- seq( 0, 40, length = 9 )\nxlbl &lt;- formatC( xbrk, digits = 0, format = 'f' )\nxlim &lt;- c( 0, 40 )\n\nybrk &lt;- seq( 0, 0.125, length = 10 )\nylbl &lt;- formatC( ybrk, digits = 3, format = 'f' )\nylim &lt;- c( 0, 0.125 )\n\nplt &lt;- ggplot() +\n  geom_line( aes( x = x, y = f ), colour = 'darkred', linewidth = 1 ) + \n  geom_point( aes( x = x, y = IFf ), colour = 'olivedrab3', size = 0.25 ) + \n  scale_x_continuous( breaks = xbrk,\n                      labels = xlbl, \n                      limits = xlim, \n                      expand = c( 0, 0 ) ) +\n  scale_y_continuous( breaks = ybrk, \n                      labels = ylbl, \n                      limits = ylim, \n                      expand = c( 0, 0 ) ) +\n  xlab( TeX( \"$x$\" ) ) + \n  ylab( TeX( \"$f(x)$\" ) ) + \n  theme_bw()\nplot( plt )\n\n\n\n\n\n\n\n\n\n\n\nCode\ne &lt;- f - IFf\n\nxmax &lt;- max( abs( e ) )\nxbrk &lt;- seq( -xmax, xmax, length = 9 )\nxlbl &lt;- formatC( xbrk, digits = 2, format = 'e' )\nxlim &lt;- c( -xmax, xmax )\n\nplt &lt;- ggplot() +\n  geom_histogram( \n    aes( x = e, y = after_stat( density ) ), \n    fill = 'grey50', \n    colour = 'dodgerblue3', \n    bins = nclass.scott( e ) ) + \n  geom_density() +\n  scale_x_continuous( breaks = xbrk,\n                      labels = xlbl, \n                      limits = xlim, \n                      expand = c( 0, 0 ) ) +\n  scale_y_continuous( labels = label_scientific( digits = 4 ) ) +\n  ylab( TeX( \"distribución del error\" ) ) + \n  xlab( TeX( \"error\" ) ) + \n  theme_bw()\nplot( plt )",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probabilidad y Estadística</span>"
    ]
  },
  {
    "objectID": "lectura_04.html#consideraciones-financieras",
    "href": "lectura_04.html#consideraciones-financieras",
    "title": "5  Probabilidad y Estadística",
    "section": "5.6 Consideraciones financieras",
    "text": "5.6 Consideraciones financieras\nAntes de desarrollar el contenido propio del curso, debemos tener en cuenta algunas consideraciones financieras como las siguientes:\n\n\n5.6.1 Función de actualización o descuento\n\nDefinition 5.38: Funciones de atualización y capitalizaciónLa función de actualización de flujos \\(v: \\R \\times \\R \\longrightarrow [0,1]\\), al evaluar en \\(s, t \\in \\R, s\\leq t, v(s,t)\\), diremos que actualizamos los flujos que se producen en el tiempo \\(t\\), valorados desde el tiempo \\(s\\). Además la función de actualización tiene las siguientes propiedades:\n\nSi \\(s = t, v(s,t) = 1\\),\nSi \\(s \\leq t, v(s,t) \\leq 1\\),\nSi \\(r \\leq s \\leq t, v( r, s ) v( s, t ) = v( r, t )\\).\n\nLa función de capitalización, es la función \\(u: \\R \\times \\R \\longrightarrow [0,1]\\), tal que \\(u( s, t ) v( s, t ) = 1\\).\n\n\nEl caso más particular y sencillo se presenta cuando la función de actualización es generada por una tasa constante \\(i \\in \\R\\) en el tiempo, es decir, la función de actualización toma la forma \\[\nv(s,t) = ( 1 + i )^{-(t-s)}\n\\]\n\n\n\n5.6.2 Flujos financieros\nUn flujo financiero discreto \\(c\\) es una serie de valores reales \\(c(t_1), c(t_2), \\cdots, c(t_n)\\) que se producen en un número discreto de tiempos \\(t_0 &lt; t_1 &lt; \\cdots &lt; t_n\\).\nEl valor presente de estos flujos, en un tiempo \\(t \\leq t_0\\), se lo puede calcular utilizando precisamente la función de actualización \\(v\\) \\[\nVP_t( c ) = \\sum\\limits_{k = 1}^n v( t, t_k )  c( t_k )\n\\] cuando \\(t=0\\), se suele solo expresar \\(VP( c ) = VP_0( c )\\).\n\n\n\n5.6.3 Flujos financieros probables\nUn flujo financiero discreto \\(c\\) es una serie de valores reales \\(c(t_1), c(t_2), \\cdots, c(t_n)\\) que se producen en un número discreto de tiempos \\(t_0 &lt; t_1 &lt; \\cdots &lt; t_n\\).\nEl valor actuarial presente de estos flujos, en un tiempo \\(t \\leq t_0\\), se lo puede calcular utilizando precisamente la función de actualización \\(v\\) \\[\nVAP_t( c ) = \\E\\left[ \\sum\\limits_{k = 1}^n v( t, t_k )  c( t_k ) \\right]\n= \\sum\\limits_{k = 1}^n v( t, t_k ) \\E\\left[ c( t_k ) \\right]\n\\] cuando \\(t=0\\), se suele solo expresar \\(VAP( c ) = VAP_0( c )\\).\nSi cada \\(c(t_k)\\) es una variable aleatoria discreta \\[\nVAP_t( c ) = \\sum\\limits_{k = 1}^n v( t, t_k ) \\E\\left[ c( t_k ) \\right]\n= \\sum\\limits_{k = 1}^n \\sum\\limits_{i=1}^{\\infty} v( t, t_k ) c_i( t_k ) p_i( t_k )\n\\]\n\n\n\n5.6.4 Equilibrio financiero\nSe dice que un flujo financiero \\(c(t_1), c(t_2), \\cdots, c(t_n)\\) como el anterior, está en equilibrio financiero si: \\[\nVP_0( c ) = \\sum\\limits_{k=0}^{n} v( 0, t_k ) c( t_k ) =  0\n\\]\nEl equilibrio financiero se mantiene en el tiempo, basta observar que para cualquier instante \\(t \\geq 0\\) \\[\n\\begin{split}\n0 & = u( 0, t ) VP_0( c ) \\\\\n& = u( 0, t ) \\sum\\limits_{k=0}^{n} v( 0, t_k ) c( t_k ) \\\\\n& = \\sum\\limits_{t_k \\leq t} u( 0, t ) v( 0, t_k ) c( t_k )\n+ \\sum\\limits_{t_k &gt; t} u( 0, t ) v( 0, t_k ) c( t_k ) \\\\\n& = \\sum\\limits_{t_k \\leq t} u( 0, t_k ) u( t_k, t ) v( 0, t_k ) c( t_k )\n+ \\sum\\limits_{t_k &gt; t} u( 0, t ) v( 0, t ) v( t, t_k ) c( t_k ) \\\\\n& = \\sum\\limits_{t_k \\leq t} u( t_k, t ) c( t_k )\n+ \\sum\\limits_{t_k &gt; t} v( t, t_k ) c( t_k ) \\\\\n\\end{split}\n\\]\nEsto implica que el valor actualizado a cualquier instante \\(t\\) de un flujo financiero \\(c\\) que está en equilibrio en un inicio, se mantiene también en equilibrio; siempre y cuando se preserve los flujos y tasas de actualización. A pesar de ser un resultado evidente, en la izquierda tenemos los flujos capitalizados hasta el tiempo \\(t\\) y en la derecha tenemos los flujos actualizados al tiempo \\(t\\). La expresión de la izquierda se conoce como la parte retrospectiva y la expresión de la derecha como la parte prospectiva.\nEn condiciones de equilibrio financiero la parte retrospectiva es igual a menos la parte prospectiva. \\[\n\\sum\\limits_{t_k \\leq t} u( t_k, t ) c( t_k ) =\n-\\sum\\limits_{t_k &gt; t} v( t, t_k ) c( t_k )\n\\] algunas veces se considera la parte prospectiva con el signo menos.\n\n\n\n\n\n\n1. Kolmogorov A, Fomin S, Silverman R (2012) Introductory Real Analysis. Dover Publications",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probabilidad y Estadística</span>"
    ]
  },
  {
    "objectID": "lectura_05.html",
    "href": "lectura_05.html",
    "title": "6  Distribuciones",
    "section": "",
    "text": "6.1 Distribuciones discretas",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distribuciones</span>"
    ]
  },
  {
    "objectID": "lectura_05.html#distribuciones-discretas",
    "href": "lectura_05.html#distribuciones-discretas",
    "title": "6  Distribuciones",
    "section": "",
    "text": "6.1.1 Distribución binomial\n\nDefinition 6.1: Distribución binomialUna variable aleatoria \\(N\\) que toma valores en \\(\\N\\) se dice que sigue una distribución o ley de binomial \\(N \\rightsquigarrow Bin( n, p )\\), con parámetros \\(n \\in \\N\\) y \\(p \\in [0, 1]\\), si: \\[\nP( N = k ) = \\binom{n}{k} p^k ( 1 - p )^{n-k}, \\qquad\n\\forall k \\in \\{0,\\ldots, n\\}\n\\] esta distribución discreta se caracteriza por presentar el valor \\(k \\frac{p_k}{p_{k-1}}\\) decreciente conforme cambia \\(k \\in \\N\\)\n\n\n\n\nCode\nset.seed(94312)\nn &lt;- 50\np &lt;- 0.3\nk &lt;- 2\nm &lt;- 100\nN &lt;- rbinom( n = m, size = n, prob = p ) # simular una muestra de tamaño m\npk &lt;- dbinom( x = k, size = n, prob = p ) # cálculo de probabilidad P( N = k )\nPk &lt;- pbinom( q = k, size = n, prob = p ) # cálculo de probabilidad P( N &lt;= k )\n\np &lt;- dbinom( x = 0:n, size = n, prob = p )\nv &lt;- 1:n * p[ 2:(n + 1) ] / p[ 1:n ]\n\n\n\n\nCode\nplt &lt;- ggplot() +\n  geom_point( aes( x = 1:n, y = v ), colour = 'darkred' ) + \n  xlab( TeX( \"$k$\" ) ) + \n  ylab( TeX( \"$k \\\\frac{p_k}{p_{k-1}}$\" ) ) + \n  theme_bw()\nplot( plt )\n\n\n\n\n\n\n\n\n\n\n\n\n6.1.2 Distribución de Poisson\n\nDefinition 6.2: Distribución de PoissonUna variable aleatoria \\(N\\) que toma valores en \\(\\N\\) se dice que sigue una distribución o ley de Poisson \\(N \\rightsquigarrow Pois( n, p )\\), con parámetro \\(\\lambda \\in \\R\\), si: \\[\nP( N = k ) = \\exp\\left( -\\lambda \\right) \\frac{\\lambda^k}{k!}, \\qquad\n\\forall k \\in \\N\n\\] esta distribución discreta se caracteriza por presentar el valor \\(k \\frac{p_k}{p_{k-1}}\\) constante conforme cambia \\(k \\in \\N\\)\n\n\n\n\nCode\nlambda &lt;- 2\nk &lt;- 2\nm &lt;- 100\nN &lt;- rpois( n = m, lambda = lambda ) # simular una muestra de tamaño m\npk &lt;- dpois( x = k, lambda = lambda ) # cálculo de probabilidad P( N = k )\nPk &lt;- ppois( q = k, lambda = lambda ) # cálculo de probabilidad P( N &lt;= k )\n\nn &lt;- 50\np &lt;- dpois( x = 0:n, lambda = lambda )\nv &lt;- 1:n * p[ 2:(n + 1) ] / p[ 1:n ]\n\n\n\n\nCode\nplt &lt;- ggplot() +\n  geom_point( aes( x = 1:n, y = v ), colour = 'darkred' ) + \n  xlab( TeX( \"$k$\" ) ) + \n  ylab( TeX( \"$k \\\\frac{p_k}{p_{k-1}}$\" ) ) + \n  theme_bw()\nplot( plt )\n\n\n\n\n\n\n\n\n\n\n\n\n6.1.3 Distribución binomial negativa\n\nDefinition 6.3: Distribución binomial negativaUna variable aleatoria \\(N\\) que toma valores en \\(\\N\\) se dice que sigue una distribución o ley de binomial negativa \\(N \\rightsquigarrow NBin( \\alpha, p )\\), con parámetro \\(\\alpha &gt; 0\\) y \\(p \\in (0,1)\\), si: \\[\nP( N = k ) = \\binom{\\alpha + k - 1}{k} p^\\alpha ( 1 - p )^k\n= \\frac{\\Gamma( \\alpha + k )}{\\Gamma(k+1) \\Gamma(\\alpha)}p^\\alpha ( 1 - p )^k, \\qquad\n\\forall k \\in \\N\n\\] donde \\(\\Gamma( \\alpha ) = \\int\\limits_0^{+\\infty} x^{\\alpha - 1} \\exp(-x)\\ dx\\), \\(\\forall \\alpha \\geq 0\\).\nEsta distribución discreta se caracteriza por presentar el valor \\(k \\frac{p_k}{p_{k-1}}\\) creciente conforme cambia \\(k \\in \\N\\)\n\n\n\n\nCode\nalpha &lt;- 2.5\np &lt;- 0.3\nk &lt;- 2\nm &lt;- 100\nN &lt;- rnbinom( n = m, size = alpha, prob = p ) # simular una muestra de tamaño m\npk &lt;- dnbinom( x = k, size = alpha, prob = p ) # cálculo de probabilidad P( N = k )\nPk &lt;- pnbinom( q = k, size = alpha, prob = p ) # cálculo de probabilidad P( N &lt;= k )\n\nn &lt;- 50\np &lt;- dnbinom( x = 0:n, size = alpha, prob = p )\nv &lt;- 1:n * p[ 2:(n + 1) ] / p[ 1:n ]\n\n\n\n\nCode\nplt &lt;- ggplot() +\n  geom_point( aes( x = 1:n, y = v ), colour = 'darkred' ) + \n  xlab( TeX( \"$k$\" ) ) + \n  ylab( TeX( \"$k \\\\frac{p_k}{p_{k-1}}$\" ) ) + \n  theme_bw()\nplot( plt )\n\n\n\n\n\n\n\n\n\n\nDefinition 6.4: Distribución geométricaUna variable aleatoria \\(N\\) que toma valores en \\(\\N\\) se dice que sigue una distribución o ley geométrica \\(N \\rightsquigarrow Geo( p )\\), con parámetro \\(p \\in (0,1]\\), si: \\[\nP( N = k ) = ( 1 - p )^k p, \\qquad\n\\forall k \\in \\N\n\\]\n\n\n\n\nCode\np &lt;- 0.3\nk &lt;- 2\nm &lt;- 100\nN &lt;- rgeom( n = m, prob = p ) # simular una muestra de tamaño m\npk &lt;- dgeom( x = k, prob = p ) # cálculo de probabilidad P( N = k )\nPk &lt;- pgeom( q = k, prob = p ) # cálculo de probabilidad P( N &lt;= k )\n\n\nEs fácil darse cuenta que la distribución geométrica \\(Geo( p )\\) es una binomial negativa \\(BN( 1, p )\\), con \\(\\alpha = 1\\).\nAsociado a estas distribuciones discretas existe un resultado de caracterización, el cual permite seleccionar la distribución de conteo.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distribuciones</span>"
    ]
  },
  {
    "objectID": "lectura_05.html#familia-de-panjer",
    "href": "lectura_05.html#familia-de-panjer",
    "title": "6  Distribuciones",
    "section": "6.2 Familia de Panjer",
    "text": "6.2 Familia de Panjer\nEl criterio anterior para identificar el tipo de distribución, mediante la observación del comportamiento de la variable \\(k \\frac{p_k}{p_{k-1}}\\), se formaliza precisamente en la definición de la familia de Panjer.\n\nDefinition 6.5: Familia de PanjerUna variable aleatoria discreta \\(N\\), que toma valores enteros positivos \\(N \\in \\N\\), se dice que pertenece a la familia de Panjer, si sus probabilidades \\(p_k = P( N = k )\\) para cada \\(k \\in \\N\\), satisfacen la siguiente relación de recurrencia. \\[\np_k = \\left( a + \\frac{b}{k} \\right)p_{k-1},\\qquad \\forall k \\in \\N \\setminus \\{0\\}\n\\]\n\n\nAdemás tenemos la siguiente proposición que caracteriza a la distribución de las variables aleatorias en la familia de Panjer.\n\nProposition 6.6: Caracterización familia de Panjer\nLas únicas leyes de probabilidad que satisfacen la relación de recurrencia anterior son:\n\nLa ley de Poisson, la cual se obtiene para \\(a = 0\\) y \\(b &gt; 0\\) \\[\nk \\frac{p_k}{p_{k-1}} = b &gt; 0,\\quad \\text{constante en $k$}\n\\]\nLa ley binomial negativa, la cual se obtiene para \\(0 &lt; a &lt; 1\\) y \\(a + b &gt; 0\\) \\[\nk \\frac{p_k}{p_{k-1}} = a k + b &gt; 0,\\quad \\text{creciente en $k$}\n\\]\nLa ley binomial, la cual se obtenida para \\(a &lt; 0\\) y \\(b = -a(m + 1)\\), para cierto \\(m\\) entero y positivo. \\[\nk \\frac{p_k}{p_{k-1}} = a( k - m - 1 ) &lt; 0, \\quad \\text{decreciente en $k$}\n\\]\n\n\n\nPara una demostración detallada de la proposición anterior se puede consultar [3] o en https://nonlifemaths.github.io/.\n\n\nCode\n# la librería CASdatasets fue previamente cargada\ndata( beMTPL97 )\nbeMTPL97 &lt;- as.data.table( beMTPL97 )\nconteo &lt;- beMTPL97[ , list( fn = .N ), by = list( sex, fuel, N = nclaims ) ]\nconteo[ , pn := fn / sum( fn ), by = list( sex, fuel ) ]\nsetorder( conteo, sex, fuel, N )\nconteo[ , pns := shift( pn, type = 'lag', fill = 0 ) ]\nconteo[ , jn := N * pns / pn ]\n\nconteo %&gt;%\n  kable(\n    label = NA,\n    caption = 'Estimación conteos por sexo',\n    row.names = FALSE,\n    col.names = c( \"sexo\", \"fuel\", \"$N$\", \"$f_k$\", \"$p_k$\", \"$p_{k+1}$\", \"$k \\\\frac{p_{k+1}}{p_k}$\" ),\n    align = 'llrrrrr',\n    digits = c( 0, 0, 0, 0, 5, 5, 5 ),\n    format.args = list( big.mark = ',', decimal.mark = '.', scientific = FALSE ),\n    escape = FALSE,\n    centering = TRUE, \n    booktabs = TRUE ) %&gt;%\n  kable_classic( full_width = FALSE, html_font = \"Cambria\", position = \"center\" ) %&gt;%\n  scroll_box( width = \"100%\", height = \"500px\" )\n\n\n\n\nEstimación conteos por sexo\n\n\nsexo\nfuel\n$N$\n$f_k$\n$p_k$\n$p_{k+1}$\n$k \\frac{p_{k+1}}{p_k}$\n\n\n\n\nfemale\ngasoline\n0\n29,533\n0.88741\n0.00000\n0.00000\n\n\nfemale\ngasoline\n1\n3,384\n0.10168\n0.88741\n8.72725\n\n\nfemale\ngasoline\n2\n326\n0.00980\n0.10168\n20.76074\n\n\nfemale\ngasoline\n3\n33\n0.00099\n0.00980\n29.63636\n\n\nfemale\ngasoline\n4\n3\n0.00009\n0.00099\n44.00000\n\n\nfemale\ngasoline\n5\n1\n0.00003\n0.00009\n15.00000\n\n\nfemale\ndiesel\n0\n8,557\n0.86539\n0.00003\n0.00000\n\n\nfemale\ndiesel\n1\n1,206\n0.12197\n0.86539\n7.09536\n\n\nfemale\ndiesel\n2\n109\n0.01102\n0.12197\n22.12844\n\n\nfemale\ndiesel\n3\n14\n0.00142\n0.01102\n23.35714\n\n\nfemale\ndiesel\n4\n2\n0.00020\n0.00142\n28.00000\n\n\nmale\ngasoline\n0\n71,357\n0.89714\n0.00020\n0.00000\n\n\nmale\ngasoline\n1\n7,417\n0.09325\n0.89714\n9.62074\n\n\nmale\ngasoline\n2\n682\n0.00857\n0.09325\n21.75073\n\n\nmale\ngasoline\n3\n74\n0.00093\n0.00857\n27.64865\n\n\nmale\ngasoline\n4\n7\n0.00009\n0.00093\n42.28571\n\n\nmale\ngasoline\n5\n1\n0.00001\n0.00009\n35.00000\n\n\nmale\ndiesel\n0\n35,489\n0.87614\n0.00001\n0.00000\n\n\nmale\ndiesel\n1\n4,532\n0.11188\n0.87614\n7.83076\n\n\nmale\ndiesel\n2\n439\n0.01084\n0.11188\n20.64692\n\n\nmale\ndiesel\n3\n41\n0.00101\n0.01084\n32.12195\n\n\nmale\ndiesel\n4\n5\n0.00012\n0.00101\n32.80000\n\n\n\n\n\n\n\n\n\nCode\nplt &lt;- ggplot() +\n  geom_point( data = conteo, aes( x = N, y = jn ), colour = 'purple' ) + \n  xlab( TeX( \"$k$\" ) ) + \n  ylab( TeX( \"$k \\\\frac{p_k}{p_{k-1}}$\" ) ) + \n  theme_bw()\nplot( plt )\n\n\n\n\n\n\n\n\n\nAdemás, una forma sencilla de estimar si una variable sigue una distribución de las tres antes descritas es estudiando su coeficiente de variación \\(\\VC( N ) = \\frac{\\V[N]}{\\E[N]}\\).\n\nSi \\(N\\) sigue una ley de Poisson \\(Pois(\\lambda)\\), entonces: \\[\n\\VC( N ) = \\frac{\\V[N]}{\\E[N]} = \\frac{\\lambda}{\\lambda} = 1\n\\]\nSi \\(N\\) sigue una ley de binomial negativa \\(NBinom( \\alpha, p )\\), entonces: \\[\n\\VC( N ) = \\frac{\\V[N]}{\\E[N]} = \\frac{\\alpha\\frac{1-p}{p^2}}{\\alpha\\frac{1-p}{p}} = \\frac{1}{p} &gt; 1\n\\]\nSi \\(N\\) sigue una ley de binomial \\(Binom( n, p )\\), entonces: \\[\n\\VC( N ) = \\frac{\\V[N]}{\\E[N]} = \\frac{np(1-p)}{np} = 1 - p &lt; 1\n\\]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distribuciones</span>"
    ]
  },
  {
    "objectID": "lectura_05.html#distribuciones-continuas",
    "href": "lectura_05.html#distribuciones-continuas",
    "title": "6  Distribuciones",
    "section": "6.3 Distribuciones continuas",
    "text": "6.3 Distribuciones continuas\n\n\n6.3.1 Distribución uniforme\n\nDefinition 6.7: Distribución uniformeUna variable aleatoria \\(X\\) a valores reales, sigue una distribución uniforme \\(X \\rightsquigarrow Unif( a, b )\\) de parámetros \\(a, b \\in \\R\\), si su función de distribución acumulada es de la siguiente forma: \\[\nF_X( x ) = \\frac{x-a}{b-a} \\mathbf{1}_{[a,b)}( x ) + \\mathbf{1}_{[b,+\\infty)}( x )\n\\] sin mucho esfuerzo se puede verificar que su densidad de probabilidad está dada por la función \\[\nf_X( x ) = \\frac{1}{b-a}\\mathbf{1}_{[a,b]}( x )\n\\] \\[\nM_X( t ) = \\frac{\\exp(bt)-\\exp(at)}{t(b-a)}\n\\] \\[\n\\E[X] = \\frac{a + b}{2},\\qquad \\V[X] = \\frac{(b - a)^2}{12}\n\\]\n\n\n\n\nCode\na &lt;- 1\nb &lt;- 2\nx &lt;- 1.5\nm &lt;- 100\nX &lt;- runif( n = m, min = a, max = b ) # simular una muestra de tamaño m\nfx &lt;- dunif( x = x, min = a, max = b ) # cálculo de la densidad f(x)\nFk &lt;- punif( q = x, min = a, max = b ) # cálculo de probabilidad F(x)\n\n\n\n\n\n6.3.2 Distribución exponencial\n\nDefinition 6.8: Distribución exponencialUna variable aleatoria \\(X\\) a valores reales, sigue una distribución exponencial \\(X \\rightsquigarrow Exp( \\lambda )\\) de parámetros \\(\\lambda &gt; 0\\), si su función de distribución acumulada es de la siguiente forma: \\[\nF_X( x ) = \\mathbf{1}_{(0,+\\infty)}( x ) \\left( 1 - \\exp\\left( -\\lambda x \\right) \\right)\n\\] sin mucho esfuerzo se puede verificar que su densidad de probabilidad está dada por la función \\[\nf_X( x ) = \\mathbf{1}_{(0,+\\infty)}( x ) \\lambda \\exp\\left( -\\lambda x \\right)\n\\] \\[\nM_X( t ) = \\frac{\\lambda}{\\lambda - t}\n\\] \\[\n\\E[X] = \\frac{1}{\\lambda},\\qquad \\V[X] = \\frac{1}{\\lambda^2}\n\\]\n\n\n\n\nCode\nlambda &lt;- 2\nx &lt;- 1.5\nm &lt;- 100\nX &lt;- rexp( n = m, rate = lambda ) # simular una muestra de tamaño m\nfx &lt;- dexp( x = x, rate = lambda ) # cálculo de la densidad f(x)\nFk &lt;- pexp( q = x, rate = lambda ) # cálculo de probabilidad F(x)\n\n\n\n\n\n6.3.3 Distribución gamma\n\nDefinition 6.9: Distribución gammaUna variable aleatoria \\(X\\) a valores reales, sigue una distribución gamma \\(X \\rightsquigarrow Gamma( \\alpha, \\beta )\\) de parámetros \\(\\alpha &gt; 0\\), \\(\\beta &gt; 0\\), si su función de distribución acumulada es de la siguiente forma: \\[\nF_X( x ) = \\frac{\\beta^\\alpha}{\\Gamma( \\alpha )} \\int\\limits_{0}^{x} u^{\\alpha-1} \\exp(-\\beta u)\\ du\n\\] si en caso \\(\\alpha\\) un entero positivo, i.e. \\(\\alpha \\in \\N^*\\), se puede calcular \\(F_X( x )\\) con la siguiente serie \\[\nF_X( x ) = 1 - \\exp( -\\lambda x ) \\sum\\limits_{n=0}^{\\alpha-1} \\frac{(\\lambda x)^n}{n!}\n= \\exp( -\\lambda x ) \\sum\\limits_{n=\\alpha}^{+\\infty} \\frac{(\\lambda x)^n}{n!}\n\\]\npor su parte, la densidad de probabilidad automáticamente está dada por la función: \\[\nf_X( x ) = \\mathbf{1}_{[0,+\\infty}( x ) \\frac{\\beta^\\alpha}{\\Gamma( \\alpha )} x^{\\alpha-1} \\exp(-\\beta x)\n\\] \\[\nM_X( t ) = \\left( \\frac{\\beta}{\\beta - t} \\right)^\\alpha,\\qquad \\text{si}\\ t &lt; \\beta\n\\] \\[\n\\E[X] = \\frac{\\alpha}{\\beta},\\qquad\n\\V[X] = \\frac{\\alpha}{\\beta^2}\n\\]\n\n\n\n\nCode\nalpha &lt;- 2\nbeta &lt;- 1\nx &lt;- 3\nm &lt;- 100\nX &lt;- rgamma( n = m, shape = alpha, scale = beta ) # simular una muestra de tamaño m\nfx &lt;- dgamma( x = x, shape = alpha, scale = beta ) # cálculo de la densidad f(x)\nFk &lt;- pgamma( q = x, shape = alpha, scale = beta ) # cálculo de probabilidad F(x)\n\n\n\n\n\n6.3.4 Distribución normal\n\nDefinition 6.10: Distribución normalUna variable aleatoria \\(X\\) a valores reales, sigue una distribución normal \\(X \\rightsquigarrow N( \\mu, \\sigma )\\) de parámetros \\(\\mu \\in \\R\\), \\(\\sigma &gt; 0\\), si su función de distribución acumulada es de la siguiente forma: \\[\nF_X( x ) = \\frac{1}{\\sqrt{2\\pi} \\sigma} \\int\\limits_{-\\infty}^x \\exp\\left( -\\frac{(y - \\mu)^2}{\\sigma^2} \\right)\\ dy\n\\] la densidad de probabilidad automáticamente está dada por la función: \\[\nf_X( x ) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left( -\\frac{(x - \\mu)^2}{\\sigma^2} \\right)\n\\] \\[\nM_X( t ) = \\exp\\left( t \\mu + \\frac{1}{2} t^2 \\sigma^2 \\right)\n\\] \\[\n\\E[X] = \\mu,\\qquad \\V[X] = \\sigma^2\n\\]\n\n\n\n\nCode\nmu &lt;- 2\nsigma &lt;- 1\nx &lt;- 3\nm &lt;- 100\nX &lt;- rnorm( n = m, mean = mu, sd = sigma ) # simular una muestra de tamaño m\nfx &lt;- dnorm( x = x, mean = mu, sd = sigma ) # cálculo de la densidad f(x)\nFk &lt;- pnorm( q = x, mean = mu, sd = sigma ) # cálculo de probabilidad F(x)\n\n\n\n\n\n6.3.5 Distribución log-normal\n\nDefinition 6.11: Distribución log-normalUna variable aleatoria \\(X\\) a valores reales, sigue una distribución log-normal \\(X \\rightsquigarrow LN( \\mu, \\sigma )\\) de parámetros \\(\\mu &gt; 0\\), \\(\\sigma &gt; 0\\), si su función de distribución acumulada es de la siguiente forma: \\[\nF_X( x ) = \\frac{1}{\\sqrt{2\\pi} \\sigma} \\int\\limits_{0}^{x} \\frac{1}{y} \\exp\\left( -\\frac{(\\ln(y) - \\mu)^2}{\\sigma^2} \\right)\\ dy\n\\] la densidad de probabilidad automáticamente está dada por la función: \\[\nf_X( x ) = \\frac{1}{x\\sqrt{2\\pi} \\sigma} \\exp\\left( -\\frac{(\\ln(x) - \\mu)^2}{\\sigma^2} \\right)\n\\] No hay forma analítica para \\(M_X\\) \\[\n\\E[X] = \\exp\\left( \\mu + \\frac{1}{2}\\sigma^2 \\right),\\qquad\n\\V[X] = \\exp\\left( 2 \\mu + \\sigma^2 \\right) \\left( \\exp( \\sigma^2 ) - 1 \\right)\n\\]\n\n\n\n\nCode\nmu &lt;- 2\nsigma &lt;- 1\nx &lt;- 3\nm &lt;- 100\nX &lt;- rlnorm( n = m, meanlog = mu, sdlog = sigma ) # simular una muestra de tamaño m\nfx &lt;- dlnorm( x = x, meanlog = mu, sdlog = sigma ) # cálculo de la densidad f(x)\nFk &lt;- plnorm( q = x, meanlog = mu, sdlog = sigma ) # cálculo de probabilidad F(x)\n\n\nEn pocas, una variable aleatoria \\(X \\rightsquigarrow LN( \\mu, \\sigma )\\) sigue una distribución log-normal si y solamente si la variable aleatoria dada por su logaritmo \\(\\ln( X ) \\rightsquigarrow N( \\mu, \\sigma )\\) sigue una distribución normal.\n\n\n\n6.3.6 Distribución de beta\nUna variable aleatoria \\(X \\in [0,1]\\) a valores reales, sigue una distribución beta \\(X \\rightsquigarrow Beta( \\alpha, \\beta )\\) de parámetros \\(\\alpha &gt; 0\\), \\(\\beta &gt; 0\\), si su función de distribución acumulada es de la siguiente forma: \\[\nF_X( x ) = \\frac{1}{B(\\alpha,\\beta)} \\int\\limits_{0}^x t^{\\alpha-1} ( 1 - t )^{\\beta - 1}\\ dt\n\\] la densidad de probabilidad automáticamente está dada por la función: \\[\nf_X( x ) = \\frac{x^{\\alpha-1} ( 1 - x )^{\\beta - 1}}{B(\\alpha,\\beta)}\n\\] \\[\nM_X( t ) = 1 + \\sum\\limits_{k=1}^{+\\infty} \\left( \\prod\\limits_{l=0}^{k-1} \\frac{\\alpha + l}{\\alpha + \\beta + l} \\right) \\frac{t^k}{k!}\n\\] \\[\n\\E[X] = \\frac{\\alpha}{\\alpha + \\beta},\\qquad\n\\V[X] = \\frac{\\alpha\\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)}\n\\]\n\n\nCode\nalpha &lt;- 1\nbeta &lt;- 2\nx &lt;- 0.5\nm &lt;- 100\nX &lt;- rbeta( n = m, shape1 = alpha, shape2 = beta ) # simular una muestra de tamaño m\nfx &lt;- dbeta( x = x, shape1 = alpha, shape2 = beta ) # cálculo de la densidad f(x)\nFk &lt;- pbeta( q = x, shape1 = alpha, shape2 = beta ) # cálculo de probabilidad F(x)\n\n\n\n\n\n6.3.7 Distribución de Pareto generalizada\n\nDefinition 6.12: Distribución de Pareto generalizadaUna variable aleatoria \\(X\\) a valores reales, sigue una distribución de Pareto generalizada \\(X \\rightsquigarrow GPD( \\mu, \\sigma, \\xi )\\) de parámetros \\(\\mu \\in \\R, \\sigma &gt; 0, \\xi \\in \\R\\), si su función de distribución acumulada es de la siguiente forma: \\[\nF_X( x )\n= \\left \\{\n\\begin{array}{ll}\n1 - \\left( 1 + \\xi \\frac{x-\\mu}{\\sigma} \\right)^{-\\frac{1}{\\xi}} & \\text{si}\\ \\xi \\neq 0 \\\\\n1 - \\exp\\left( -\\frac{x-\\mu}{\\sigma} \\right) & \\text{si}\\ \\xi = 0\n\\end{array}\n\\right.\n\\] y su densidad de probabilidad está dada por la función \\[\nf_X( x )\n= \\left \\{\n\\begin{array}{ll}\n\\frac{1}{\\sigma} \\left( 1 + \\xi \\frac{x-\\mu}{\\sigma} \\right)^{-1-\\frac{1}{\\xi}} & \\text{si}\\ \\xi \\neq 0 \\\\\n\\frac{1}{\\sigma} \\exp\\left( -\\frac{x-\\mu}{\\sigma} \\right) & \\text{si}\\ \\xi = 0\n\\end{array}\n\\right.\n\\] \\[\nM_X( t ) = \\exp(\\theta \\mu) \\sum\\limits_{j=0}^{+\\infty} \\frac{\\theta^j \\sigma^j}\n{\\prod\\limits_{k=0}^j ( 1 - k \\xi )}\n\\]\n\n\n\n\nCode\nxi &lt;- 1\nmu &lt;- 2\nsigma &lt;- 1\nx &lt;- 3\nm &lt;- 100\nX &lt;- rgpd( n = m, xi = xi, mu = mu, beta = sigma ) # simular una muestra de tamaño m\nfx &lt;- dgpd( x = x, xi = xi, mu = mu, beta = sigma ) # cálculo de la densidad f(x)\nFk &lt;- pgpd( q = x, xi = xi, mu = mu, beta = sigma ) # cálculo de probabilidad F(x)\n\n\n\n\n\n6.3.8 Distribución de valores extremos generalizada\n\nDefinition 6.13: Distribución de valores extremos generalizadaUna variable aleatoria \\(X\\) a valores reales, sigue una distribución generalizada de valores extremos \\(X \\rightsquigarrow GEV( \\mu, \\sigma, \\xi )\\) de parámetros \\(\\mu \\in \\R, \\sigma &gt; 0, \\xi \\in \\R\\), si su función de distribución acumulada es de la siguiente forma: \\[\nF_X( x )\n= \\left\\{\n\\begin{array}{ll}\n\\exp\\left( -\\exp\\left( -\\frac{x-\\mu}{\\sigma} \\right) \\right)\n& \\text{si}\\ \\xi = 0 \\\\\n\\exp\\left( -\\left( 1 + \\xi \\frac{x-\\mu}{\\sigma} \\right)^{-\\frac{1}{\\xi}} \\right)\n& \\text{si}\\ \\xi \\neq 0, 1 + \\xi\\frac{x - \\mu}{\\sigma} &gt; 0\n\\end{array}\n\\right.\n\\] además se puede verificar que su densidad de probabilidad está dada por la función \\[\nf_X( x )\n= \\left\\{\n\\begin{array}{ll}\n\\exp\\left(-\\frac{x-\\mu}{\\sigma}\\right)\n\\exp\\left(-\\exp\\left(-\\frac{x-\\mu}{\\sigma}\\right)\\right)\n& \\text{si}\\ \\xi = 0 \\\\\n\\left( 1 + \\xi \\frac{x - \\mu}{\\sigma}\\right)^{-1-\\frac{1}{\\xi}}\n\\exp\\left( -\\left( 1 + \\xi \\frac{x-\\mu}{\\sigma} \\right)^{-\\frac{1}{\\xi}} \\right)\n& \\text{si}\\ \\xi \\neq 0, 1 + \\xi\\frac{x - \\mu}{\\sigma} &gt; 0\n\\end{array}\n\\right.\n\\]\n\n\n\n\nCode\nxi &lt;- -1\nmu &lt;- 2\nsigma &lt;- 1\nx &lt;- 3\nm &lt;- 100\nX &lt;- rgev( n = m, xi = xi, mu = mu, beta = sigma ) # simular una muestra de tamaño m\nfx &lt;- dgev( x = x, xi = xi, mu = mu, beta = sigma ) # cálculo de la densidad f(x)\nFk &lt;- pgev( q = x, xi = xi, mu = mu, beta = sigma ) # cálculo de probabilidad F(x)\n\n\n\n\n\n6.3.9 Distribución t de Student\n\nDefinition 6.14: Distribución t de StudentUna variable aleatoria \\(X\\) a valores reales, sigue una distribución t de Student \\(X \\rightsquigarrow t( \\nu )\\) de parámetros \\(\\nu &gt; 0\\), si su función de distribución acumulada es de la siguiente forma: \\[\nF_X( x )\n= \\frac{1}{2} + \\frac{x}{\\sqrt{\\pi \\nu}}\n\\frac{\\Gamma\\left( \\frac{\\nu + 1}{2} \\right)}{\\Gamma\\left( \\frac{\\nu}{2} \\right)}\nF\\left( \\frac{1}{2}, \\frac{\\nu+1}{2}, \\frac{3}{2}, -\\frac{x^2}{\\nu} \\right)\n\\] donde \\(F\\) es la función hipergeométrica. \\[\nF( a, b, c, z ) = \\sum\\limits_{n=0}^{+\\infty} \\frac{(a)_n (b)_n}{(c)_n} \\frac{z^n}{n!}\n\\] con \\[\n(a)_n\n= \\left\\{\n\\begin{array}{ll}\n1 & n = 0 \\\\\na( a + 1 ) \\cdots (a + n - 1) & n &gt; 0\n\\end{array}\n\\right.\n\\] Además, se puede verificar que su densidad de probabilidad está dada por la función \\[\nf_X( x )\n= \\frac{x}{\\sqrt{\\pi \\nu}}\n\\frac{\\Gamma\\left(\\frac{\\nu+1}{2}\\right)}{\\Gamma\\left( \\frac{\\nu}{2} \\right)}\n\\left( 1 + \\frac{x^2}{\\nu} \\right)^{-\\frac{\\nu+1}{2}}\n\\] La función generadora de momentos \\(M_X( t )\\) no está definida \\[\n\\E[X]\n= \\left\\{\n\\begin{array}{ll}\n0 & \\text{si}\\ \\nu &gt; 0 \\\\\n\\text{no definida} & \\text{si}\\ \\nu \\leq 0\n\\end{array}\n\\right.\n\\]\n\\[\n\\V[X]\n= \\left\\{\n\\begin{array}{ll}\n\\frac{\\nu}{\\nu-2} & \\text{si}\\ \\nu &gt; 2 \\\\\n+\\infty & \\text{si}\\ 1 &lt; \\nu \\leq 2 \\\\\n\\text{no definida} & \\text{si}\\ \\nu \\leq 1\n\\end{array}\n\\right.\n\\]\n\n\n\n\nCode\nnu &lt;- 3\nx &lt;- 4\nm &lt;- 100\nX &lt;- rt( n = m, df = nu ) # simular una muestra de tamaño m\nfx &lt;- dt( x = x, df = nu ) # cálculo de la densidad f(x)\nFk &lt;- pt( q = x, df = nu ) # cálculo de probabilidad F(x)\n\n\n\n\n\n6.3.10 Distribución gamma transformada\n\nDefinition 6.15: Distribución gamma transformadaUna variable aleatoria \\(X\\) a valores reales, sigue una distribución gamma transformada \\(X \\rightsquigarrow GT( \\alpha, \\tau, \\theta )\\) de parámetros \\(\\alpha &gt; 0, \\tau &gt; 0, \\theta &gt; 0\\), si su función de distribución acumulada es de la siguiente forma: \\[\nF_X( x ) = \\frac{\\tau}{\\Gamma( \\alpha )}\n\\int\\limits_{0}^x\n\\frac{1}{u} \\left( \\frac{u}{\\theta} \\right)^{\\alpha} \\exp\\left(-\\left( \\frac{u}{\\theta} \\right)^{\\tau}\\right)\\ du\n\\]\nademás se puede verificar que su densidad de probabilidad está dada por la función: \\[\nf_X( x )\n=\n\\left\\{\n\\begin{array}{ll}\n0 & \\text{si}\\ x \\leq 0 \\\\\n\\frac{\\tau}{x \\Gamma( \\alpha )} \\left( \\frac{x}{\\theta} \\right)^{\\alpha} \\exp\\left(-\\left( \\frac{x}{\\theta} \\right)^{\\tau}\\right) & \\text{si}\\ x &gt; 0\n\\end{array}\n\\right.\n\\] \\[\n\\begin{split}\n\\E[X^k]\n& = \\frac{\\theta^k \\Gamma\\left( \\alpha + \\frac{k}{\\tau}\\right)}{\\Gamma( \\alpha )},\n\\quad \\text{si}\\ k &gt; -\\alpha \\tau \\\\\n\\E[X]\n& = \\frac{\\theta \\Gamma\\left( \\alpha + \\frac{1}{\\tau}\\right)}{\\Gamma( \\alpha )},\n\\quad \\text{si}\\ 1 &gt; -\\alpha \\tau \\\\\n\\V[X]\n& = \\frac{\\theta^2 \\Gamma\\left( \\alpha + \\frac{2}{\\tau}\\right)}{\\Gamma( \\alpha )}\n- \\frac{\\theta^2 \\Gamma\\left( \\alpha + \\frac{1}{\\tau}\\right)^2}{\\Gamma( \\alpha )^2}\n\\end{split}\n\\]\n\n\n\n\nCode\nalpha &lt;- 1\ntau &lt;- 1\ntheta &lt;- 1\nx &lt;- 3\nm &lt;- 100\nX &lt;- rtrgamma( n = m, shape1 = alpha, shape2 = tau, scale = theta ) # simular una muestra de tamaño m\nfx &lt;- dtrgamma( x = x, shape1 = alpha, shape2 = tau, scale = theta ) # cálculo de la densidad f(x)\nFk &lt;- ptrgamma( q = x, shape1 = alpha, shape2 = tau, scale = theta ) # cálculo de probabilidad F(x)\n\n\nEn la familia gamma se incluyen las siguientes distribuciones:\n\nLa distribución inversa gamma transformada, es decir es una familia estable por inversión\nLa distribución gamma para \\(\\alpha = n/2\\) y \\(\\theta = 2\\)\nLa distribución inversa gamma\nLa distribución de Weibull\nLa distribución inversa de Weibull\nLa distribución exponencial\nLa distribución inversa exponencial\n\n\n\n\n6.3.11 Distribución beta transformada\n\nDefinition 6.16: Distribución beta transformadaUna variable aleatoria \\(X\\) a valores reales, sigue una distribución beta transformada \\(X \\rightsquigarrow BT( \\alpha, \\gamma, \\tau, \\theta )\\) de parámetros \\(\\alpha &gt; 0, \\gamma &gt; 0, \\tau &gt; 0, \\theta &gt; 0\\), si su función de distribución acumulada es de la siguiente forma: \\[\nF_X( x ) = \\frac{\\Gamma(\\alpha + \\tau)}{\\Gamma( \\alpha ) \\Gamma( \\tau )}\n\\int\\limits_0^x\n\\frac{\\gamma \\left( \\frac{u}{\\theta} \\right)^{\\gamma \\tau}}{u\\left( 1 + \\left( \\frac{u}{\\theta} \\right)^{\\gamma}\\right)^{\\alpha + \\tau}}\\ du\n\\]\nademás se puede verificar que su densidad de probabilidad está dada por la función: \\[\nf_X( x )\n= \\mathbf{1}_{[0,+\\infty)}( x )\n\\frac{\\Gamma(\\alpha + \\tau)}{\\Gamma( \\alpha ) \\Gamma( \\tau )}\n\\frac{ \\gamma \\left( \\frac{x}{\\theta} \\right)^{\\gamma \\tau}}{x\\left( 1 + \\left( \\frac{x}{\\theta} \\right)^{\\gamma}\\right)^{\\alpha + \\tau}}\n\\]\n\\[\n\\begin{split}\n\\E[X^k]\n& = \\frac{\\theta^k \\Gamma\\left( \\tau + \\frac{k}{\\gamma}\\right) \\Gamma\\left( \\tau - \\frac{k}{\\gamma}\\right)}{\\Gamma( \\alpha ) \\Gamma( \\tau )},\n\\quad \\text{si}\\ -\\tau \\gamma &lt; k &lt; \\tau \\gamma \\\\\n\\E[X]\n& = \\frac{\\theta \\Gamma\\left( \\tau + \\frac{1}{\\gamma}\\right) \\Gamma\\left( \\tau - \\frac{1}{\\gamma}\\right)}{\\Gamma( \\alpha ) \\Gamma( \\tau )} \\\\\n\\V[X]\n& = \\frac{\\theta^2 \\Gamma\\left( \\tau + \\frac{2}{\\gamma}\\right) \\Gamma\\left( \\tau - \\frac{2}{\\gamma}\\right)}{\\Gamma( \\alpha ) \\Gamma( \\tau )}\n- \\frac{\\theta^2 \\Gamma\\left( \\tau + \\frac{1}{\\gamma}\\right)^2 \\Gamma\\left( \\tau - \\frac{1}{\\gamma}\\right)^2}{\\Gamma( \\alpha )^2 \\Gamma( \\tau )^2}\n\\end{split}\n\\]\n\n\n\n\nCode\nalpha &lt;- 1\ngamma &lt;- 1\ntau &lt;- 1\ntheta &lt;- 1\nx &lt;- 3\nm &lt;- 100\nX &lt;- rtrbeta( n = m, shape1 = alpha, shape2 = gamma, shape3 = tau, scale = theta ) # simular una muestra de tamaño m\nfx &lt;- dtrbeta( x = x, shape1 = alpha, shape2 = gamma, shape3 = tau, scale = theta ) # cálculo de la densidad f(x)\nFk &lt;- ptrbeta( q = x, shape1 = alpha, shape2 = gamma, shape3 = tau, scale = theta ) # cálculo de probabilidad F(x)\n\n\nDentro de la familia beta transformada se cuenta algunas distribuciones de probabilidad:\n\nLa distribución de Burr para \\(\\tau = 1\\)\nLa distribución de log-logística para \\(\\alpha = \\tau = 1\\)\nLa distribución de paralogística para \\(\\alpha = \\gamma, \\tau = 1\\)\nLa distribución de generalizada de Pareto para \\(\\gamma = 1\\)\nLa distribución de Pareto para \\(\\gamma = \\tau = 1\\)\nLa distribución de inversa de Burr para \\(\\alpha = 1\\)\nLa distribución de inversa de Pareto para \\(\\alpha = \\gamma = 1\\)\nLa distribución de inversa paralogística para \\(\\alpha = 1, \\gamma = \\tau\\)\n\nLa distribución transformada gamma es un caso límite de la distribución transformada beta, cuando \\(\\theta \\rightarrow +\\infty, \\alpha \\rightarrow +\\infty\\) y \\(\\theta \\alpha^{-\\frac{1}{\\gamma}} \\rightarrow \\xi\\)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distribuciones</span>"
    ]
  },
  {
    "objectID": "lectura_05.html#estimación",
    "href": "lectura_05.html#estimación",
    "title": "6  Distribuciones",
    "section": "6.4 Estimación",
    "text": "6.4 Estimación\nEn la práctica se observa la realización de una variable aleatoria \\(X\\), es decir se tiene una muestra de la misma \\(X_1, \\ldots, X_n\\). Pero, no se dispone de la distribución \\(F\\) o de la densidad \\(f\\) que la describe. Como ya hemos mencionado, conociendo la distribución se puede inferir algunas propiedades sobre la variable. De ahí surge la necesidad de buscar la mejor distribución \\(F\\) posible a partir de la muestra [2], [5], [1], [4].\nLa estimación usualmente consiste en tratar de determinar la probabilidad \\(P_X\\) asociada a \\(X\\), o en su defecto su función de distribución acumulada. Bajo la consideración, usualmente, de alguna información adicional, se considera que para la medida de de probabilidad \\(P_X\\) que caracteriza a la variable aleatoria \\(X\\) está dentro de una familia probabilidades que dependen de un parámetro \\(\\theta\\) el cual pertenece a un conjunto \\(\\Theta\\), precisamente parametriza a la familia, es decir, para cada \\(\\theta \\in \\Theta\\), \\(P_{\\theta}\\) es una medida de probabilidad y para algún \\(\\theta \\in \\Theta, P_X = P_{\\theta}\\).\nYa en la práctica se tiene una muestra, por lo regular finita, \\(X_1, \\ldots, X_n\\) de la variable aleatoria \\(X\\) que se desea comprender. Es a partir de la muestra que se intenta crear un estimador \\(\\tau\\) el cual sea precisamente el “mejor” bajo un cierto criterio que aproxime a la probabilidad \\(P_X\\), en términos algo más matemáticos \\(P_X \\approx P_{\\tau}\\). Por lo regular, se propone un estimador \\(\\tau\\) como función de la muestra y su tamaño \\(\\tau_n( X_1, \\ldots, X_n ) = \\tau( n, X_1, \\ldots, X_n )\\), en muchos casos se considera construir una función medible, que precisamente genere eventos observables. Es de notar que el estimador \\(\\tau_n\\) al depender de una muestra, la cual está constituida por variables aleatorias, como función de variables aleatorias se convierte también en una variable aleatoria al ser evaluada en estas.\nHay algunas propiedades deseables para una familia de estimadores \\(\\mathcal{E}\\), estas son:\n\nConsistencia\nInsesgamiento\nEficiencia\nSuficiencia\n\nPara, cada adjuntamos sus respectivas definiciones\n\nDefinition 6.17: Estimadores consistentesConsideramos una variable aleatoria \\(X\\), cuya medida de probabilidad asociada \\(P_X\\) pertenece a una familia de distribuciones de probabilidad \\(\\{P_\\theta\\}_{\\theta \\in \\Theta}\\), esto quiere decir que \\(P_X\\) está determinada por algún \\(\\theta \\in \\Theta\\), i.e. \\(P_X = P_{\\theta}\\). Entonces, la familia de estimadores \\(\\mathcal{E} = \\{ \\tau_n \\}_n\\) es consistente para la familia de probabilidades \\(\\{P_\\theta\\}_{\\theta \\in \\Theta}\\) si para cualquier \\(n \\in \\N\\) y muestra \\(X_1, \\ldots, X_n\\) de \\(X\\) el estimador \\(\\tau_n( X_1,\\ldots,X_n)\\) converge en probabilidad a \\(\\theta\\). Para todo \\(\\varepsilon &gt; 0\\) \\[\n\\underset{n \\rightarrow +\\infty}{\\lim} P_{\\theta}\\left( \\left| \\tau_n( X_1,\\ldots,X_n) - \\theta \\right| &gt; \\varepsilon\\right) = 0\n\\] también se dice que la familia de estimadores es convergente.\nDecimos que converge fuertemente si la convergencia de la familia de estimadores \\(\\mathcal{E} = \\{ \\tau_n \\}_n\\) al parámetro \\(\\theta\\) se da casi seguramente o en casi todas partes. Esto en término de límites implica que: \\[\nP_{\\theta}\\left( \\underset{n \\rightarrow +\\infty}{\\lim}\n\\tau_n\\left( X_1, \\ldots, X_n \\right) = \\theta \\right) = 1\n\\] En otras palabras, la probabilidad de que el límite del estimador \\(\\tau_n( X_1, \\ldots, X_n )\\) sea igual a \\(\\theta\\) es \\(1\\), salvo un conjunto de medida nula para \\(P_{\\theta}\\). De ahí resulta la terminología de convergencia casi segura o convergencia en casi todas partes.\n\n\n\nDefinition 6.18: Estimadores insesgadosBajo el mismo contexto de la definición anterior. Decimos que el estimador \\(\\tau_n\\) es insesgado si precisamente su esperanza es igual al parámetro a estimar \\(\\theta \\in \\Theta\\). \\[\n\\E\\left[ \\tau_n( X_1,\\ldots,X_n) \\right] = \\theta\n\\] La familia de estimadores \\(\\mathcal{E} = \\{ \\tau_n \\}_n\\) será insesgada si para cada \\(n\\) se satisface lo anterior.\n\n\nEl siguiente teorema es de importancia para caracterizar una familia de estimadores consistentes.\n\nTheorem 6.19Una familia de estimadores \\(\\{ \\tau_n \\}_n\\) para la cual se cumplen los siguientes límites\n\n\\(\\E\\left[ \\tau_n( X_1,\\ldots,X_n) \\right] \\rightarrow \\theta\\), conforme \\(n \\rightarrow +\\infty\\),\n\\(\\V\\left[ \\tau_n( X_1,\\ldots,X_n) \\right] \\rightarrow 0\\), conforme \\(n \\rightarrow +\\infty\\).\n\nEntonces, la familia \\(\\{ \\tau_n \\}_n\\) es consistente como estimadores de \\(\\theta\\).\n\n\n\nDefinition 6.20: Estimador más eficienteDe igual forma en el contexto anterior. En una familia de estimadores consistentes para estimar un parámetro, si entre estos estimadores existe uno para el cual su varianza sea mínima, entonces si existe tal estimador se dice que este es el estimador más eficiente. Sin \\(\\mathcal{E}\\) es la familia de estimadores y \\(\\xi\\) es el estimador más eficiente, entonces para cualquier otro estimador \\(\\tau \\in \\mathcal{E}\\), se tiene la siguiente desigualdad \\[\n\\V\\left[ \\xi \\right] \\leq \\V\\left[ \\tau \\right]\n\\] la eficiencia \\(E\\) de los estimadores en la familia \\(\\mathcal{E}\\) está dada por \\[\n0 \\leq E( \\tau ) = \\frac{\\V\\left[ \\xi \\right]}{\\V\\left[ \\tau \\right]} \\leq 1\n\\] Si además el estimador más eficiente \\(\\xi\\) es insesgado se dirá que este es un estimador de varianza mínima insesgado, o de forma corta MVUE por el término en inglés (minimum variance unbiased estimator).\n\n\n\nTheorem 6.21Si en una familia de estimadores \\(\\mathcal{E}\\) tenemos dos estimadores MVUE para el mismo parámetro \\(\\theta \\in \\Theta\\), entonces estos dos estimadores son iguales en casi todas partes o lo que es lo mismo solo son diferentes en un conjunto de medida nula.\n\n\n\nDefinition 6.22: Estimador suficienteDada una muestra \\(X_1, \\ldots, X_n\\) de la variable aleatoria \\(X\\). Un estimador \\(\\tau\\left( X_1, \\ldots, X_n \\right)\\) se dice un estimador suficiente para el parámetro \\(\\theta \\in \\Theta\\), si la distribución condicionada de la muestra \\(X_1, \\ldots X_n\\) dado el estimador \\(\\tau\\) es independiente del parámetro \\(\\theta\\).\n\n\nEl resultado a continuación es una caracterización del criterio de suficiencia, para una demostración se puede consultar [7], [4].\n\nTheorem 6.23: Teorema de factorizaciónUn estimador \\(\\tau\\) es suficiente para el parámetro \\(\\theta\\) si y solo si la densidad conjunta de la muestra \\(X_1, \\ldots, X_n\\) puede ser expresada en la forma. \\[\nf\\left( x_1, \\ldots, x_n \\right)\n= g\\left( \\theta, \\tau\\left( x_1, \\ldots, x_n \\right) \\right) h\\left( x_1, \\ldots, x_n \\right)\n\\]\n\n\n\nDefinition 6.24: Familia completaData un estimador \\(\\tau\\) que depende de una muestra aleatoria \\(X_1, \\ldots, X_n\\) y una familia de medidas de probabilidad \\(\\{ P_\\theta \\}_{\\theta \\in \\Theta}\\), para cada \\(\\theta \\in \\Theta\\), se puede construir la medida \\(P_{\\tau,\\theta}\\) asociada a la variable aleatoria \\(\\tau(X_1, \\ldots,X_n)\\) y al parámetro \\(\\theta\\) que determina la medida \\(P_\\theta\\). Para cualquier evento \\(A \\in \\F\\). \\[\nP_{\\tau,\\theta}( A ) = P_\\theta\\left( \\tau\\left(X_1, \\ldots,X_n\\right) \\in A \\right)\n\\]\nAsí, la familia de medidas de probabilidad \\(\\{ P_{\\tau,\\theta} \\}_{\\theta \\in \\Theta}\\) se dice completa si se satisface la siguiente implicación. Si una función \\(h : \\R \\longrightarrow \\R\\), tiene esperanza nula para cualquier esperanza tomada bajo la medida \\(P_{\\tau, \\theta}\\), entonces la función \\(h\\) es nula en casi todas partes para toda medida \\(P_\\theta\\). De forma más compacta. \\[\n\\forall P_{\\tau,\\theta}, \\E_{P_{\\tau,\\theta}}\\left[ h\\left( \\tau\\left( X_1,\\ldots, X_n \\right) \\right) \\right] = 0\n\\Longrightarrow\n\\forall P_\\theta, P_\\theta\\left( h\\left( \\tau\\left( X_1,\\ldots, X_n \\right) \\right) = 0 \\right) = 1\n\\] Por extensión si sucede esta implicación, se dice que \\(\\tau\\) es un estimador completo respecto de la familia \\(\\{ P_\\theta \\}_{\\theta \\in \\Theta}\\).\n\n\nPara ello se ha formulado diferentes aproximaciones, entre las cuales citamos las siguientes:\n\nMétodo de sustitución,\nMétodo de los momentos,\nMétodo de la distancia mínima,\n\n3.1 Método de mínimos cuadrados,\n3.2 Método de mínimo Chi-cuadrado,\n3.3 Método de varianza mínima,\n3.4 Método de minimización de la divergencia de Kullback–Leibler (máxima verosimilitud).\n\nMétodo de máxima verosimilitud,\nMétodo de Stein.\n\n\n\n6.4.1 Método de sustitución\nEntonces, se parte de suponer que existe funcional \\(G\\) actuando sobre el conjunto de medidas de probabilidad \\(\\mathcal{P}\\) que contiene al conjunto \\(P( \\Theta ) = \\{ P_{\\theta} \\mid \\theta \\in \\Theta\\} \\subset \\mathcal{P}\\) y que toma valores en \\(\\Theta\\), i.e. \\(G: \\mathcal{P} \\longrightarrow \\Theta\\). De tal forma que \\(P_{\\theta}\\) es invariante, es decir: \\[\nG( P_{\\theta} ) = \\theta,\\quad \\forall \\theta \\in \\Theta\n\\]\nAsí se construye un estimador por el método de sustitución si a partir de la medida de probabilidad empírica \\(P_n\\) construida con una muestra \\(X_1, \\ldots, X_n\\) de la variable aleatoria \\(X\\), se toma como parámetro el dado por: \\[\n\\widehat{\\theta} = G\\left( P_n \\right)\n\\]\nEn otras palabras se sustituye el parámetro \\(\\theta\\) por \\(\\widehat{\\theta}\\). Esto implica que se aproxima la medida \\(P_X\\) que caracteriza a \\(X\\), con la aproximación \\(P_X \\approx P_{\\widehat{\\theta}}\\).\nEs de notar que a priori no se hace establece ninguna medida de la calidad de la aproximación, para ello hay que adjuntar algunos otros criterios que caracterizan un buen tipo de estimador.\nEn otras ocasiones a partir de la muestra se define un estimador del parámetro \\(\\theta \\in \\Theta\\) a partir de una familia de funciones medibles que dependen directamente de la muestra \\(X_1, \\ldots, X_n\\) y su tamaño \\(n\\), i.e. una función \\(\\theta_n( X_1, \\ldots, X_n )\\). Se puede considerar el caso anterior como un caso en particular de este tipo de funciones, ya que se puede definir \\(\\theta_n( X_1, \\ldots, X_n ) = G( P_n )\\), pero hay que tener cuidado que \\(G\\) es un funcional y como tal puede resultar una función que no es medible.\n\n\n\n6.4.2 Método de momentos\nUn estimador de momento se construye a partir de una relación función entre la media de una función medible \\(g\\) y el parámetro \\(\\theta \\in \\Theta\\) que caracteriza a la distribución o medida de probabilidad de la variable aleatoria \\(X\\). Es decir, existen funciones \\(g\\) y \\(m\\) a valores reales de tal forma que \\[\nm( \\theta )\n= \\E_{P_X} \\left[ g( X ) \\right]\n= \\int\\limits_{\\R} g( x ) dP_X( x )\n= \\int\\limits_{\\R} g( x ) dP_{\\theta}( x )\n\\]\nSi de alguna forma se puede invertir \\(m\\), de tal forma que se pueda determinar \\(\\theta\\), en tal caso se pude utilizar un estimador por momentos a partir de una muestra de la variable aleatoria \\(X_1, \\ldots, X_n\\) \\[\n\\widehat{\\theta}\n= \\theta_n\\left( X_1, \\ldots, X_n \\right)\n= m^{-1}\\left( \\overline{g} \\right)\n\\] donde \\[\n\\overline{g}\n= \\int\\limits_{\\R} g( x ) dP_n( x )\n= \\frac{1}{n}\\sum\\limits_{i=1}^n g\\left( X_i \\right)\n\\]\nSi en caso \\(\\overline{g}\\) está fuera de la imagen de \\(m\\), \\(\\overline{g} \\notin m( \\Theta )\\), la inversión no sería posible. Para resolver este problema se puede recurrir a una distancia \\(d\\) y buscar \\(\\hat{g}\\) tal que minimize la distancia a \\(m( \\Theta )\\), i.e.  \\(\\hat{g} = \\underset{h \\in m( \\Theta )}{\\arginf}\\ d( \\overline{g}, h)\\). Una vez determinado \\(\\hat{g}\\) se toma como estimador de momentos su inversa con \\(m\\), i.e.  \\(\\widehat{\\theta} = m^{-1}( \\hat{g} )\\).\nEs de notar que en el caso anterior, la selección de la mejor distancia \\(d\\) no es para nada evidente y puede ser un problema tanto o más difícil que la misma estimación del parámetro \\(\\theta\\) o la inversión de \\(m\\).\nLa estimación por momentos en los casos más elaborados lleva a buscar la solución de problemas no lineales, que no suelen ser estables y que deben estar bien definidos para proveer una solución única.\n\n\n\n6.4.3 Método de la distancia mínima\nEste método requiere de la definición de una distancia sobre el espacio de distribuciones de probabilidad \\(\\mathcal{P}\\), i.e. una función \\(d : \\mathcal{P} \\times \\mathcal{P} \\longrightarrow \\R_+\\) que satisface las siguientes propiedades:\n\nPropiedad de simetría, para cualquier \\(P, Q \\in \\mathcal{P}\\), \\(d( P, Q ) = d( Q, P )\\),\nPara cualquier \\(P \\in \\mathcal{P}\\), \\(d( P, P ) = 0\\).\nDesigualdad triangular, para cualquier \\(P,Q,R \\in \\mathcal{P}\\) \\[\nd( P, Q ) \\leq d( P, R ) + d( R, Q )\n\\]\n\nA partir de la medida empírica de probabilidad \\(P_n\\), se busca una probabilidad \\(P_{\\theta}\\) que minimize la distancia medida con \\(d\\). Así resulta el estimador con el método de distancia mínima \\[\n\\widehat{\\theta} = \\underset{\\theta \\in \\Theta}{\\arginf} d\\left( P_{\\theta}, P_n \\right)\n\\]\nEntra las distancias que se puede considerar tenemos las siguientes\n\nDistancia del supremo entre las distribuciones acumuladas correspondientes \\[\nd(P,Q) = \\underset{x}{\\sup} \\left| F_P( x ) - F_Q( x ) \\right|\n\\]\nDistancia cuadrática entre las distribuciones acumuladas correspondientes \\[\nd( P, Q ) = \\int\\limits_{\\R} \\left( F_P( x ) - F_Q( x ) \\right)^2\\ dF_Q( x )\n\\]\nDistancia de Wasserstein, para \\(p &gt; 1\\) \\[\nd( P, Q )\n= W_p( P, Q )\n= \\underset{(X,Y) \\rightsquigarrow \\mu \\in \\Gamma( P, Q )}{\\inf} \\E_{\\mu}\\left[ |X - Y|^p \\right]^{\\frac{1}{p}}\n\\] donde \\[\n\\Gamma( P, Q ) = \\left\\{ \\mu \\middle| \\text{$\\mu$ es medida de probabilidad sobre $\\Omega \\times \\Omega$,\ncuyas distribuciones marginales son $P$ y $Q$}\\right\\}\n\\]\n\n\n\n\n6.4.4 Método de maximización de la verosimilitud\nLa estimación de verosimilitud parte de asumir que se observan una cierta cantidad de eventos independientes \\(B_1, \\ldots, B_n\\), relacionados precisamente a la variable aleatoria en estudio \\(X\\).\nSe postula precisamente que la mejor medida de probabilidad es aquella que maximiza la probabilidad de observar estos eventos independientes. En el caso en particular de la estimación de una medida de probabilidad en una familia \\(\\{P_{\\theta}\\}\\) con \\(\\theta \\in \\Theta\\), se busca maximizar la probabilidad: \\[\n\\underset{\\theta \\in \\Theta}{\\sup} \\prod\\limits_{i=1}^n P_{\\theta}\\left( B_i \\right)\n\\]\no de forma equivalente, se puede utilizar una transformación con un logaritmo y tratar de maximizar lo que conocemos como función de verosimilitud logarítmica \\[\n\\ell = \\sum\\limits_{i=1}^n \\log P_{\\theta} \\left( B_i \\right)\n\\]\nel problema de estimación se reduce a la siguiente optimización \\[\n\\underset{\\theta \\in \\Theta}{\\sup} \\ell( \\theta )\n\\]\nPrecisamente, el estimador de verosimilitud \\(\\widehat{\\theta}\\), es el que resuelve el problema de optimización donde \\(\\ell\\) es la función objetivo y \\(\\Theta\\) representa el conjunto de restricciones. \\[\n\\widehat{\\theta}\n= \\underset{\\theta \\in \\Theta}{\\argsup} \\ell( \\theta )\n\\]\nEn la práctica los eventos que se observa \\(B_i\\) son puntuales y son precisamente los valores que toma la variable aleatoria \\(X\\) para diferentes valores en el espacio muestra \\(\\Omega\\). Es decir, se observa una muestra \\(x_1 = X( \\omega_1 ), \\ldots, x_n = X( \\omega_n )\\).\nUna forma de atacar el problema de estimación, cuando las observaciones son puntuales es trabar de encerrar cada una de las observaciones en intervalos lo suficientemente pequeños. Se puede tomar un valor \\(h &gt; 0\\), y los eventos \\(B_i = \\left\\{ \\omega \\in \\Omega \\middle| x_i - h \\leq X( \\omega ) \\leq x_i + h \\right\\}\\) y una aproximación de la función de verosimilitud en función de \\(h\\), con la siguiente forma: \\[\n\\begin{split}\n\\ell_h( \\theta )\n& = \\sum\\limits_{i=1}^n \\log P_{\\theta} \\left( B_i \\right) \\\\\n& = \\sum\\limits_{i=1}^n \\log P_{\\theta} \\left( x_i - h \\leq X \\leq x_i + h \\right) \\\\\n& = \\sum\\limits_{i=1}^n \\log \\left( F\\left( x_i + h, \\theta \\right) - F\\left( x_i - h, \\theta \\right) \\right) \\\\\n\\end{split}\n\\]\nSabemos de los diferentes resultados de análisis para funciones de variación acotada, como es el caso de las distribuciones de probabilidad, que tienen diferenciales en casi todo punto, es decir que las singularidades de estas son conjuntos con medida nula. Además la cantidad de singularidades, de saltos que puede presentar una función de variación acotada es a lo sumo numerable. Entonces, cada valor observado \\(x_i \\in \\Dif F\\) es un punto de continuidad donde la función de distribución \\(F\\) es diferenciable o es un punto de singularidad, así los valores observados en la muestra se pueden clasificar en estos dos tipos [6]. De ello se puede dividir la suma anterior. \\[\n\\begin{split}\n\\ell_h( \\theta )\n& = \\sum\\limits_{x_i \\in \\Dif F} \\log \\left( F\\left( x_i + h, \\theta \\right) - F\\left( x_i - h, \\theta \\right) \\right)\n+ \\sum\\limits_{x_i \\notin \\Dif F} \\log \\left( F\\left( x_i + h, \\theta \\right) - F\\left( x_i - h, \\theta \\right) \\right) \\\\\n\\end{split}\n\\] maximizar \\(\\ell_h\\) es equivalente a maximizar la siguiente función equivalente que notaremos con el mismo nombre \\[\n\\begin{split}\n\\ell_h( \\theta )\n& = \\frac{1}{h} \\sum\\limits_{x_i \\in \\Dif F} \\log \\left( F\\left( x_i + h, \\theta \\right) - F\\left( x_i - h, \\theta \\right) \\right)\n+ \\sum\\limits_{x_i \\notin \\Dif F} \\log \\left( F\\left( x_i + h, \\theta \\right) - F\\left( x_i - h, \\theta \\right) \\right) \\\\\n\\end{split}\n\\] la multiplicación de la primera suma por la constante \\(\\frac{1}{h}\\) tan solo escala la función, pero no cambia sus puntos críticos.\nTomando al límite, un valor \\(h\\) cada vez más pequeño, para encerrar aún más en un intervalo los valores observados \\(\\{x_i\\}\\), tenemos lo siguiente: \\[\n\\begin{split}\n\\underset{h \\searrow 0}{\\lim} \\ell_h( \\theta )\n& = \\underset{h \\searrow 0}{\\lim} \\frac{1}{h} \\sum\\limits_{x_i \\in \\Dif F} \\log \\left( F\\left( x_i + h, \\theta \\right) - F\\left( x_i - h, \\theta \\right) \\right)\n+ \\underset{h \\searrow 0}{\\lim} \\sum\\limits_{x_i \\notin \\Dif F} \\log \\left( F\\left( x_i + h, \\theta \\right) - F\\left( x_i - h, \\theta \\right) \\right) \\\\\n& = \\sum\\limits_{x_i \\in \\Dif F} \\log \\left( f(x_i,\\theta) \\right)\n+ \\sum\\limits_{x_i \\notin \\Dif F} \\log \\left( F\\left( x_i, \\theta \\right) - F\\left( x_i-, \\theta \\right) \\right),\\quad\n\\text{si existe el límite por izquierda de $F$}\n\\end{split}\n\\]\nsiendo \\(F\\left( x_i-, \\theta \\right)\\) el límite por izquierda de la función de distribución \\(F\\) en el punto \\(x_i\\). Además, recordamos que toda función de distribución es continua por derecha, esto implica que \\(F\\left( x_i, \\theta \\right) = F\\left( x_i+, \\theta \\right)\\).\nPara el caso puntual esta es precisamente la expresión de la función de verosimilitud logarítmica, la cual podemos utilizar para estimar el mejor parámetro \\(\\theta \\in \\Theta\\) que maximice el valor de verosimilitud.\n\\[\n\\ell( \\theta ) =\n\\sum\\limits_{x_i \\in \\Dif F} \\log \\left( f(x_i,\\theta) \\right)\n+ \\sum\\limits_{x_i \\notin \\Dif F} \\log \\left( F\\left( x_i, \\theta \\right) - F\\left( x_i-, \\theta \\right) \\right)\n(\\#eq:loglik)\n\\]\nEn los casos más sencillos de estimación por verosimilitud se suele cumplir una de las siguientes hipótesis: todos los puntos \\(x_i\\) sobre los cuales se calcula son puntos de continuidad, o la función de distribución acumulada \\(F\\) no tiene puntos singulares, siendo este último caso el más usual. Pero, en la práctica esto no sucede y la suma adicional a la derecha en la expresión anterior es precisamente necesaria, ya que la distribución \\(F\\) si puede tener singularidades. Al utilizar solo la primera suma sobre los puntos donde hay diferenciabilidad, la estimación por verosimilitud para el parámetro \\(\\theta\\) no será la correcta.\nLos estimadores que resulta de la maximización de verosimilitud satisfacen algunas propiedades deseables.\n\nTheorem 6.25Los estimadores por maximización de verosimilitud, si en caso existen, estos son consistentes.\n\n\n\nTheorem 6.26Los estimadores por máximización de verosimilitud, si en caso existen, estos son eficientes.\n\n\n\nTheorem 6.27Si en caso existe un estimador suficiente, este es función del estimador de máxima verosimilitud.\n\n\nUn estimador de máxima verosimilitud no necesariamente es insesgado.\n\nTheorem 6.28Un estimador por maximización de verosimilitud se comporta normalmente de forma asintótica alrededor del verdadero parámetro \\(\\theta \\in \\Theta\\). Más claramente, si \\(\\tau_n\\) es el estimador por maximización de verosimilitud, se tiene que \\(\\tau_n( X_1, \\ldots, X_n ) \\rightsquigarrow F_n\\), entonces en el límite \\(n \\rightarrow +\\infty\\), se tiene la convergencia en distribución \\(F_n \\rightarrow N\\left( \\theta, I( \\theta )^{-1} \\right)\\).\nDonde \\(I(\\theta)\\), es un criterio de información, dado por la matriz \\[\n\\left[ I(\\theta) \\right]_{i,j}\n= \\left[ \\E\\left[ -\\frac{\\D^2}{\\D \\theta_i \\D \\theta_j}\\log f( X, \\theta )\\, \\middle|\\, \\theta \\right] \\right]_{i,j}\n\\]\n\n\n\n\n\n6.4.5 Pruebas de hipótesis\nUsualmente sobre un variable aleatoria \\(X\\) a valores en \\(\\R^n\\), se suele establecer una decisión basada en el valores que toma la variable aleatoria, la selección se realiza entre diferentes hipótesis. De manera más formal, esta decisión puede ser representada por una función \\(\\delta : \\R^n \\longrightarrow D\\), donde \\(D = \\{d_1,\\ldots,d_n\\}\\) es un conjunto discreto que representa las hipótesis. Se conoce a \\(\\delta\\) como un test estadístico de elección.\nEn muchos casos uno nos interesa encontrar el test estadístico \\(\\delta\\) que tenga el menor error de cometer una selección errada, así para cada decisión en \\(d \\in D\\), el test \\(\\delta\\) tendrá probabilidades \\(P( \\delta( X ) \\neq d )\\), es decir, para cualquier otro test \\(\\eta\\), se debe tener que \\[\nP( \\delta( X ) \\neq d ) \\leq P( \\eta( X ) \\neq d ),\\qquad \\forall d \\in D\n\\]\n\nDefinition 6.29: Test más potenteEn muchos casos uno no se interesa en cualquier tipo de test estadístico, sino en un cierto tipo de pruebas, más interesados en un subconjunto \\(K \\subset \\{ \\delta : \\R^n \\longrightarrow D \\}\\), a este se lo conoce como una clase. Así se dice que un test \\(\\delta \\in K\\) es el test más potente en la clase \\(K\\) para la hipótesis \\(d \\in D\\) si para cualquier otro test \\(\\eta \\in K\\), se tiene la desigualdad. \\[\nP( \\delta( X ) \\neq d ) \\leq P( \\eta( X ) \\neq d )\n\\]\n\n\n\n\n\n\n\n\n1. Bickel PJ, Doksum KA (2015) Mathematical statistics: Basic ideas and selected topics. CRC Press\n\n\n2. Borovkov A (1988) Estadística Matemática: Estimación de los parámetros, Verificación de las hipótesis, Capítulos adicionales. MIR\n\n\n3. Denuit M, Charpentier A (2005) Mathématiques de l’assurance non-vie. Economica, Paris\n\n\n4. Gupta SG, Kapoor VK (2020) Fundamental of Mathematical Statistics: A modern approach, 12th ed. Sultan Chand & Sons\n\n\n5. Knight K (1999) Mathematical statistics. Chapman & Hall/CRC\n\n\n6. Kolmogorov A, Fomin S, Silverman R (2012) Introductory Real Analysis. Dover Publications\n\n\n7. Olive DJ (2010) Statistical Theory and Inference. Springer",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distribuciones</span>"
    ]
  },
  {
    "objectID": "lectura_06.html",
    "href": "lectura_06.html",
    "title": "7  Modelos de pérdida agregada",
    "section": "",
    "text": "7.1 Mutualización del riesgo\nLa idea de mantener un seguro está basada en la mutualización de los riesgos. La mutualización como tal nace del mismo mecanismo bajo el cual funciona un seguro, cada asegurado transfiera su riesgo individual a la compañía de seguros, la suma total de estos riesgos \\(S\\) es el riesgo total que asume el asegurador.\nLos riesgos de cada uno de los \\(n \\in \\N\\) asegurados, pueden ser representados por variables aleatorias \\(X_1,\\ldots X_n\\), las mismas pueden ser independientes o dependientes entre ellas. El costo total del portafolio está dado por la suma de todos estos riesgos: \\[\nS = \\sum\\limits_{i=1}^n X_i\n\\] El conocer la distribución del costo total \\(S\\) es una tarea crucial para el asegurador.\nEl valor esperado de los reclamos totales, puede ser calculado fácilmente utilizando las propiedades de linealidad de la esperanza matemática \\(\\E\\): \\[\n\\E[ S ]\n= \\E\\left[ \\sum\\limits_{i=1}^n X_i \\right]\n= \\sum\\limits_{i=1}^n \\E\\left[ X_i \\right]\n\\] La varianza de la variable aleatoria del costo total \\(S\\), está dada por: \\[\n\\begin{split}\n\\V\\left[ S \\right]\n& = \\V\\left[ \\sum\\limits_{i=1}^n X_i \\right] \\\\\n& = \\sum\\limits_{i=1}^n \\V\\left[ X_i \\right]\n+ \\sum\\limits_{i=1}^n \\sum\\limits_{j=1,j\\neq i}^n \\mathbb{C}\\left[ X_i, X_j \\right] \\\\\n& = \\sum\\limits_{i=1}^n \\V\\left[ X_i \\right]\n+ 2\\sum\\limits_{i=1}^{n-1} \\sum\\limits_{j=i+1}^n \\mathbb{C}\\left[ X_i, X_j \\right]\n\\end{split}\n\\]\nMuchas de las veces el número total de reclamos \\(n\\) es incierto y por tal razón es mejor considerar que el número de siniestros vendrá dado por otra variable aleatoria discreta \\(N\\), que solo tomará valores en \\(\\N\\). \\[\nS = \\sum\\limits_{i=1}^N X_i\n\\]\nEsto nos lleva a considerar diferentes modelos de agregación de reclamos o pérdidas, en inglés “loss models” [1], [2].\nEstudiemos la forma general que podría tener la distribución de \\(S\\), sin realizar hipótesis previa sobre el comportamiento de las variables aleatorias \\(X_1, \\ldots, X_n\\) y \\(N\\). \\[\n\\begin{split}\nF_S( s )\n& = P( S \\leq s ) \\\\\n& = P\\left( \\sum\\limits_{i=1}^N X_i \\leq s \\right) \\\\\n& = P\\left( \\sum\\limits_{i=1}^N X_i \\leq s \\land N \\in \\N \\right) \\\\\n& = P\\left( \\bigcup\\limits_{n\\in \\N} \\left\\{ \\sum\\limits_{i=1}^N X_i \\leq s \\land N \\in \\{n\\} \\right\\}\\right) \\\\\n& = \\sum\\limits_{n=0}^{+\\infty} P\\left( \\sum\\limits_{i=1}^n X_i \\leq s \\land N = n \\right)\n\\qquad \\text{probabilidad de eventos disjuntos} \\\\\n& = \\sum\\limits_{n=0}^{+\\infty} P\\left( \\sum\\limits_{i=1}^n X_i \\leq s\\ \\middle|\\ N = n \\right) P( N = n )\n\\qquad \\text{propiedades de la probabilidad condicional} \\\\\n& = \\sum\\limits_{n=0}^{+\\infty} \\int\\limits_{\\left\\{\\sum\\limits_{i=1}^n x_i \\leq s\\right\\}} dF_n( x_1, \\ldots, x_n )\\ p_n\n\\qquad \\text{$F_n$ es la distribución conjunta de $X_1, \\ldots, X_n$ dado $N = n$}\n\\end{split}\n\\] La última expresión nos dice que para comprender el comportamiento de los reclamos totales \\(S\\), debemos estudiar y estimar la frecuencia de los reclamos \\(N\\), además de comprender y estimar cada uno de los reclamos \\(X_i\\) y su interacción, recordando que \\(N\\) puede tomar valores tendiendo al infinito. En términos resumidos, hay que comprender y estimar la frecuencia y severidad de los reclamos que están asociados al riesgo cubierto.\nLos términos integrales \\(\\int\\limits_{\\left\\{\\sum\\limits_{i=1}^n x_i \\leq s\\right\\}} dF_n( x_1, \\ldots, x_n )\\) asociados a la severidad presentan un verdadero reto estadístico y computacional; años atrás se desarrollaron algoritmos para calcular estos términos [3], [4].",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Modelos de pérdida agregada</span>"
    ]
  },
  {
    "objectID": "lectura_06.html#modelo_individual",
    "href": "lectura_06.html#modelo_individual",
    "title": "7  Modelos de pérdida agregada",
    "section": "7.2 Modelo individual",
    "text": "7.2 Modelo individual\n\nDefinition 7.1: Modelo individualEn el modelo individual de pérdida consideramos que el número de siniestros que se producirán es conocido, por ejemplo, puede ser a lo sumo el tamaño de la población asegurada, en tal caso la variable aleatoria \\(N\\) pasa a ser una constante, que representaremos por \\(n\\). De esta forma, la severidad total puede ser fácilmente representada por: \\[\nS = \\sum\\limits_{i=1}^n X_i\n\\]\nLa hipótesis más usual que sostiene a este modelo es la indenpendencia entre cada uno de los reclamos \\(X_i\\) y \\(X_j\\) para cualquier \\(1 \\leq i \\neq j \\leq n\\).\n\n\nEl valor esperado de la severidad total \\(S\\) es: \\[\n\\begin{split}\n\\E[S]\n& = \\E\\left[ \\sum\\limits_{i=1}^n X_i \\right] \\\\\n& = \\sum\\limits_{i=1}^n \\E\\left[ X_i \\right] \\\\\n& = n \\E[X] \\quad \\text{si $\\{X_i\\}$ son idénticamente distribuidas}\n\\end{split}\n\\]\nAsí mismo, la varianza de \\(S\\): \\[\n\\begin{split}\n\\V\\left[ S \\right]\n& = \\V\\left[ \\sum\\limits_{i=1}^n X_i \\right] \\\\\n& = \\sum\\limits_{i=1}^n \\V\\left[ X_i \\right]\n+ \\sum\\limits_{i=1}^n \\sum\\limits_{j=1,j\\neq i}^n \\mathbb{C}\\left[ X_i, X_j \\right] \\\\\n& = \\sum\\limits_{i=1}^n \\V\\left[ X_i \\right]\\quad \\text{si $\\{X_i\\}$ si son independientes entre si} \\\\\n& = n \\V\\left[ X \\right]\\quad \\text{si $\\{X_i\\}$ si son idénticamente distribuidas}\n\\end{split}\n\\]\nAl tener un número determinado de reclamos \\(n \\in \\N\\), la distribución de probabilidad del total de reclamos \\(S\\) puede ser calculada de la forma más sencilla: \\[\n\\begin{split}\nF_S( x )\n& = F_{X_1} \\star \\cdots \\star F_{X_n} ( s )\\quad \\text{si $\\{X_i\\}$ son independientes} \\\\\n& = F_{X}^{\\star n}( s )\\quad \\text{si $\\{X_i\\}$ son idénticamente distribuidas}\n\\end{split}\n\\]\nasí mismo, si las densidades de probabilidad están bien definidas, entonces: \\[\n\\begin{split}\nf_S( s )\n& = f_{X_1} \\star \\cdots \\star f_{X_n} ( s )\\quad \\text{si $\\{X_i\\}$ son independientes} \\\\\n& = f_{X}^{\\star n}( s )\\quad \\text{si $\\{X_i\\}$ son idénticamente distribuidas}\n\\end{split}\n\\]\nUna estrategia para estimar \\(F_S\\) o \\(f_S\\) es utilizar la transformada de Fourier \\(\\Fo\\), la cual convierte las convoluciones en productos y luego invertir de nuevo la transformada de Fourier: \\[\n\\begin{split}\nF_S\n& = \\Fo^{-1}\\left( \\Fo\\left( F_S \\right) \\right) \\\\\n& = \\Fo^{-1}\\left( \\Fo\\left( F_{X_1} \\star \\cdots \\star F_{X_n} \\right) \\right)\n\\quad \\text{si $\\{X_i\\}$ son independientes} \\\\\n& = \\Fo^{-1}\\left( \\prod\\limits_{i=1}^n \\Fo\\left( F_{X_i} \\right) \\right) \\\\\n& = \\Fo^{-1}\\left( \\Fo\\left( F_{X} \\right)^n \\right)\n\\quad \\text{si $\\{X_i\\}$ si son idénticamente distribuidas }\n\\end{split}\n\\]\nde forma equivalente para la densidad \\(f_S\\): \\[\n\\begin{split}\nf_S\n& = \\Fo^{-1}\\left( \\Fo\\left( f_S \\right) \\right) \\\\\n& = \\Fo^{-1}\\left( \\Fo\\left( f_{X_1} \\star \\cdots \\star f_{X_n} \\right) \\right)\n\\quad \\text{si $\\{X_i\\}$ son independientes} \\\\\n& = \\Fo^{-1}\\left( \\prod\\limits_{i=1}^n \\Fo\\left( f_{X_i} \\right) \\right) \\\\\n& = \\Fo^{-1}\\left( \\Fo\\left( f_{X} \\right)^n \\right)\n\\quad \\text{si $\\{X_i\\}$ si son idénticamente distribuidas }\n\\end{split}\n\\] Por otra parte, la función característica de la variable aleatoria \\(S\\), está dada para todo \\(t \\in \\R\\): \\[\n\\begin{split}\n\\varphi_S( t )\n& = \\E\\left[ \\exp\\left( it S \\right) \\right] \\\\\n& = \\E\\left[ \\exp\\left( it \\sum\\limits_{i=1}^n X_i \\right) \\right] \\\\\n& = \\E\\left[ \\prod\\limits_{i=1}^n \\exp\\left( it X_i \\right) \\right] \\\\\n& = \\prod\\limits_{i=1}^n \\E\\left[ \\exp\\left( it X_i \\right) \\right],\\quad\n\\text{si $\\{X_i\\}$ son independientes} \\\\\n& = \\prod\\limits_{i=1}^n \\varphi_{X_i}\\left( t \\right) \\\\\n& = \\varphi_{X}\\left( t \\right)^n,\\quad\n\\text{si $\\{X\\}$ son identicamente distribuidas} \\\\\n\\end{split}\n\\]\n\n\n7.2.1 Algoritmo de simulación\nSe puede simular la severidad total \\(S\\) para el caso donde se asume independencia entre cada una de las severidades \\(X_1, \\ldots, X_n\\) y se conoce cada una de sus densidades de probabilidad \\(f_{X_1}, \\ldots, f_{X_n}\\) o distribuciones de probabilidad \\(F_{X_1}, \\ldots, F_{X_n}\\).\nLos pasos son los siguientes:\n\nSe tiene fijo \\(n \\in \\N\\),\nSe fija el número de simulaciones \\(m \\in \\N\\)\nPara cada \\(i \\in \\{1, \\ldots, m\\}\\) se extrae una muestra \\(X_{i,1} \\rightsquigarrow f_{X_1},\\ldots,X_{i,n} \\rightsquigarrow f_{X_n}\\),\nPara cada \\(i \\in \\{1, \\ldots, m\\}\\) se calcula la severidad total para la muestra \\(i\\), \\(S_i = \\sum\\limits_{j=1}^n X_{i,j}\\)\n\n\nEl siguiente código ejemplifica el algoritmo anterior, donde se asume que cada variable aleatoria de severidad \\(X_i\\) sigue una ley log-normal \\(LN( \\mu_i, \\sigma_i )\\) con parámetros \\(\\mu_i, \\sigma_i\\), para cada \\(i \\in \\{1,\\ldots, n\\}\\). El reclamo total \\(S = \\sum\\limits_{i=1}^n X_i\\) sigue una ley de probabilidad que estará dada por la convolución de cada una de las leyes de probabilidad de cada reclamo \\(X_i\\), sin embargo estas leyes en este ejemplo son log-normales y no se conoce una forma explícita analítica para la convolución de log-normales. Por tal razón, debemos aproximar la distribución de probabilidad de \\(F_S\\) a partir de la distribución empírica.\n\n\nCode\nset.seed(94312)\n\n# 1. número de distribuciones\nn &lt;- 200\n# parámetros para las n distribuciones\nmu &lt;- seq( 1, 2, length.out = n )\nsigma &lt;- seq( 2, 3, length.out = n )\n\n# 2. número de simulaciones\nm &lt;- 1e3 \n\n# 3. simulación de severidades\nX &lt;- lapply( \n  X = 1:m, \n  FUN = function( i ) {\n    return( sapply( 1:n, FUN = function( j ) rlnorm( 1, meanlog = mu[ j ], sdlog = sigma[ j ] ) ) )\n  } \n)\n\n# 4. simulación de severidad total\nS &lt;- sapply( X, FUN = sum )\n\n\nComo mencionamos, en este ejemplo, la densidad de probabilidad de la severidad \\(S\\) resulta de la convolución de las \\(n\\) densidades individuales \\(f_S = f_{X_1} \\star \\cdots \\star f_{X_n}\\), la cual no presenta una forma analítica conocida.\nDe la imposibilidad anterior, se puede ver la utilidad de trabajar con la muestra aleatoria de la variable \\(S\\). De esta manera, se puede estimar la distribución acumulada de probabilidad \\(F_S\\) a partir de la distribución empírica \\(F_m\\) generada con la muestra \\(\\{S_i\\}\\). \\[\nF_S( s ) \\approx F_m( s ) = \\frac{1}{m} \\sum\\limits_{i=1}^m \\mathbf {1}_{(-\\infty,s]}( S_i )\n\\] el resultado de concentración 5.32 nos brinda un criterio de convergencia de \\(F_m\\) a \\(F_S\\)\nPara construir \\(F_m\\) en R, se puede utilizar la función ya empaquetada ecdf (empirical cumulative distribution function).\n\n\nCode\n# estimación distribución acumulada empírica de S\nFm &lt;- ecdf( S )\n\n# esperanza empírica\nEeS &lt;- mean( S )\n\n# esperanza teórica\nES &lt;- sum( sapply( 1:n, FUN = function( i ) exp( mu[i] + 0.5 * sigma[i]^2 ) ) )\n\ns &lt;- sort( unique( S ) )\nFms &lt;- sapply( s, FUN = Fm )\n\n\n\n\nCode\nsmax &lt;- 2.0e5\n\nplt &lt;- ggplot() +\n  geom_step( aes( x = s, y = Fms ), colour = 'dodgerblue4', linewidth = 1 ) + \n  geom_vline( xintercept = c( EeS, ES ), colour = c( 'olivedrab3', 'gold3' ), linewidth = 1 ) +\n  scale_x_continuous( breaks = seq( 0, smax, length = 11 ),\n                      labels = formatC( seq( 0, smax, length = 11 ), digits = 2, format = 'f' ), \n                      limits = c( 0, smax ), \n                      expand = c( 0, 0 ) ) +\n  scale_y_continuous( breaks = seq( 0, 1, length = 11 ),\n                      labels = formatC( seq( 0, 1, length = 11 ), digits = 2, format = 'f' ), \n                      limits = c( 0.0, 1.0 ), \n                      expand = c( 0.005, 0.005 ) ) +\n  xlab( TeX( \"$s$\" ) ) + \n  ylab( TeX( \"$F_m(s)$\" ) ) + \n  theme_bw()\nplot( plt )\n\n\n\n\n\n\n\n\n\nPara calcular la esperanza de la severidad total \\(S\\), también se puede usar una aproximación a la integral de Riemann-Stieltjes utilizando la distribución acumulada empírica \\(F_m\\). \\[\n\\E[ S ] = \\int\\limits_{\\R} s dF_S( s )\n\\approx \\int\\limits_{\\R} s dF_m( s )\n\\approx \\sum\\limits_{i=1}^N s_{i} \\left( F_m( s_{i+1} ) - F_m( s_{i} ) \\right)\n\\]\nEsta aproximación en R puede ser implementada de la siguiente forma:\n\n\nCode\nN &lt;- 1e5\ns &lt;- seq( min( S ), max( S ), length.out = N )\nEmS &lt;- sum( s[-N] * diff( sapply( s, FUN = Fm ) ) )\n\n\nDe ello, tenemos los siguientes resultados de cálculo para el valor esperado de la severidad total: \\[\n\\begin{split}\n\\E[S] = \\sum\\limits_{i=1}^n e^{ \\mu_i + \\frac{1}{2} \\sigma_i^2 } & = 34562.7208, \\\\\n\\overline{S} = \\frac{1}{m} \\sum\\limits_{i=1}^m S_i & = 33515.9195, \\\\\n\\sum\\limits_{i=1}^N s_{i} \\left( F_m( s_{i+1} ) - F_m( s_{i} ) \\right) & = 33494.1981\n\\end{split}\n\\]\n\nLo bueno de poseer una buena aproximación a la distribución acumulada de una variable aleatoria, es que podemos calcular otros valores de importancia relacionados a la variable aleatoria y no tan solo utilizar medidas de tendencia central. Sin embargo, para que esta aproximación sea útil se requiere reducir el error de probabilidad 5.32.\n\nPodemos considerar el caso sencillo donde el valor posible de severidad es determinista, es decir para cada póliza \\(i\\in \\{1,\\ldots,n\\}\\), el valor de severidad probable es único \\(M &gt; 0\\) si en caso se da un evento \\(A_i\\), esto lo podemos expresar como \\(X_i = M \\mathbf{1}_{A_i}\\) donde M es constante. Así, la pérdida total está dada por: \\[\nS = \\sum\\limits_{i=1}^n X_i = \\sum\\limits_{i=1}^n M \\mathbf{1}_{A_i}\n\\]\nEl valor total esperado de reclamos está dado por: \\[\n\\E[ S ]\n= \\sum\\limits_{i=1}^n \\E\\left[ M \\mathbf{1}_{A_i} \\right]\n= M \\sum\\limits_{i=1}^n P( A_i )\n\\] la última igualdad resulta de las propiedades de la función indicatriz 5.11.\nSi en caso todos los \\(P(A_i) = p\\) tienen la misma probabilidad, el valor total esperado de reclamos toma la siguiente forma: \\[\n\\E[S] = n M p\n\\]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Modelos de pérdida agregada</span>"
    ]
  },
  {
    "objectID": "lectura_06.html#modelo_colectivo",
    "href": "lectura_06.html#modelo_colectivo",
    "title": "7  Modelos de pérdida agregada",
    "section": "7.3 Modelo colectivo",
    "text": "7.3 Modelo colectivo\n\nDefinition 7.2: Modelo colectivoEl modelo colectivo de riesgo considera un número de reclamos descritos por una variable aleatoria discreta \\(N\\). Los reclamos corresponden a un número de pólizas en un periodo específico, el valor de cada reclamo \\(i \\in \\{1,\\ldots,N\\}\\) está representado por las variables aleatorias \\(X_i\\). Usualmente, se considera que cada uno de los reclamos \\(X_i\\) están idénticamente distribuidos. \\[\nS =\n\\left\\{\n\\begin{array}{ll}\n\\sum\\limits_{i=1}^N X_i & \\text{si}\\ N &gt; 0 \\\\\n0 & \\text{si}\\ N = 0\n\\end{array}\n\\right.\n\\] o de forma más compacta, se puede definir \\(S = \\sum\\limits_{i=1}^N X_i\\), donde se asume que la suma es igual a \\(0\\) si el número de elementos en la suma \\(N = 0\\).\n\n\nEl valor esperado del total de reclamos \\(S\\), está dado por: \\[\n\\begin{split}\n\\E[S]\n& = \\E\\left[ \\sum\\limits_{i=1}^N X_i \\right] \\\\\n& = \\sum\\limits_{n=0}^{+\\infty} \\E\\left[ \\sum\\limits_{i=1}^n X_i\\ \\middle|\\ N = n \\right]P( N = n )\n\\quad \\text{utilizando la esperanza condicional} \\\\\n& = 0 P( N = 0 ) + \\sum\\limits_{n=1}^{+\\infty} \\E\\left[ \\sum\\limits_{i=1}^n X_i\\ \\middle|\\ N = n \\right]P( N = n ) \\\\\n& = \\sum\\limits_{n=1}^{+\\infty} \\sum\\limits_{i=1}^n \\E\\left[ X_i \\mid N = n \\right]P( N = n )\n\\quad \\text{linealidad de la esperanza} \\\\\n& = \\sum\\limits_{n=0}^{+\\infty} n \\E\\left[ X \\mid N = n \\right]P( N = n )\n\\quad \\text{si $\\{X_i\\}$ son idénticamente distribuidas} \\\\\n& = \\E\\left[ X \\right] \\sum\\limits_{n=0}^{+\\infty} n P( N = n )\n\\quad \\text{si $X$ y $N$ son independientes} \\\\\n& = \\E[ N ] \\E[ X ]\n\\end{split}\n\\]\nTambién podemos calcular la varianza de los reclamos totales \\(S\\), para ello necesitamos calcular su segundo momento: \\[\n\\begin{split}\n\\E[S^2]\n& = \\E\\left[ \\left( \\sum\\limits_{i=1}^N X_i \\right)^2 \\right] \\\\\n& = \\sum\\limits_{n=0}^\\infty \\E\\left[ \\sum\\limits_{i, j=1}^n X_i X_j \\middle| N = n \\right] P( N = n )\n\\quad \\text{propiedades de la esperanza condicional} \\\\\n& = \\sum\\limits_{n=0}^\\infty \\sum\\limits_{i, j=1}^n \\E\\left[ X_i X_j \\right] P( N = n )\n\\quad \\text{si $\\{X_i\\}$ y $N$ son independientes} \\\\\n& = \\sum\\limits_{n=0}^\\infty \\left( \\sum\\limits_{i=1}^n \\E\\left[ X_i^2 \\right]\n+ \\sum\\limits_{i,j=1,i\\neq j}^n \\E\\left[ X_i X_j \\right] \\right) P( N = n ) \\\\\n& = \\sum\\limits_{n=0}^\\infty \\left( n \\E\\left[ X^2 \\right] + n(n-1) \\E\\left[ X \\right]^2 \\right) P( N = n )\n\\quad \\text{si $\\{X_i\\}$ son i.i.d} \\\\\n& = \\E\\left[N\\right] \\E\\left[X^2\\right] + \\E\\left[N^2\\right] \\E\\left[X\\right]^2\n- \\E\\left[N\\right] \\E\\left[X\\right]^2 \\\\\n& = \\E\\left[N\\right] \\V\\left[X\\right] + \\E\\left[N^2\\right] \\E\\left[X\\right]^2\n\\end{split}\n\\]\nfinalmente la varianza de \\(S\\) tiene la siguiente expresión: \\[\n\\begin{split}\n\\V\\left[S\\right]\n& = \\E\\left[S^2\\right] - \\E\\left[S\\right]^2 \\\\\n& = \\E\\left[N\\right] \\V\\left[X\\right]\n+ \\E\\left[N^2\\right] \\E\\left[X\\right]^2 - \\E\\left[N\\right]^2 \\E\\left[X\\right]^2 \\\\\n& = \\E\\left[N\\right] \\V\\left[X\\right] + \\V\\left[N\\right] \\E\\left[X\\right]^2\n\\end{split}\n\\]\nLa distribución acumulada del reclamo total \\(S\\) tiene la forma: \\[\n\\begin{split}\nF_S( s )\n& = P( S \\leq s ) \\\\\n& = P\\left( \\sum\\limits_{i=1}^N X_i \\leq s \\right) \\\\\n& = \\sum\\limits_{n=0}^{+\\infty} P\\left( \\sum\\limits_{i=1}^n X_i \\leq s \\middle| N = n \\right) P( N = n )\\quad\n\\text{utilizando la probabilidad condicional} \\\\\n& = \\sum\\limits_{n=0}^{+\\infty} P\\left( \\sum\\limits_{i=1}^n X_i \\leq s \\right) p_n\\quad\n\\text{si $X_i$ y $N$ son independientes} \\\\\n& = \\sum\\limits_{n=0}^{+\\infty} F_{X_1} \\star \\cdots \\star F_{X_n}( s ) p_n\\quad\n\\text{si $\\{X_i\\}$ son independientes} \\\\\n& = \\sum\\limits_{n=0}^{+\\infty} F^{\\star n}_{X}( s ) p_n\\quad\n\\text{si $\\{X_i\\}$ son idénticamente distribuidas}\n\\end{split}\n\\] tomando \\(F^{\\star 0}_{X}( s ) = 1\\).\nEl último resultado muestra que la distribución de probabilidad de \\(S\\) no es más que una mixtura de las distribuciones para los modelos individuales \\(F^{\\star n}_{X}\\), tomando para mezclarlas las probabilidades \\(p_n = P( N = n )\\) de la variable aleatoria discreta \\(N\\) que describe la frecuencia de los reclamos.\nAl igual que para el modelo individual, podemos estimar \\(F_S\\) o \\(f_S\\) utilizando la transformada de Fourier \\(\\Fo\\): \\[\n\\begin{split}\nF_S\n& = \\Fo^{-1}\\left( \\Fo\\left( F_S \\right) \\right) \\\\\n& = \\Fo^{-1}\\left( \\Fo\\left( \\sum\\limits_{n=0}^{+\\infty} F_{X_1} \\star \\cdots \\star F_{X_n}( s ) p_n \\right) \\right)\n\\quad \\text{si $\\{X_i\\}$ son independientes} \\\\\n& = \\Fo^{-1}\\left( \\sum\\limits_{n=0}^{+\\infty} \\prod\\limits_{i=1}^n \\Fo\\left( F_{X_i} \\right) p_n \\right) \\\\\n& = \\Fo^{-1}\\left( \\sum\\limits_{n=0}^{+\\infty} \\Fo\\left( F_{X} \\right)^n p_n \\right)\n\\quad \\text{si $\\{X_i\\}$ si son idénticamente distribuidas }\n\\end{split}\n\\]\nde forma equivalente para la densidad \\(f_S\\): \\[\n\\begin{split}\nf_S\n& = \\Fo^{-1}\\left( \\Fo\\left( f_S \\right) \\right) \\\\\n& = \\Fo^{-1}\\left( \\Fo\\left( \\sum\\limits_{n=0}^{+\\infty} f_{X_1} \\star \\cdots \\star f_{X_n}( s ) p_n \\right) \\right)\n\\quad \\text{si $\\{X_i\\}$ son independientes} \\\\\n& = \\Fo^{-1}\\left( \\sum\\limits_{n=0}^{+\\infty} \\prod\\limits_{i=1}^n \\Fo\\left( f_{X_i} \\right) p_n \\right) \\\\\n& = \\Fo^{-1}\\left( \\sum\\limits_{n=0}^{+\\infty} \\Fo\\left( f_{X} \\right)^n p_n \\right)\n\\quad \\text{si $\\{X_i\\}$ si son idénticamente distribuidas }\n\\end{split}\n\\]\nEn la práctica es imposible realizar la suma hasta infinito, por esto, se puede utilizar una aproximación escogiendo un número máximo \\(M \\in \\N\\) de términos en la suma, obteniendo así las aproximaciones.\nPor otra parte, la función característica de la variable aleatoria \\(S\\), está dada para todo \\(t \\in \\R\\): \\[\n\\begin{split}\n\\varphi_S( t )\n& = \\E\\left[ \\exp\\left( it S \\right) \\right] \\\\\n& = \\E\\left[ \\exp\\left( it \\sum\\limits_{i=1}^N X_i \\right) \\right] \\\\\n& = \\sum\\limits_{n=0}^{+\\infty}\n\\E\\left[ \\exp\\left( it \\sum\\limits_{i=1}^n X_i \\right) \\middle| N = n \\right] P( N = n ) \\\\\n& = \\sum\\limits_{n=0}^{+\\infty}\n\\E\\left[ \\prod\\limits_{i=1}^n \\exp\\left( it X_i \\right) \\middle| N = n \\right] p_n \\\\\n& = \\sum\\limits_{n=0}^{+\\infty} \\prod\\limits_{i=1}^n\n\\E\\left[ \\exp\\left( it X_i \\right) \\middle| N = n \\right] p_n,\\quad\n\\text{si $\\{X_i\\}$ son independientes} \\\\\n& = \\sum\\limits_{n=0}^{+\\infty} \\prod\\limits_{i=1}^n \\varphi_{X_i}\\left( t \\right) p_n,\\quad\n\\text{si $\\{X_i\\}$ y $N$ son independientes} \\\\\n& = \\sum\\limits_{n=0}^{+\\infty} \\varphi_{X}\\left( t \\right)^n p_n,\\quad\n\\text{si $\\{X\\}$ son identicamente distribuidas} \\\\\n& = \\E\\left[ \\varphi_{X}\\left( t \\right)^N \\right] \\\\\n& = \\E\\left[ \\exp\\left( N \\log\\left( \\varphi_{X}\\left( t \\right) \\right) \\right) \\right] \\\\\n& = \\varphi_N\\left( -i\\log\\left( \\varphi_{X}\\left( t \\right) \\right) \\right) \\\\\n\\end{split}\n\\]\n\n\n7.3.1 Algoritmo de simulación\nLa variable del reclamo total \\(S\\) puede ser simulada mediante el siguiente método montecarlo:\nSi \\(N\\) sigue una ley discreta \\(f_N\\) y cada valor severidad \\(X\\) son idénticamente distribuidos con ley \\(f_X\\).\n\nSeleccionar el número de simulaciones \\(m\\),\nSe genera una muestra de tamaño \\(m\\) de variables \\(N_1, \\ldots, N_m\\) con ley \\(f_N\\),\nSe genera para cada \\(i \\in \\{1,\\ldots,m\\}\\) una muestra de tamaño \\(N_i\\) de variables aleatorias \\(X_{i,1}, \\ldots X_{i,N_i}\\) con ley \\(f_X\\),\nSe calcula los reclamos totales \\(S_1, \\ldots, S_m\\) para cada simulación \\(i = \\{1, \\ldots, m\\}\\), mediante la siguiente suma \\(S_i = \\sum\\limits_{j=1}^{N_i} X_{i,j}\\).\n\nEn el lenguaje de programación R, este método de simulación puede ser fácilmente implementado, utilizando las funciones de aplicación vectorial sapply y lapply.\n\nConsideramos el caso de un modelo colectivo para el cuál el conteo de siniestros \\(N \\rightsquigarrow Pois( \\lambda )\\) y la distribución de cada uno de los reclamos \\(\\{X_i\\}_{i\\in \\N}\\), está dada por la misma distribución de probabilidad \\(X \\rightsquigarrow LN( \\mu, \\sigma )\\), siendo además cada uno de los reclamos independientes entre si.\n\n\nCode\n# 1. selección de simulaciones\nm &lt;- 1e4\n\n# 2. especificación de los parámetros para las distribuciones\nu &lt;- 5\ns &lt;- 2\nl &lt;- 3\n\n# 3. simulación del conteo de siniestros\nN &lt;- rpois( n = m, lambda = l )\n\n# 4. simulación de la severidad de los reclamos\nX &lt;- lapply( N, FUN = function( n ) rlnorm( n, meanlog = u, sdlog = s ) )\n\n# 5. reclamo total, agregación por cada simulación\nS &lt;- sapply( X, FUN = function( x ) sum( x, na.rm = TRUE ) )\n\n\n\nEs de notar que un algoritmo como el descrito anteriormente tiene una falencia cuando la frecuencia de siniestros es poco observada, esto sucede cuando la probabilidad \\(P( N = 0 )\\) es alta y por tal razón para generar suficientes reclamos y así poder tener cálculos con una buena aproximación numérica, se necesitarán muchas simulaciones.\nPara ciertos casos particulares, el modelo colectivo puede ser muchas veces calculado de forma similar a un modelo individual. Por ejemplo, si consideramos el caso cuando la variable aleatoria del conteo de siniestros \\(N \\rightsquigarrow Bin( n, p )\\) sigue una distribución binomial con parámetros \\(n\\) y \\(p\\) y los reclamos individuales \\(\\{ X_i \\}\\) son i.i.d. A partir de esto, el reclamo total \\(S\\) puede ser expresado de dos formas: \\[\nS = \\sum\\limits_{k=0}^{N} X_k\n= \\sum\\limits_{k=0}^n B_k X_k\n\\] donde las variables aleatorias \\(B_k \\rightsquigarrow Ber( p )\\) son independientes de las variables aleatorias \\(X_k\\) de cada uno de los reclamos. Así el modelo colectivo se transforma en un modelo individual con \\(n\\) constante y valores de reclamos individuales dados por la variable aleatoria \\(Y_k = B_k X_k\\).\nPara este caso en particular: \\[\n\\begin{split}\n\\E[ S ]\n& = \\E\\left[ \\sum\\limits_{k=0}^{n} B_k X_k \\right] \\\\\n& = \\sum\\limits_{k=0}^{n} \\E\\left[ B_k X_k \\right] \\\\\n& = n \\E[ B X ] \\\\\n& = n \\E[ B ] \\E[ X ] \\\\\n& = n p \\E[ X ]\n\\end{split}\n\\]\nasí mismo: \\[\n\\begin{split}\n\\V[ S ]\n& = n \\V[ B X ] \\\\\n& = n \\left( \\E[ B^2 X^2 ] - \\E[ B X ]^2 \\right) \\\\\n& = n \\left( \\E[ B^2 ] \\E[ X^2 ] - \\E[ B ]^2 \\E[ X ]^2 \\right) \\\\\n& = n \\left( p \\E[ X^2 ] - p^2 \\E[ X ]^2 \\right) \\\\\n& = n \\left( p \\E[ X^2 ] - p \\E[ X ]^2 + p \\E[ X ]^2  - p^2 \\E[ X ]^2 \\right) \\\\\n& = n \\left( p \\V[ X ] + p ( 1 - p ) \\E[ X ]^2 \\right) \\\\\n& = n \\left( \\E[B] \\V[ X ] + \\V[B] \\E[ X ]^2 \\right)\n\\end{split}\n\\]\n\nHacemos uso del algoritmo de simulación, pero bajo la consideración anterior donde el conteo de siniestros \\(N \\rightsquigarrow Bin( n, p )\\), en algunos casos se puede considerar \\(n\\) como el número de pólizas vendidas, esto supone que solo se presenta un reclamo por póliza. Por su parte, consideraremos los reclamos \\(X_i \\rightsquigarrow LN( \\mu, \\sigma )\\), para todo \\(i \\in \\{1,\\ldots,n\\}\\).\nSi consideramos generar la simulación como un modelo individual, generamos variables aleatorias \\(B_i \\rightsquigarrow Ber(p)\\), para todo \\(i \\in \\{1,\\ldots,n\\}\\).\n\n\nCode\nm &lt;- 1e4\nn &lt;- 1000\np &lt;- 0.2\nmu &lt;- 5\nsigma &lt;- 2\nB &lt;- lapply( 1:m, FUN = function( i ) rbinom( n, size = 1, prob = p ) )\nX &lt;- lapply( 1:m, FUN = function( i ) rlnorm( n, meanlog = mu, sdlog = sigma ) )\nS &lt;- sapply( 1:m, FUN = function( i ) sum( B[[ i ]] * X[[ i ]], na.rm = TRUE ) )\n\nEeS &lt;- mean( S )\nES &lt;- n * p * exp( mu + 0.5 * sigma^2 )\n\n\n\\[\n\\begin{split}\n\\E[S] = n p e^{ \\mu + \\frac{1}{2} \\sigma^2 } & = 219326.6317, \\\\\n\\overline{S} = \\frac{1}{m} \\sum\\limits_{i=1}^m S_i & = 219346.3972,\n\\end{split}\n\\]\n\nEs importante observar que si la frecuencia de reclamos es superior a \\(1\\), sea esta por póliza, individuo o en general por unidad asegurada, la aproximación anterior no es la correcta si no se realiza un ajuste al valor \\(n\\) que es el número máximo de siniestros. También, se puede considerar que las variables de los reclamos es la suma total de reclamos por póliza.\nPor otra parte, es de notar que en el cálculo hay un gasto innecesario de valores simulados de reclamos \\(X_i\\), ya que algunos se multiplicaran por \\(B_i\\), la cual solo toma valores \\(\\{0,1\\}\\).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Modelos de pérdida agregada</span>"
    ]
  },
  {
    "objectID": "lectura_06.html#modelos_mixtos",
    "href": "lectura_06.html#modelos_mixtos",
    "title": "7  Modelos de pérdida agregada",
    "section": "7.4 Modelos mixtos",
    "text": "7.4 Modelos mixtos\n\nDefinition 7.3: Modelo mixtoEn algunos casos en particular se considera un modelo de agregación que es una mixtura entre el modelo individual y el modelo colectivo. Se parte de considerar un número \\(n\\) de unidades aseguradas, donde para cada unidad \\(i \\in \\{1, \\ldots, n\\}\\) se considera que puede presentar una cantidad de reclamos dados por una variable aleatoria discreta \\(N_i\\), cada uno de las \\(i\\) unidades pueden presentar un número de reclamos \\(X_{i,1}, \\ldots, X_{i,N_i}\\). De agregar el total de reclamos, se tiene la expresión: \\[\nS\n= \\sum\\limits_{i=1}^n \\sum\\limits_{j=1}^{N_i} X_{i,j}\n\\]\n\n\nSi en caso todos los reclamos \\(X_{i,j}\\) son idénticamente distribuidos entre unidades aseguradas y reclamos, entonces el reclamo total anterior puede ser visto como un modelo colectivo de pérdida: \\[\nS\n= \\sum\\limits_{j=1}^{N} X_j\n\\] tomando el conteo de siniestros como la variable aleatoria \\(N = \\sum\\limits_{i=1}^n N_i\\).\nEl mismo modelo mixto, también puede ser interpretado como un modelo individual de pérdida, con una suma constante de siniestros. \\[\nS\n= \\sum\\limits_{i=1}^n Y_i\n\\]\ndonde cada variable aleatoria \\(Y_i = \\sum\\limits_{j=1}^{N_i} X_{i,j}\\).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Modelos de pérdida agregada</span>"
    ]
  },
  {
    "objectID": "lectura_06.html#uso_limite_central",
    "href": "lectura_06.html#uso_limite_central",
    "title": "7  Modelos de pérdida agregada",
    "section": "7.5 Teorema del límite central y su uso",
    "text": "7.5 Teorema del límite central y su uso\nEn varias situaciones es difícil determinar la distribución de los reclamos totales \\(S\\), como ya hemos visto no hay una forma explícita sencilla.\nPara tener una primera aproximación a la distribución, se puede utilizar el teorema del límite central 5.28, para ello se requiere un gran número de elementos en la suma de los reclamos totales \\(S\\). Como ya lo muestra la desigualdad 5.31, la convergencia requiere un número de términos inversamente proporcional al cuadrado del error de aproximación. Sin embargo, si en caso el número de reclamos \\(n\\) no es lo suficientemente grande como para obtener un error de aproximación deseado, podemos hacer uso de un resultado más fuerte como la desigualdad de concentración de 5.31, la misma no solo implica el teorema del límite central, sino que además proporciona una acotación y velocidad de convergencia conforme se aumenta \\(n\\). Esta desigualdad puede ser de gran utilidad para tener una acotación de la distribución de los reclamos totales, para cualquier valor de \\(n\\).\nMás claramente, para el caso del modelo individual, donde \\(n\\) es una constante, el teorema 5.31 nos proporciona la siguiente desigualdad: \\[\n\\begin{split}\n\\underset{u\\in \\R}{\\sup}\\left( 1 + |u|^3 \\right)\\left| P\\left( \\frac{S - \\E[S]}{\\sqrt{\\V[S]}}  \\leq u\\right) - \\Phi( u ) \\right|\n& \\leq \\frac{C}{\\sqrt{n}} \\frac{\\E\\left[\\left|X - \\E[X]\\right|^3\\right]}{\\sqrt{\\V[X]}^3} \\\\\n\\left( 1 + |u|^3 \\right)\\left| P\\left( \\frac{S - \\E[S]}{\\sqrt{\\V[S]}}  \\leq u\\right) - \\Phi( u ) \\right|\n& \\leq \\frac{C}{\\sqrt{n}} \\frac{\\E\\left[\\left|X - \\E[X]\\right|^3\\right]}{\\sqrt{\\V[X]}^3},\\qquad \\forall u \\in \\R \\\\\n\\end{split}\n\\] Para simplificar la manipulación de las expresiones podemos tomar: \\(\\tau = \\frac{\\E\\left[\\left|X - \\E[X]\\right|^3\\right]}{\\sqrt{\\V[X]}^3}\\) \\[\\begin{gather*}\n-\\frac{C}{\\sqrt{n}} \\tau\n\\leq \\left( 1 + |u|^3 \\right)\\left( P\\left( \\frac{S - \\E[S]}{\\sqrt{\\V[S]}}  \\leq u\\right) - \\Phi( u ) \\right)\n\\leq \\frac{C}{\\sqrt{n}} \\tau \\\\\n\\Phi( u ) - \\frac{C}{\\sqrt{n}\\left( 1 + |u|^3 \\right)} \\tau\n\\leq P\\left( \\frac{S - \\E[S]}{\\sqrt{\\V[S]}}  \\leq u\\right)\n\\leq \\Phi( u ) + \\frac{C}{\\sqrt{n}\\left( 1 + |u|^3 \\right)} \\tau  \\\\\n\\Phi( u ) - \\frac{C}{\\sqrt{n}\\left( 1 + |u|^3 \\right)} \\tau\n\\leq P\\left( S \\leq \\E[S] + \\sqrt{\\V[S]} u\\right)\n\\leq \\Phi( u ) + \\frac{C}{\\sqrt{n}\\left( 1 + |u|^3 \\right)} \\tau \\\\\n\\max\\left( \\Phi\\left( \\frac{s - \\E[S]}{\\sqrt{\\V[S]}} \\right) - \\frac{C}{\\sqrt{n}\\left( 1 + \\left| \\frac{s - \\E[S]}{\\sqrt{\\V[S]}} \\right|^3 \\right)} \\tau, 0 \\right)\n\\leq P\\left( S \\leq s \\right)\n\\leq \\min\\left( \\Phi\\left( \\frac{s - \\E[S]}{\\sqrt{\\V[S]}} \\right) + \\frac{C}{\\sqrt{n}\\left( 1 + \\left|\\frac{s - \\E[S]}{\\sqrt{\\V[S]}}\\right|^3 \\right)} \\tau, 1 \\right)\n\\end{gather*}\\] cambiando la variable \\(s = \\E[S] + \\sqrt{\\V[S]} u\\). El \\(\\max\\) y \\(\\min\\) resultan debido a que una probabilidad siempre está acotada en el rango de \\(0\\) a \\(1\\).\nEl ejemplo a continuación muestra un claro uso del resultado anterior.\n\nEn este caso en particular consideramos un modelo individual con reclamos determinados por variables aleatorias que siguen una distribución \\(LN( \\mu, \\sigma )\\) y un número de \\(n = 7000\\) reclamos. Además, como es usual asumimos que las variables aleatorias de los reclamos \\(X_1, \\ldots, X_n\\) son i.i.d.\n\n\nCode\nn &lt;- 7e3\nmu &lt;- 4.0\nsigma &lt;- 2.0\nEX &lt;- exp( mu + 0.5 * sigma^2 )\nVX &lt;- ( exp( sigma^2 ) - 1 ) * exp( 2 * mu + sigma^2 )\nES &lt;- n * EX\nVS &lt;- n * VX\n\n# utilizamos simulación, no hay forma analítica para la distribución de la suma de log-normales\nm &lt;- 5e3\nS &lt;- sapply( 1:m, FUN = function( i ) sum( rlnorm( n, meanlog = mu, sdlog = sigma ) ) )\nFS &lt;- ecdf( S ) \n\nC &lt;- 0.7655 + 8 * ( 1 + exp(1) ) # constante universal del teorema\nf &lt;- function( x ) ( abs( x - EX )^3 ) * dlnorm( x, meanlog = mu, sdlog = sigma )\n\n# forzando este término, la distribución log-normal no tiene función generadora de momentos\ntau &lt;- integrate( f = f, lower = 0, upper = 1e10, subdivisions = 1e4, rel.tol = 1e-10 )$value / sqrt( VX )^3\n\ns &lt;- seq( 0, 6e6, length = 5000 )\nFms &lt;- sapply( s, FUN = FS )\n\nu &lt;- ( s - ES ) / sqrt( VS )\nr &lt;- 1 + abs( u )^3\nFmax &lt;- pmin( pnorm( u ) + C * tau / ( sqrt( n ) * r ), 1 ) # cota superior\nFmin &lt;- pmax( pnorm( u ) - C * tau / ( sqrt( n ) * r ), 0 ) # cota inferior\nPhi &lt;- sapply( s, FUN = function( s ) pnorm( ( s - ES ) / sqrt( VS ) ) )\n\n\nComo se puede ver en el siguiente gráfico, la acotación funciona muy bien y al aumentar el número de siniestros \\(n\\) se disminuye el rango de la acotación, proporcionando así una mejor estimación de la distribución de los reclamos totales \\(S\\).\n\n\nCode\nylim &lt;- c( 0, 1 )\nybrk &lt;- seq( ylim[1], ylim[2], 0.1 )\nylbl &lt;- formatC( ybrk, digits = 2, format = 'f' )\n\nxlim &lt;- c( min( s ), max( s ) )\nxbrk &lt;- seq( xlim[1], xlim[2], length = 11 )\nxlbl &lt;- paste0( formatC( xbrk / 1e6, digits = 1, format = 'f' ), 'M' )\n\nplt &lt;- ggplot() +\n  geom_line( aes( x = s, y = Phi ), colour = 'orange2', linewidth = 2 ) +\n  geom_step( aes( x = s, y = Fms ), colour = 'dodgerblue3',  linewidth = 1 ) +\n  geom_line( aes( x = s, y = Fmax ), colour = 'olivedrab3', linewidth = 0.5 ) +\n  geom_line( aes( x = s, y = Fmin ), colour = 'olivedrab3', linewidth = 0.5 ) +\n  scale_x_continuous( breaks = xbrk,\n                      labels = xlbl, \n                      limits = xlim, \n                      expand = c( 0.01, 0.01 ) ) +\n  scale_y_continuous( breaks = ybrk,\n                      labels = ylbl,\n                      limits = ylim,\n                      expand = c( 0.01, 0.01 ) ) +\n  xlab( TeX( \"$S$\" ) ) + \n  ylab( TeX( \"$F_S$\" ) ) + \n  theme_bw() +\n  theme( legend.position = \"none\" )\nplt\n\n\n\n\n\n\n\n\n\n\nEs de notar que las variables aleatorias de los reclamos pueden seguir cualquier otra distribución, el método no depende de la distribución en cuestión, sino de que la variable que representa los reclamos tenga esperanza finita, varianza finita y la variación de tercer orden \\(\\E\\left[\\left|X - \\E[X]\\right|^3 \\right] &lt; +\\infty\\), también sea finita.\nEn la práctica se suele tomar lo que se conoce como una aproximación por el teorema de límite central.\n\\[\nP( S \\leq s )\n\\approx \\Phi\\left( \\frac{s - \\E[S]}{\\sqrt{\\V[S]}} \\right)\n\\]\nEsto es válido siempre y cuando la cota del error de aproximación \\(\\frac{C}{\\sqrt{n}\\left( 1 + \\left|\\frac{s - \\E[S]}{\\sqrt{\\V[S]}}\\right|^3 \\right)} \\tau\\), sea lo suficientemente pequeña. Además, es de notar que se está trabajando con valores que solo están entre \\(0\\) y \\(1\\), pequeñas variaciones pueden ser muy relevantes al nivel del ajuste de las distribuciones acumuladas.\nFinalmente, debemos tomar en cuenta que las cotas, superior como inferior, no son funciones de de distribución acumuladas en la variable \\(s\\), no necesariamente son siempre crecientes.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Modelos de pérdida agregada</span>"
    ]
  },
  {
    "objectID": "lectura_06.html#deducibles",
    "href": "lectura_06.html#deducibles",
    "title": "7  Modelos de pérdida agregada",
    "section": "7.6 Aplicación del deducible",
    "text": "7.6 Aplicación del deducible\nEn muchas ocasiones según las condiciones de los contratos de seguro y el apetito de riesgo del asegurador, se configura funciones deducibles sobre los reclamos. El objetivo de aplicar un deducible es evitar valores de reclamo en determinados rangos que no están acorde a las configuraciones y estructuración de productos de seguro. Por ejemplo, se puede colocar un deducible para evitar el pago de valores muy pequeños de reclamos, cuya atención incluso puede implicar costos operativos o administrativos mayores al valor mismo de los reclamos, ante esta situación se coloca una cota mínima para que de esta forma el asegurado se haga cargo por su cuenta de estos valores menores.\nEs usual colocar mínimos en las colas de la distribución de reclamos, evitando así valores muy pequeños o valores muy grandes, pero no sería sustentable, ni financieramente o económicamente, el colocar un deducible justo donde se presenta la mayor parte de los valores de reclamos. Desde la perspectiva de un asegurado, ¿De qué serviría adquirir una póliza que no cubre precisamente los siniestros más usuales?, esta no sería una póliza apetecible en el mercado y por otra parte un producto difícil de comercializar.\n\nDefinition 7.4: DeducibleFormalmente, un deducible es una función \\(D : \\R \\longrightarrow \\R\\) a valores reales, tal que para todo valor real \\(x \\in \\R\\), se satisface la siguiente desigualdad: \\(D( x ) \\leq x\\).\n\n\nA continuación presentamos un lista de funciones deducibles más comunes que presenta el mercado:\n\nPara evitar pérdidas pequeñas se suele configurar la función deducible \\[\nD( X ) = \\max( X - d, 0 )\n\\] que se aplicará sobre las variables aleatorias de los reclamos \\(X \\rightsquigarrow F_X\\). Así, el valor ya deducido \\(Z = D( X ) \\rightsquigarrow F_Z( z )\\), donde \\[\nF_Z( z ) = \\mathbf{1}_{[0,+\\infty)}( z ) F_X( z + d )\n\\]\n\n\n\nCode\nD &lt;- function( x, d ) return( max( x - d, 0 ) )\nd &lt;- 50\nalpha &lt;- 2\ngamma &lt;- 4\ntau &lt;- 5\ntheta &lt;- 80\nn &lt;- 1e3\n\nxmax &lt;- 5e2\nx &lt;- seq( 0, xmax, length = n )\nFX &lt;- sapply( x, FUN = function( x ) ptrbeta( x, shape1 = alpha, shape2 = gamma, shape3 = tau, scale = theta ) )\nFDX &lt;- sapply( x, FUN = function( x ) ifelse( x &gt;= d, 1, 0 ) * ptrbeta( x + d, shape1 = alpha, shape2 = gamma, shape3 = tau, scale = theta ) )\n\nplt &lt;- ggplot( ) +\n  geom_line( aes( x = x, y = FX, colour = 'a' ), linewidth = 2 ) +\n  geom_line( aes( x = x, y = FDX, colour = 'b' ), linewidth = 1 ) +\n  scale_colour_manual( breaks = c( 'a', 'b' ), \n                       values = c( 'dodgerblue3', 'red3' ) ) +\n  scale_x_continuous( breaks = seq( 0, xmax, length = 11 ),\n                      labels = formatC( seq( 0, xmax, length = 11 ), digits = 2, format = 'f' ), \n                      limits = c( 0, xmax ), \n                      expand = c( 0, 0 ) ) +\n  scale_y_continuous( breaks = seq( 0, 1, length = 11 ),\n                      labels = formatC( seq( 0, 1, length = 11 ), digits = 2, format = 'f' ), \n                      limits = c( 0.0, 1.0 ), \n                      expand = c( 0.005, 0.005 ) ) +\n  xlab( TeX( \"$x$\" ) ) + \n  ylab( TeX( \"$F_X, F_{D(X)}$\" ) ) + \n  theme_bw() +\n  theme( legend.position = \"none\" )\nplot( plt )\n\n\n\n\n\n\n\n\n\n\nPara prevenir los excesos de pérdida (stop loss) se propone el deducible con un tope máximo dado por una constante \\(M &gt; 0\\), el cual se aplica a las variables aleatorias de los reclamos \\(X \\rightsquigarrow F_X\\): \\[\nD( X ) = \\min( X, M )\n\\],\n\nen este caso, si la variable aleatoria del pago correspondiente al reclamo ya deducido \\(Z = D( X ) \\rightsquigarrow F_Z( z )\\), donde: \\[\nF_Z( z ) = \\mathbf{1}_{(-\\infty,M)}( z ) F_X( z ) + \\mathbf{1}_{[M,+\\infty)}( z )\n\\]\n\n\nCode\nD &lt;- function( x, M ) return( min( x, M ) )\nM &lt;- 150\nalpha &lt;- 2\ngamma &lt;- 4\ntau &lt;- 5\ntheta &lt;- 80\nn &lt;- 1e3\n\nxmax &lt;- 5e2\nx &lt;- seq( 0, xmax, length = n )\nFX &lt;- sapply( x, FUN = function( x ) ptrbeta( x, shape1 = alpha, shape2 = gamma, shape3 = tau, scale = theta ) )\nFDX &lt;- sapply( x, FUN = function( x ) ifelse( x &lt; M, 1, 0 ) * ptrbeta( x, shape1 = alpha, shape2 = gamma, shape3 = tau, scale = theta ) +  ifelse( x &gt;= M, 1, 0 ) )\n\nplt &lt;- ggplot( ) +\n  geom_line( aes( x = x, y = FX, colour = 'a' ), linewidth = 2 ) +\n  geom_line( aes( x = x, y = FDX, colour = 'b' ), linewidth = 1 ) +\n  scale_colour_manual( breaks = c( 'a', 'b' ), \n                       values = c( 'dodgerblue3', 'red3' ) ) +\n  scale_x_continuous( breaks = seq( 0, xmax, length = 11 ),\n                      labels = formatC( seq( 0, xmax, length = 11 ), digits = 2, format = 'f' ), \n                      limits = c( 0, xmax ), \n                      expand = c( 0, 0 ) ) +\n  scale_y_continuous( breaks = seq( 0, 1, length = 11 ),\n                      labels = formatC( seq( 0, 1, length = 11 ), digits = 2, format = 'f' ), \n                      limits = c( 0.0, 1.0 ), \n                      expand = c( 0.005, 0.005 ) ) +\n  xlab( TeX( \"$x$\" ) ) + \n  ylab( TeX( \"$F_X, F_{D(X)}$\" ) ) + \n  theme_bw() +\n  theme( legend.position = \"none\" )\nplot( plt )\n\n\n\n\n\n\n\n\n\n\nCaso mixto para evitar pérdidas pequeñas y exceso de pérdida, se propone la siguiente composición de deducibles que se aplicará sobre las variables aleatorias de los reclamos \\(X \\rightsquigarrow F_X\\): \\[\nD( X ) = \\min( \\max( X - d, 0 ), M )\n\\] en este caso, si la variable aleatoria del pago correspondiente al reclamo ya deducido \\(Z = D( X ) \\rightsquigarrow F_Z( z )\\), tenemos que \\(Z \\rightsquigarrow F_Z\\), con: \\[\nF_Z( z ) = \\mathbf{1}_{[0,M)}( z ) F_X( z + d ) + \\mathbf{1}_{[M,+\\infty)}( z )\n\\]\n\n\n\nCode\nD &lt;- function( x, M ) return( min( max( x - d ), M ) )\nd &lt;- 50\nM &lt;- 150\nalpha &lt;- 2\ngamma &lt;- 4\ntau &lt;- 5\ntheta &lt;- 80\nn &lt;- 1e3\n\nxmax &lt;- 5e2\nx &lt;- seq( 0, xmax, length = n )\nFX &lt;- sapply( x, FUN = function( x ) ptrbeta( x, shape1 = alpha, shape2 = gamma, shape3 = tau, scale = theta ) )\nFDX &lt;- sapply( x, FUN = function( x ) ifelse( x &lt; M & x &gt;= d, 1, 0 ) * ptrbeta( x + d, shape1 = alpha, shape2 = gamma, shape3 = tau, scale = theta ) +  ifelse( x &gt;= M, 1, 0 ) )\n\nplt &lt;- ggplot( ) +\n  geom_line( aes( x = x, y = FX, colour = 'a' ), linewidth = 2 ) +\n  geom_line( aes( x = x, y = FDX, colour = 'b' ), linewidth = 1 ) +\n  scale_colour_manual( breaks = c( 'a', 'b' ), \n                       values = c( 'dodgerblue3', 'red3' ) ) +\n  scale_x_continuous( breaks = seq( 0, xmax, length = 11 ),\n                      labels = formatC( seq( 0, xmax, length = 11 ), digits = 2, format = 'f' ), \n                      limits = c( 0, xmax ), \n                      expand = c( 0, 0 ) ) +\n  scale_y_continuous( breaks = seq( 0, 1, length = 11 ),\n                      labels = formatC( seq( 0, 1, length = 11 ), digits = 2, format = 'f' ), \n                      limits = c( 0.0, 1.0 ), \n                      expand = c( 0.005, 0.005 ) ) +\n  xlab( TeX( \"$x$\" ) ) + \n  ylab( TeX( \"$F_X, F_{D(X)}$\" ) ) + \n  theme_bw() +\n  theme( legend.position = \"none\" )\nplot( plt )\n\n\n\n\n\n\n\n\n\n\nOtro tipo de deducible acota el número de pagos de reclamos a un número fijo máximo de reclamos, es decir, se establece un deducible en función de la frecuencia. Usualmente, esto se realiza cuando se considera un modelo donde se dispone de la frecuencia de reclamos por póliza. \\[\nD(N) = \\min( N, W )\n\\] con la constante \\(W \\in \\N\\). Para el reclamo total, si se considera un portafolio con \\(n\\) pólizas tenemos las siguiente variación del modelo: \\[\nS = \\sum\\limits_{i=1}^n \\sum\\limits_{j=1}^{D(N_i)} X_{i,j}\n\\]\nEn otros casos más elaborados se segmenta el riesgo cubierto, al existir variables explicativas de la naturaleza del riesgo \\(Y_1, \\ldots, Y_m\\), las cuales pueden tomar valores en un conjunto \\(E\\) solo se toma un subconjunto de valores posibles \\(A\\subset E\\), para los cuales es válida la cobertura. Así, si un reclamo \\(X\\) se explica o caracteriza por las variables anteriores, se configura el deducible: \\[\nD( X ) = \\mathbf{1}_{A}\\left( Y_1, \\ldots, Y_m \\right) X\n\\]\n\nUn ejemplo del anterior deducible se da en el caso de seguros de salud, donde se cubre ciertos tipos de enfermedades, usualmente se evita las enfermedades de tipo catastrófico que estén ya presentes; en otros productos se evita los gastos odontológicos, etc.\nLos deducibles, pueden ser configurados de forma genérica para todos los asegurados o incluso de forma personalizada por asegurado, según los productos que estos han adquirido. En general, al emplear deducibles para cada uno de los reclamos individuales, el reclamo total \\(S\\) se ve modificado, y toma la forma: \\[\nS = \\sum\\limits_{i=1}^{N} D_i\\left( X_i \\right)\n\\] Hay casos donde se aplica el deducible al grupo de siniestros que presenta una póliza o grupo de pólizas. Esto precisamente se lo puede ver cuando se utiliza un modelo mixto de pérdida. \\[\nS = \\sum\\limits_{i=1}^n D_i\\left( \\sum\\limits_{n=1}^{N_i} X_{i,j} \\right)\n\\]\nEs de notar, que la distribución acumulada de la variable aleatoria, correspondiente a los reclamos ya deducidos, comienza a presentar singularidades, esto complica el estudio del comportamiento de los reclamos totales.\n\n\n7.6.1 Algoritmo de simulación\nPresentamos una adaptación sencilla de los algoritmos de simulación anterior, incluyendo el caso que se tenga un deducible aplicado de forma uniforme a todas las pólizas.\n\nSeleccionar el número de simulaciones \\(m\\),\nSe genera una muestra de tamaño \\(m\\) de variables \\(N_1, \\ldots, N_m\\) con ley \\(f_N\\),\nSe genera para cada \\(i \\in \\{1,\\ldots,m\\}\\) una muestra de tamaño \\(N_i\\) de variables aleatorias \\(X_{i,1}, \\ldots X_{i,N_i}\\) con ley \\(f_X\\),\nSe establece la función deducible \\(D : \\R \\longrightarrow \\R_+\\) y los parámetros y condiciones necesarias para su definición,\nSe calcula los reclamos totales utilizando el deducible \\(S_1, \\ldots, S_m\\) para cada simulación \\(i = \\{1, \\ldots, m\\}\\), mediante la siguiente suma \\(S_i = \\sum\\limits_{j=1}^{N_i} D\\left( X_{i,j} \\right)\\),\nPara la determinación de varios estadísticos se puede utilizar la simulación generada \\(\\{S_i\\}\\) y generar la distribución de probabilidad empírica \\(F_m\\).\n\n\nConsideremos el caso de un modelo colectivo de pérdida donde el proceso de conteo \\(N \\rightsquigarrow Pois( \\lambda )\\) y la sucesión \\(\\{X_i\\}\\) de reclamos individuales se considera conformada por variables i.i.d, con distribución de probabilidad \\(LN( \\mu, \\sigma )\\). El deducible a aplicar será una función mixta de la forma \\(D( X ) = \\min( \\max( X - d, 0 ), M )\\).\n\n\nCode\n# 1. selección de simulaciones\nm &lt;- 5e4\n\n# 2. especificación de los parámetros para las distribuciones\nu &lt;- 2\ns &lt;- 3\nlambda &lt;- 6\n\n# 3. se especifica la función deducible\nD &lt;- function( x, d, M ) {\n  return( min( max( x - d, 0 ), M ) )\n}\n\nset.seed( 312341 )\n\n# 4. simulación del conteo de siniestros\nN &lt;- rpois( n = m, lambda = lambda )\n\n# 5. simulación de la severidad de los reclamos\nX &lt;- lapply( N, FUN = function( n ) rlnorm( n, meanlog = u, sdlog = s ) )\n\n# 6. se aplica el deducible a los reclamos\nd &lt;- 100\nM &lt;- 1000\nDX &lt;- lapply( X, FUN = function( x ) sapply( x, FUN = function( y ) D( y, d, M ) ) )\n\n# 7. reclamo total, agregación por cada simulación\nS &lt;- sapply( X, FUN = function( x ) ifelse( length( x ) == 0, 0, sum( x, na.rm = TRUE ) ) )\nDS &lt;- sapply( DX, FUN = function( x ) ifelse( length( x ) == 0, 0, sum( x, na.rm = TRUE ) ) )\n\n\nEl efecto del deducible sobre la distribución del reclamo es evidente, produciendo así una densidad multimodal, como se observa en el gráfico a continuación, en el mismo incluimos dos histogramas, uno correspondiente a los reclamos totales sin aplicar el deducible y el otro el histograma de los reclamos totales aplicados el deducible.\n\n\nCode\nsmax &lt;- lambda * M\nfmax &lt;- 0.002\n\nplt &lt;- ggplot( ) +\n  geom_histogram( aes( S, after_stat( density ) ), fill = 'olivedrab3', colour = 'olivedrab4', alpha = 0.3, bins = nclass.FD( DS ) ) +\n  geom_histogram( aes( DS, after_stat( density ) ), fill = 'dodgerblue3', colour = 'dodgerblue4', alpha = 0.6, bins = nclass.FD( DS ) ) +\n  scale_x_continuous( breaks = seq( 0, smax, length = 7 ),\n                      labels = formatC( seq( 0, smax, length = 7 ), digits = 2, format = 'f' ),\n                      limits = c( 0, smax ),\n                      expand = c( 0, 0 ) ) +\n  scale_y_continuous( breaks = seq( 0, fmax, length = 11 ),\n                      labels = formatC( seq( 0, fmax, length = 11 ), digits = 4, format = 'f' ),\n                      limits = c( 0, fmax ),\n                      expand = c( 0, 0 ) ) +\n  xlab( TeX( \"$s$\" ) ) + \n  ylab( TeX( \"$f_S$\" ) ) + \n  theme_bw() +\n  theme( legend.position = \"none\" )\nplot( plt )\n\n\n\n\n\n\n\n\n\nEs de notar que la probabilidad del evento \\(\\{S=0\\}\\) no es nula y se debe a la aplicación del deducible. \\[\n\\begin{split}\nP( S = 0 )\n& = P\\left( \\left\\{ S = 0 \\middle| N \\in \\N \\right\\} \\right) \\\\\n& = P\\left( \\bigcup\\limits_{n=0}^{+\\infty} \\left\\{ S = 0 \\land N = n \\right\\} \\right) \\\\\n& = P\\left( \\bigcup\\limits_{n=0}^{+\\infty} \\left\\{ D( X_1 ) + \\cdots + D( X_N ) = 0 \\land N = n \\right\\} \\right) \\\\\n& = P\\left( \\bigcup\\limits_{n=0}^{+\\infty} \\left\\{ X_1 \\leq d \\land \\cdots \\land X_N \\leq d \\land N = n \\right\\} \\right) \\\\\n& = \\sum\\limits_{n=0}^{+\\infty} P\\left( X_1 \\leq d \\land \\cdots \\land X_N \\leq d \\land N = n \\right) \\\\\n& = \\sum\\limits_{n=0}^{+\\infty} P\\left( X_1 \\leq d \\land \\cdots \\land X_n \\leq d \\middle| N = n \\right)P( N = n ) \\\\\n& = \\sum\\limits_{n=0}^{+\\infty} P\\left( X_1 \\leq d \\land \\cdots \\land X_n \\leq d \\right) P( N = n ) \\\\\n& = \\sum\\limits_{n=0}^{+\\infty} P\\left( X \\leq d \\right)^n p_n \\\\\n& = \\E\\left[ P\\left( X \\leq d \\right)^N \\right] \\\\\n& \\approx \\sum\\limits_{n=0}^{n'} P( X \\leq d )^n p_n,\\quad\n\\text{para un valor $n'$ suficientemente grande}\n\\end{split}\n\\]\n\n\nCode\nnp &lt;- 1e3\nns &lt;- 0:np\nPS0 &lt;- sum( ( plnorm( d, meanlog = u, sdlog = s )^ns ) * dpois( ns, lambda = lambda ) )\n\n\n\\[\nP( S = 0 ) = 0.314886664\n\\]\nAsí mismo, se puede estudiar el evento extremo, caso en el cual se paga el valor máximo que admite el deducible, en este caso en particular es \\(M\\) por cada siniestro y si se producen \\(n\\), entonces como máximo se pagara \\(n M\\). Por tanto, nos interesa la probabilidad del evento \\(\\{ S\\ \\text{es máximo} \\}\\), el cual lo podemos calcular como: \\[\n\\begin{split}\nP( S\\ \\text{es máximo} )\n& = P\\left( \\left\\{ S = NM \\middle| N \\in \\N \\right\\} \\right) \\\\\n& = P\\left( \\bigcup\\limits_{n=0}^{+\\infty} \\left\\{ S = N M \\land N = n \\right\\} \\right) \\\\\n& = P\\left( \\bigcup\\limits_{n=0}^{+\\infty} \\left\\{ D( X_1 ) + \\cdots + D( X_N ) = N M \\land N = n \\right\\} \\right) \\\\\n& = P\\left( \\bigcup\\limits_{n=0}^{+\\infty} \\left\\{ X_1 - d &gt; M \\land \\cdots \\land X_N - d  \\geq M \\land N = n \\right\\} \\right) \\\\\n& = P\\left( \\bigcup\\limits_{n=0}^{+\\infty} \\left\\{ X_1 &gt; M + d \\land \\cdots \\land X_N \\geq M + d \\land N = n \\right\\} \\right) \\\\\n& = \\sum\\limits_{n=0}^{+\\infty} P\\left( X_1 &gt; M + d \\land \\cdots \\land X_N &gt; M + d \\land N = n \\right) \\\\\n& = \\sum\\limits_{n=0}^{+\\infty} P\\left( X_1 &gt; M + d \\land \\cdots \\land X_n &gt; M + d \\middle| N = n \\right) P( N = n ) \\\\\n& = \\sum\\limits_{n=0}^{+\\infty} P\\left( X_1 &gt; M + d \\land \\cdots \\land X_n &gt; M + d \\right) P( N = n ) \\\\\n& = \\sum\\limits_{n=0}^{+\\infty} P\\left( X &gt; M + d \\right)^n p_n \\\\\n& = \\E\\left[ P\\left( X &gt; M + d \\right)^N \\right] \\\\\n& \\approx \\sum\\limits_{n=0}^{n'} P( X &gt; M + d )^n p_n,\\quad\n\\text{para un valor $n'$ suficientemente grande}\n\\end{split}\n\\]\n\n\nCode\nPSmax &lt;- sum( ( ( 1 - plnorm( M + d, meanlog = u, sdlog = s ) )^ns ) * dpois( ns, lambda = lambda ) )\n\n\n\\[\nP( S\\ \\text{es máximo} ) = 0.003299886\n\\] Además, podemos calcular el valor máximo esperado de \\(S\\): \\[\n\\begin{split}\n\\E\\left[ S \\middle| S\\ \\text{es máximo} \\right]\n& = \\E\\left[ S \\middle| \\left\\{ S = NM \\middle| N \\in \\N \\right\\} \\right] \\\\\n& = \\E\\left[ S \\middle| \\bigcup\\limits_{n=0}^{+\\infty} \\left\\{ S = NM \\land N = n \\right\\} \\right] \\\\\n& = \\E\\left[ NM \\middle| \\bigcup\\limits_{n=0}^{+\\infty} \\left\\{ N = n \\right\\} \\right] \\\\\n& = \\sum\\limits_{n=0}^{+\\infty} \\E\\left[ n M \\middle| N = n \\right] P( N = n ) \\\\\n& = \\sum\\limits_{n=0}^{+\\infty} n M P( N = n ) \\\\\n& = M \\sum\\limits_{n=0}^{+\\infty} n P( N = n ) \\\\\n& = M \\E\\left[ N \\right] \\\\\n\\end{split}\n\\]\n\n\nCode\nESmax &lt;- M * sum( ns * dpois( ns, lambda = lambda ) )\n\n\n\\[\n\\E\\left[ S \\middle| S\\ \\text{es máximo} \\right] = 6000.00\n\\]\nDe este razonamiento se puede evidenciar que al aplicar un deducible con cota superior, se corta la cola de la distribución y por tal razón no se puede esperar valores extremos por parte de la severidad, sino tan solo por un aumento de frecuencia, e incluso el valor máximo esperado es proporcional a la cota superior \\(M\\) veces la frecuencia esperada. De ahí que, al aplicar un deducible con cota superior no se puede esperar un encarecimiento de primas por aumento de severidad, ya que los reclamos están acotados, así como el máximo esperado; lo único que puede encarecer la prima es un aumento de frecuencia.\nComo ya se lo mencionó, utilizando la simulación \\(\\{S_i\\}\\) podemos determinar la distribución de probabilidad empírica \\(F_m\\) que aproxima la distribución de \\(F_S\\).\n\n\nCode\nFeS &lt;- ecdf( S )\nFeDS &lt;- ecdf( DS )\ns &lt;- seq( 0, ESmax, length = 1000 )\nFes &lt;- sapply( s, FUN = FeS )\nFeDs &lt;- sapply( s, FUN = FeDS )\n\n\n\n\nCode\nSmax &lt;- ESmax\n\nxbrk &lt;- sort( unique( c( d, M, seq( 0, Smax, length = 7 ) ) ) )\nxlbl &lt;- formatC( xbrk, digits = 0, format = 'f' )\nxlim &lt;- c( 0, Smax )\n\nybrk &lt;- seq( 0, 1, length = 11 )\nylbl &lt;- formatC( ybrk, digits = 2, format = 'f' )\nylim &lt;- c( 0, 1 )\n\nplt &lt;- ggplot( ) +\n  geom_step( aes( x = s, y = Fes, colour = 'a' ), linewidth = 1 ) +\n  geom_step( aes( x = s, y = FeDs, colour = 'b' ), linewidth = 1 ) +\n  geom_vline( xintercept = c( d, M ), colour = 'orange', linewidth = 0.7 ) +\n  geom_point( aes( 0, PS0 ), colour = 'red3', size = 3 ) +\n  scale_colour_manual( breaks = c( 'a', 'b' ),\n                       values = c( 'olivedrab4', 'dodgerblue4' ) ) +\n  scale_x_continuous( breaks = xbrk,\n                      labels = xlbl, \n                      limits = xlim, \n                      expand = c( 0.008, 0.008 ) ) +\n  scale_y_continuous( breaks = ybrk,\n                      labels = ylbl, \n                      limits = ylim, \n                      expand = c( 0.005, 0.005 ) ) +\n  xlab( TeX( \"$s$\" ) ) + \n  ylab( TeX( \"$F_{S,m},F_{S^D,m}$\" ) ) + \n  theme_bw() +\n  theme( legend.position = \"none\" )\nplot( plt )",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Modelos de pérdida agregada</span>"
    ]
  },
  {
    "objectID": "lectura_06.html#algoritmo_panjer",
    "href": "lectura_06.html#algoritmo_panjer",
    "title": "7  Modelos de pérdida agregada",
    "section": "7.7 Algoritmo de Panjer",
    "text": "7.7 Algoritmo de Panjer\nEl siguiente algoritmo es de especial interés para determinar la densidad de probabilidad \\(f_S\\) del total de reclamos \\(S\\). Se asume que el conteo de siniestro \\(N\\) viene dado por alguna de las distribuciones en la familia \\((a,b,0)\\) y los reclamos individuales son \\(\\{X_i\\}\\) son i.i.d y también independientes de \\(N\\). En ese contexto podemos observar que para cualquier \\(s &gt; 0\\), tenemos: \\[\n\\begin{split}\nf_S( s )\n& = \\sum\\limits_{n=1}^{+\\infty} p_n f_X^{\\star n}( s ) \\\\\n& = p_1 f_X( s ) + \\sum\\limits_{n=2}^{+\\infty} p_n f_X^{\\star n}( s ) \\\\\n& = p_1 f_X( s ) + \\sum\\limits_{n=1}^{+\\infty} p_{n+1} f_X^{\\star n + 1}( s ) \\\\\n& = p_1 f_X( s ) + \\sum\\limits_{n=1}^{+\\infty} \\left( a + \\frac{b}{n+1} \\right) p_{n} f_X^{\\star n + 1}( s ) \\\\\n& = p_1 f_X( s )\n+ \\sum\\limits_{n=1}^{+\\infty} p_{n} \\int\\limits_0^s \\left( a + \\frac{b y}{s} \\right) f_X( y ) f_X^{\\star n}( s - y )\\ dy \\\\\n& = p_1 f_X( s )\n+ \\int\\limits_0^s \\left( a + \\frac{b y}{s} \\right) f_X( y ) \\sum\\limits_{n=1}^{+\\infty} p_{n} f_X^{\\star n}( s - y )\\ dy \\\\\n& = p_1 f_X( s ) + \\int\\limits_0^s \\left( a + \\frac{b y}{s} \\right) f_X( y ) f_S( s - y )\\ dy\n\\end{split}\n\\] cuando \\(s = 0\\), la probabilidad se concentra en \\(0\\), y por tanto \\(f_S( 0 ) = p_0\\)\nComo consecuencia del análisis anterior resulta la relación recurrente, la cual es precisamente explotada de forma numérica por el algoritmo de Panjer. \\[\nf_S( s ) = p_1 f_X( s ) + \\int\\limits_0^s \\left( a + \\frac{b y}{s} \\right) f_X( y ) f_S( s - y )\\ dy,\n\\qquad \\forall s &gt; 0\n\\]\nSi se toma una discretización \\(0 = s_0 &lt; s_1 &lt; \\cdots &lt; s_n\\) para aproximar la integral anterior \\[\nf_S\\left( s_k \\right)\n= p_1 f_X\\left( s_k \\right) + \\sum\\limits_{i=0}^{k-1} \\left( a + \\frac{b s_i}{s_k} \\right)\nf_X\\left( s_i \\right) f_S\\left( s_{k-i} \\right) \\left( s_{i+1} - s_i \\right)\n\\]\nAl tomar un división uniforme para la malla utilizada para aproximar la integral, por ejemplo, \\(s_k = k h\\), tenemos que: \\[\nf_S\\left( s_k \\right)\n= p_1 f_X\\left( s_k \\right) + h \\sum\\limits_{i=0}^{k-1} \\left( a + \\frac{b i}{k} \\right)\nf_X\\left( s_i \\right) f_S\\left( s_{k-i} \\right)\n\\] de donde resulta una forma bien conocida para el algoritmo de Panjer. sin embargo, es importante tener en cuenta que una malla de paso no constante puede ser mejor adaptada para realizar la integral.\n\nConsideremos el caso para un reclamo total \\(S\\) donde el conteo de los siniestros \\(N \\rightsquigarrow NBin( \\alpha, p )\\) y las severidades \\(\\{X_i\\}\\) son i.i.d con distribución \\(LN( \\mu, \\sigma )\\). Utilizaremos el algoritmo de Panjer para aproximar la densidad de probabilidad de \\(f_S\\) de \\(S\\).\n\n\nCode\nn &lt;- 7e4\np &lt;- 0.3\nalpha &lt;- 3\na &lt;- 1 - p\nb &lt;- ( 1 - p ) * ( alpha - 1 )\np0 &lt;- p^alpha\nu &lt;- 5\ns &lt;- 0.3\nsmax &lt;- 5e3\ndist &lt;- dlnorm\n\npardist &lt;- list( meanlog = u, sdlog = s )\nsg &lt;- seq( 0, smax, length = n )\nds &lt;- diff( sg )\nfX &lt;- sapply( sg, FUN = function( y ) do.call( dist, c( y, pardist ) ) )\n\nfS &lt;- p0\nK &lt;- 1\np1 &lt;- ( a + b ) * p0\nfor ( k in 2:n ) {\n  fS &lt;- c( fS, p1 * fX[ k ] + sum( ( a + b * sg[ K ] / sg[ k ] ) * fX[ K ] * fS[ rev( K ) ] * ds[ K ] ) )\n  K &lt;- c( K, k )\n}\n\nset.seed( 32141223 )\nm &lt;- 1e5\nN &lt;- rnbinom( n = m, size = alpha, prob = p )\nX &lt;- lapply( N, FUN = function( n ) rlnorm( n, meanlog = u, sdlog = s ) )\nS &lt;- sapply( X, FUN = function( x ) sum( x, na.rm = TRUE ) )\n\n\nEl algoritmo de Panjer suele tener problemas para ajustarse en los primeros valores de la densidad \\(f_S\\), tal como lo muestra el gráfico a continuación, usualmente hay una sobre estimación. Además, el algoritmo solo es válido cuando el conteo de siniestros pertenece a la familia de distribuciones \\((a,b,0)\\).\n\n\nCode\nfmax &lt;- 0.0007\n\nplt &lt;- ggplot( ) +\n  geom_histogram( aes( S, after_stat( density ) ), fill = 'dodgerblue4', colour = 'white', alpha = 0.6, bins = nclass.FD( S ) ) +\n  geom_line( aes( x = sg, y = fS ), colour = 'purple3', linewidth = 1 ) +\n  scale_x_continuous( breaks = seq( 0, smax, length = 7 ),\n                      labels = formatC( seq( 0, smax, length = 7 ), digits = 2, format = 'f' ), \n                      limits = c( 0, smax ), \n                      expand = c( 0, 0 ) ) +\n  scale_y_continuous( breaks = seq( 0, fmax, length = 11 ),\n                      labels = formatC( seq( 0, fmax, length = 11 ), digits = 4, format = 'f' ),\n                      limits = c( 0, fmax ),\n                      expand = c( 0, 0 ) ) +\n  xlab( TeX( \"$s$\" ) ) + \n  ylab( TeX( \"$f_S$\" ) ) + \n  theme_bw() +\n  theme( legend.position = \"none\" )\nplot( plt )\n\n\n\n\n\n\n\n\n\n\n\nCode\ne &lt;- cumsum( fS[ -1 ] * ds ) - ecdf( S )( sg[ -1 ] )\n\neb &lt;- max( abs( e ) )\n\nplt &lt;- ggplot() +\n  geom_histogram( aes( x = e, y = after_stat( density ) ), fill = 'grey50', colour = 'dodgerblue3', bins = nclass.scott( e ) ) +\n  scale_y_continuous( labels = label_scientific( digits = 2 ) ) +\n  ylab( TeX( \"$e$\" ) ) +\n  theme_bw()\nplot( plt )",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Modelos de pérdida agregada</span>"
    ]
  },
  {
    "objectID": "lectura_06.html#estimacion_transformada_fourier",
    "href": "lectura_06.html#estimacion_transformada_fourier",
    "title": "7  Modelos de pérdida agregada",
    "section": "7.8 Estimación usando la transformada de Fourier",
    "text": "7.8 Estimación usando la transformada de Fourier\nSe puede utilizar la transformada de Fourier para determinar la densidad de la suma \\(S_n = \\sum\\limits_{i=1}^n X_i\\) de varias variables aleatorias \\(X_1 \\rightsquigarrow f_{X_1}, \\ldots, X_n \\rightsquigarrow f_{X_n}\\) que son independientes, pero que podrían ser o no idénticamente distribuidas. Claramente, la densidad de probabilidad de \\(S\\) está dada por la convolución de las densidades de probabilidad de cada una de las variables aleatorias \\(X_1, \\ldots, X_n\\). \\[\nf_S = f_{X_1} \\star \\cdots \\star f_{X_n}\n\\] sin embargo esta convolución implica el realizar una interacción en \\(n\\)-dimensiones.\nSi las las variables aleatorias \\(\\{X_i\\}\\) son independientes sabemos además que \\[\n\\Fo\\left( f_S \\right) = \\prod\\limits_{i=1}^n \\Fo\\left( f_{X_i} \\right)\n\\]\nPor otra parte, con lo anterior sabemos que es posible aproximar numéricamente cada \\(\\Fo\\left( f_{X_i} \\right)\\) con una serie \\(\\{\\hat{f}_{i,j}\\}\\) dada por la discretización de la transformada de Fourier y su aplicación sobre la discretización de la densidad de probabilidad \\(f_{i,k} = f_{X_i}( s_k )\\). Por tanto, para cada de las densidades con \\(i \\in \\{1, \\ldots, n\\}\\) y \\(\\omega_j = \\frac{j}{b-a}\\) con \\(j \\in \\{0,\\ldots, N-1\\}\\), se puede calcular de forma separada las aproximaciones a cada una de las transformadas \\[\n\\Fo( f_{X_i} )\\left( \\omega_j \\right)  \n\\approx \\hat{f}_{i,j}\n= h \\exp\\left( -2\\pi i \\omega_j a \\right) \\left( \\DFT\\left[ \\{f_{i,k}\\} \\right]  \\right)_j\n\\]\ncon lo anterior, también, se puede realizar una aproximación a la trasformada de Fourier \\(\\Fo\\left( f_S \\right)\\) de la densidad de probabilidad que buscamos \\(f_S\\), como el producto de sus transformadas de Fourier. \\[\n\\Fo( f_{S} )\\left( \\omega_j \\right)  \n= \\prod\\limits_{i=1}^n \\Fo\\left( f_{X_i} \\right)\\left( \\omega_j \\right)\n\\approx \\prod\\limits_{i=1}^n \\hat{f}_{i,j}\n\\]\nutilizando la inversión de la transformada de Fourier discreta podemos calcular una serie que precisamente aproxima a la densidad \\(f_S\\) \\[\n\\{ f_{S}( s_k ) \\}\n\\approx \\Re\\left( \\DFT^{-1}\\left[ \\left\\{ \\frac{1}{h} \\exp\\left( 2\\pi i \\omega_j a \\right)\n\\prod\\limits_{i=1}^n \\hat{f}_{i,j} \\right\\} \\right] \\right)\n\\]\n\n\nCode\nN &lt;- 50000\nset.seed(94312)\nalpha &lt;- sample( x = seq( 1, 20, length = 40 ), size = 50, replace = TRUE )\ntheta &lt;- 3\nb &lt;- sum( sapply( alpha, FUN = function( a ) qgamma( 0.99, shape = a, scale = theta ) ) )\na &lt;- 0\nh &lt;- ( b - a ) / N\nn &lt;- 0:N\ns &lt;- a + n * h\nw &lt;- n / ( b - a )\neta &lt;- h * exp( -2 * pi * 1i * w * a )\nfX &lt;- lapply( alpha, function( a ) sapply( s, FUN = function( sk ) dgamma( sk, shape = a, scale = theta ) ) )\nFof &lt;- lapply( fX, FUN = function( fi ) eta * fft( fi ) )\nFoS &lt;- Fof[[ 1 ]]\nfor ( i in 2:length( fX ) ) {\n  FoS &lt;- FoS * Fof[[ i ]]\n}\nfS &lt;- fft( eta^(-1) * FoS, inverse = TRUE ) / ( N + 1 )\nfS &lt;- Re( fS )\nFS &lt;- cumsum( fS ) * h\nfs &lt;- sapply( s, FUN = function( sk ) dgamma( sk, shape = sum( alpha ), scale = theta ) )\nFs &lt;- sapply( s, FUN = function( sk ) pgamma( sk, shape = sum( alpha ), scale = theta ) )\n\n\n\n\nCode\nplt &lt;- ggplot() +\n  geom_line( aes( x = s, y = fs ), colour = 'darkred', linewidth = 1 ) + \n  geom_point( aes( x = s, y = fS ), colour = 'olivedrab3', size = 0.2 ) + \n  xlab( TeX( \"$x$\" ) ) + \n  ylab( TeX( \"$f_{S}(x)$\" ) ) + \n  theme_bw()\nplot( plt )\n\n\n\n\n\n\n\n\n\nA pesar que la aproximación a la densidad de probabilidad parece ser buena, la condición de normalización, no es satisfecha, como muestra el siguiente ejemplo. Ello sugiere que al algoritmo de estimación de la densidad hay que realizarle algún ajuste o en su defecto incrementar la precisión numérica.\n\n\nCode\nplt &lt;- ggplot() +\n  geom_line( aes( x = s, y = Fs ), colour = 'darkred', linewidth = 1 ) + \n  geom_point( aes( x = s, y = FS ), colour = 'olivedrab3', size = 0.2 ) + \n  geom_hline( yintercept = 1, color = 'dodgerblue2', linewidth = 0.1 ) +\n  xlab( TeX( \"$x$\" ) ) + \n  ylab( TeX( \"$F_{S}(x)$\" ) ) + \n  theme_bw()\nplot( plt )\n\n\n\n\n\n\n\n\n\n\n\nCode\ne &lt;- fS - fs\neb &lt;- max( abs( e ) )\n\nplt &lt;- ggplot() +\n  geom_histogram( aes( x = e, y = after_stat( density ) ), fill = 'grey50', colour = 'dodgerblue3', bins = nclass.scott( e ) ) + \n  scale_y_continuous( labels = label_scientific( digits = 4 ) ) +\n  ylab( TeX( \"$e$\" ) ) + \n  theme_bw()\nplot( plt )\n\n\n\n\n\n\n\n\n\nUtilizando la misma idea anterior, se puede aproximar cada uno de los términos presentes en la serie que determina la densidad de probabilidad \\(f_S\\) de un modelo colectivo. Se selecciona un número máximo \\(M \\in \\N\\) de términos a ser considerados en la serie, \\(M\\) lo suficientemente grande como para que la siguiente aproximación resulte adecuada. \\[\n\\{ f_{S}( s_k ) \\}\n\\approx  \\Re\\left( \\DFT^{-1}\\left[ \\sum\\limits_{n=1}^{M} \\left\\{ \\frac{1}{h} \\exp\\left( 2\\pi i \\omega_j a \\right)\n\\prod\\limits_{i=1}^n \\hat{f}_{i,j} \\right\\}\\ p_n \\right] \\right),\n\\qquad \\forall s_k &gt; 0\n\\]\npara el caso cuando \\(s_o = 0\\), tenemos que \\(f_S( s_0 ) = f_S( 0 ) = p_0\\).\n\n\nCode\nM &lt;- 200\nN &lt;- 7e4\n\np &lt;- 0.3\nalpha &lt;- 3\npn &lt;- dnbinom( 0:M, size = alpha, prob = p )\n\nu &lt;- 5\ns &lt;- 0.3\n\nb &lt;- 5e3\na &lt;- 0\nh &lt;- ( b - a ) / N\nn &lt;- 0:N\nsg &lt;- a + n * h\nw &lt;- n / ( b - a )\neta &lt;- h * exp( -2 * pi * 1i * w * a )\nf &lt;- sapply( sg, FUN = function( sk ) dlnorm( sk, meanlog = u, sdlog = s ) )\nFof &lt;- eta * fft( f )\nFoS &lt;- rep( 0, N + 1 )\nfor ( n in 1:M ) {\n  FoS &lt;- FoS + pn[ n + 1 ] * Fof^n\n}\nfS &lt;- fft( eta^(-1) * FoS, inverse = TRUE ) / ( N + 1 )\nfS &lt;- Re( fS )\nfS[1] = pn[1]\n\nset.seed( 32141223 )\nm &lt;- 1e5\nN &lt;- rnbinom( n = m, size = alpha, prob = p )\nX &lt;- lapply( N, FUN = function( n ) rlnorm( n, meanlog = u, sdlog = s ) )\nS &lt;- sapply( X, FUN = function( x ) sum( x, na.rm = TRUE ) )\n\n\n\n\nCode\nfmax &lt;- 0.0007\nsmax &lt;- 5e3\n\nplt &lt;- ggplot() +\n  geom_histogram( aes( S, after_stat( density ) ), fill = 'dodgerblue4', colour = 'white', alpha = 0.6, bins = nclass.FD( S ) ) +\n  geom_line( aes( x = sg, y = fS ), colour = 'purple3', linewidth = 1 ) +\n  scale_x_continuous( breaks = seq( 0, b, length = 7 ),\n                      labels = formatC( seq( 0, smax, length = 7 ), digits = 2, format = 'f' ), \n                      limits = c( 0, smax ), \n                      expand = c( 0, 0 ) ) +\n  scale_y_continuous( breaks = seq( 0, fmax, length = 11 ),\n                      labels = formatC( seq( 0, fmax, length = 11 ), digits = 4, format = 'f' ),\n                      limits = c( 0, fmax ),\n                      expand = c( 0, 0 ) ) +\n  xlab( TeX( \"$s$\" ) ) + \n  ylab( TeX( \"$f_S$\" ) ) + \n  theme_bw() +\n  theme( legend.position = \"none\" )\nplot( plt )\n\n\n\n\n\n\n\n\n\n\n\nCode\ne &lt;- cumsum( fS[ -1 ] * diff( sg ) ) - ecdf( S )( sg[-1] )\n\neb &lt;- max( abs( e ) )\n\nplt &lt;- ggplot() +\n  geom_histogram( aes( x = e, y = after_stat( density ) ), fill = 'grey50', colour = 'dodgerblue3', bins = nclass.FD( e ) ) +\n  scale_y_continuous( labels = label_scientific( digits = 2 ) ) +\n  ylab( TeX( \"$e$\" ) ) +\n  theme_bw()\nplot( plt )",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Modelos de pérdida agregada</span>"
    ]
  },
  {
    "objectID": "lectura_06.html#temporalidad_numero_reclamos",
    "href": "lectura_06.html#temporalidad_numero_reclamos",
    "title": "7  Modelos de pérdida agregada",
    "section": "7.9 Temporalidad y ajustes del número de reclamos",
    "text": "7.9 Temporalidad y ajustes del número de reclamos\nHasta el momento no hemos tomado en cuenta cierta componente de temporalidad que muestra la frecuencia de los reclamos. Debemos ser muy consientes que los reclamos se producen en el tiempo y que el número de estos dependerá del periodo de exposición de cada uno de los asegurados, en otras palabras en la estimación de la frecuencia se debe tomar en cuenta que las pólizas tienen un periodo de cobertura, una fecha inicial y una fecha final.\nMás puntualmente, para una inferencia sencilla si estimamos reclamos de pólizas que duran 2 años entonces la frecuencia del número de reclamos \\(N\\) será bianual. Si por el contrario se quiere utilizar esta frecuencia para estimar el número de reclamos para pólizas con cobertura anual entonces, es necesario realizar un ajuste acorde.\n\nDefinition 7.5: Exposición al riesgoDando algo más de sentido matemático a este razonamiento, a cada póliza podemos asociar su respectiva exposición al riesgo, que no es más que el ancho de intervalo dado por la fecha inicial de la póliza, menos la fecha final de la póliza. Usualmente la unidad que su utiliza para medir este intervalo es en años, pero puede cambiársela, siempre y cuando la formulación del modelo sea clara y el uso de estas unidades sea consistente para cálculos futuros. Así si tenemos \\(n\\) pólizas para cada una de ellas hay un tiempo de inicio \\(s_i\\) y un tiempo \\(e_i\\) final, para todo \\(i \\in \\{1, \\ldots, n\\}\\), así la exposición al riesgo \\(ER_i\\) está dada por una distancia \\(d\\) \\[\nER_i = d( s_i, e_i )\n\\]\n\n\nPor ejemplo, una buena distancia puede ser \\[\nd( s, e ) = \\frac{\\# \\left\\{ \\text{días entre $s$ y $e$} \\right\\}}{365.25}\n\\] En la librería lubridate de R, tenemos una implementación que puede utilizarse fácilmente para calcular la distancia en años entre dos fechas dadas.\n\n\nCode\ns &lt;- ymd( '1951-04-09' )\ne &lt;- ymd( '2025-04-09' )\nd1 &lt;- interval( s, e ) / dyears( 1 )\nd2 &lt;- length( seq( s, e, by = 'day') ) / 365.25\n\n\nLa idea es ajustar la estimación de la frecuencia de forma proporcional a la exposición al riesgo, esto se puede realizar postulando que la variable aleatoria del conteo de siniestros, tienen una esperanza proporcional a la exposición al riesgo. \\[\n\\E\\left[ N \\right] = \\lambda ER\n\\] además se puede ajustar la distribución de probabilidad que se desee estimar en función del parámetro \\(\\lambda\\), es así que para las distribuciones de probabilidad discretas que hemos introducido con anterioridad, tenemos la siguiente modificación.\n\nPara la distribución de Poisson, tenemos la ley de probabilidad \\[\nP( N = k ) = \\exp\\left( -\\lambda ER \\right) \\frac{( \\lambda ER )^k}{k!}, \\qquad\n\\forall k \\in \\N\n\\] entonces \\(N \\rightsquigarrow Pois( \\lambda ER )\\).\n\nEn caso de disponer de una muestra de \\(N_1, \\ldots, N_m\\) de variables aleatorias independientes, y por otra parte exposiciones al riesgo \\(ER_1, \\ldots, ER_m\\) asociadas a cada una de las variables aleatorias, de tal forma que \\(N_i \\rightsquigarrow Pois( \\lambda ER_i )\\), entonces la función de verosimilitud logarítmica toma la forma: \\[\n\\ell( \\lambda )\n= - \\lambda \\sum\\limits_{i=1}^m ER_i\n+ \\sum\\limits_{i=1}^m N_i \\log\\left( ER_i \\right)\n+ \\sum\\limits_{i=1}^m N_i \\log\\left( \\lambda \\right)\n- \\sum\\limits_{i=1}^m \\log\\left( N_i! \\right)\n\\] en este caso el estimador por maximización de verosimilitud es bien sencillo de determinar y está dado por: \\[\n\\lambda = \\frac{\\sum\\limits_{i=1}^m N_i}{\\sum\\limits_{i=1}^m ER_i }\n\\]\n\nPara la distribución binomial begativa, tenemos que si \\[\n\\lambda ER = \\alpha \\frac{1-p}{p} \\Rightarrow p = \\frac{\\alpha}{\\lambda ER + \\alpha}\n\\] de donde la ley de probabilidad queda modificada como \\[\nP( N = k )\n= \\frac{\\Gamma( \\alpha + k )}{\\Gamma(k+1) \\Gamma(\\alpha)}\n\\left( \\frac{\\alpha}{\\lambda ER + \\alpha} \\right)^\\alpha \\left( \\frac{\\lambda ER}{\\lambda ER + \\alpha} \\right)^k, \\qquad\n\\forall k \\in \\N\n\\] entonces \\(N \\rightsquigarrow NBinom\\left( \\alpha, \\frac{\\alpha}{\\lambda ER + \\alpha} \\right)\\).\n\nSimilar que antes, si disponemos de una muestra de \\(N_1, \\ldots, N_m\\) de variables aleatorias independientes y también exposiciones al riesgo \\(ER_1, \\ldots, ER_m\\) asociadas a cada una de las variables aleatorias, de tal forma que \\(N_i \\rightsquigarrow NBinom\\left( \\alpha, \\frac{\\alpha}{\\lambda ER_i + \\alpha} \\right)\\), entonces la función de verosimilitud logarítmica toma la forma: \\[\n\\begin{split}\n\\ell( \\alpha, \\lambda )\n& = \\sum\\limits_{i=1}^m \\log \\Gamma( \\alpha + N_i )\n- \\sum\\limits_{i=1}^m \\log \\Gamma( N_i + 1 )\n- m \\log \\Gamma( \\alpha ) \\\\\n& + \\sum\\limits_{i=1}^m N_i \\log\\left( \\lambda \\right)\n+ \\sum\\limits_{i=1}^m N_i \\log\\left( ER_i \\right)\n- \\sum\\limits_{i=1}^m N_i \\log\\left( \\lambda ER_i + \\alpha \\right)\n+ m \\alpha \\log\\left( \\alpha \\right) \\\\\n& - \\alpha \\sum\\limits_{i=1}^m \\log\\left( \\lambda ER_i + \\alpha \\right)\n\\end{split}\n\\]\n\nPara la distribución binomial, tenemos que si \\[\n\\lambda ER = np \\Rightarrow p = \\frac{\\lambda ER}{n}\n\\] de donde la ley de probabilidad queda modificada como \\[\nP( N = k ) = \\binom{n}{k} \\left( \\frac{\\lambda ER}{n} \\right)^k \\left( 1 - \\frac{\\lambda ER}{n} \\right)^{n-k},\n\\qquad \\forall k \\in \\{0,\\ldots, n\\}\n\\] entonces \\(N \\rightsquigarrow Binom\\left( n, \\frac{\\lambda ER}{n} \\right)\\)\n\nMismo razonamiento, si disponemos de una muestra de \\(N_1, \\ldots, N_m\\) de variables aleatorias independientes y también exposiciones al riesgo \\(ER_1, \\ldots, ER_m\\) asociadas a cada una de las variables aleatorias, de tal forma que \\(N_i \\rightsquigarrow  Binom\\left( n, \\frac{\\lambda ER_i}{n} \\right)\\), entonces la función de verosimilitud logarítmica toma la forma: \\[\n\\begin{split}\n\\ell( n, \\lambda )\n& = m \\log( n! )\n- \\sum\\limits_{i=1}^m \\log( N_i! )\n- \\sum\\limits_{i=1}^m \\log( (n - N_i)! ) \\\\\n& + \\sum\\limits_{i=1}^m N_i \\log \\lambda\n+ \\sum\\limits_{i=1}^m N_i \\log ER_i\n- n \\sum\\limits_{i=1}^m N_i \\\\\n& + n \\sum\\limits_{i=1}^m \\log\\left( 1 - \\frac{\\lambda ER_i}{n} \\right)\n- \\sum\\limits_{i=1}^m N_i \\log\\left( 1 - \\frac{\\lambda ER_i}{n} \\right)\n\\end{split}\n\\]\nEn los tres casos anteriores, hemos desarrollado una metodología para la estimación de la distribución de frecuencia de los reclamos, tomando en cuenta la exposición al riesgo de cada una de las pólizas a las cuales estos reclamos corresponden.\n\nAhora bien, pongamos en aplicación todo lo aprendido hasta el momento. Para ello, trabajaremos con información de pólizas de un seguro de salud. La información está compuesta de dos objetos, uno la producción de pólizas y dos los reclamos de varios productos asociados al ramo.\nLa información corresponde a un seguro de salud el cual tiene pólizas individuales y colectivas, para cada una de sus contratos se especifica un deducible de la forma \\(D( X ) = \\min( \\max( X - d, 0 ), M )\\), donde precisamente \\(d\\) es lo que se conoce como el deducible y \\(M\\) como el monto máximo de cobertura.\n\n\nCode\nload( '../RData/production_claims_health.RData' )\n\n\nEl objeto con la producción de pólizas tienen la siguiente estructura. Es un objeto con una forma usual que uno puede encontrar en las bases de producción de un asegurador. Se incluye en particular campos de especial interés como la fecha de nacimiento bdate, el inicio de vigencia de la cobertura start y el final de vigencia de la cobertura end.\n\n\nCode\nproduction[ 1:10 ] %&gt;%\n  kable(\n    label = NA,\n    caption = 'Pólizas',\n    row.names = FALSE,\n    align = 'llllrrrrrrrl',\n    digits = c( 0, 0, 0, 0, 0, 2, 2, 2, 0, 0, 0, 0 ),\n    format.args = list( big.mark = ',', decimal.mark = '.', scientific = FALSE ),\n    escape = FALSE,\n    centering = TRUE,\n    booktabs = TRUE,\n    longtable = TRUE ) %&gt;%\n  kable_classic( \n    full_width = FALSE, \n    html_font = \"Cambria\", \n    position = 'center', \n    latex_options = c( 'hold_position', 'repeat_header', 'scale_down' ) ) %&gt;%\n  scroll_box( width = \"100%\", height = \"500px\" )\n\n\n\n\nPólizas\n\n\nid\npolicy\ninsured\nsex\nbdate\npremium\nfpremium\nstart\nend\ntype\ndeductible\namount\n\n\n\n\n59f51c382ebf\n288dd685\n00\nMALE\n1986-09-14\n297.68\n4\n2017-12-22\n2018-09-01\nCOL\n25\n11,250\n\n\nf1b316dbca9c\n708ee54e\n00\nMALE\n1998-11-30\n390.00\n4\n2018-02-22\n2018-06-05\nIND\n35\n12,250\n\n\n9924a3ba6b5c\n16fa4935\n00\nFEMALE\n1996-09-15\n380.00\n4\n2018-06-03\n2018-08-24\nIND\n35\n14,500\n\n\n59e63ffc9688\n66d6f1d0\n00\nFEMALE\n1973-07-28\n288.64\n4\n2017-12-18\n2018-12-17\nCOL\n25\n9,250\n\n\nd28db8f7c754\n6d64a583\n00\nMALE\n1979-02-14\n564.08\n12\n2018-01-11\n2021-01-10\nIND\n30\n12,750\n\n\nfb9e3f6e5c6f\nb16ef666\n00\nFEMALE\n1964-12-19\n375.00\n4\n2018-01-23\n2018-05-06\nIND\n25\n15,000\n\n\n78e98ce3bcdd\n4ab54a09\n00\nMALE\n1962-07-11\n615.00\n2\n2020-12-02\n2020-12-16\nCOL\n100\n13,750\n\n\n7e723f43e928\n1f9bde7f\n00\nFEMALE\n1966-11-23\n370.00\n4\n2018-02-08\n2018-06-05\nIND\n35\n14,750\n\n\n2aac4c6a8625\nc74ca3ae\n00\nFEMALE\n1984-11-02\n478.36\n12\n2018-01-28\n2018-07-05\nIND\n40\n11,250\n\n\n08cf0c0c7a47\ne735ef11\n00\nFEMALE\n1972-09-28\n518.72\n12\n2018-01-11\n2018-08-31\nIND\n15\n13,750\n\n\n\n\n\n\n\nLos reclamos se detallan en la tabla a continuación. En la información no se dispone del valor original del reclamo, sino tan solo del valor pagado, después de aplicar la función deducible \\(D\\). En la misma información se incluye el deducible \\(d\\) y el monto de máximo de cobertura \\(M\\) que se utilizó para aplicar la deducción.\n\n\nCode\nclaims[ 1:10 ] %&gt;%\n  kable(\n    label = NA,\n    caption = 'Reclamos',\n    row.names = FALSE,\n    align = 'lllllrrrll',\n    digits = c( 0, 0, 0, 0, 0, 0, 2, 0, 0, 0 ),\n    format.args = list( big.mark = ',', decimal.mark = '.', scientific = FALSE ),\n    escape = FALSE,\n    centering = TRUE,\n    booktabs = TRUE, \n    longtable = TRUE ) %&gt;%\n  kable_classic( \n    full_width = FALSE, \n    html_font = \"Cambria\", \n    position = 'center', \n    latex_options = c( 'hold_position', 'repeat_header', 'scale_down' ) ) %&gt;%\n  scroll_box( width = \"100%\", height = \"500px\" )\n\n\n\n\nReclamos\n\n\npolicy\nid\nidc\ntype\nservice\ncdate\nclaim\nicd\ndeductible\namount\ninsured\nbdate\nsex\nstart\nend\n\n\n\n\n708ee54e\nf1b316dbca9c\nf3481be7c3be56\nIND\nAMB\n2018-04-08\n27.95\nM545\n35\n12,250\n00\n1998-11-30\nMALE\n2018-02-22\n2018-06-05\n\n\n708ee54e\nf1b316dbca9c\naea75b4dbe7d32\nIND\nAMB\n2018-04-06\n35.45\nM545\n35\n12,250\n00\n1998-11-30\nMALE\n2018-02-22\n2018-06-05\n\n\n708ee54e\nf1b316dbca9c\n427923d6709ea8\nIND\nAMB\n2018-04-06\n96.06\nI10\n35\n12,250\n00\n1998-11-30\nMALE\n2018-02-22\n2018-06-05\n\n\n708ee54e\nf1b316dbca9c\n46406cc78413ac\nIND\nAMB\n2018-04-07\n71.06\nI10\n35\n12,250\n00\n1998-11-30\nMALE\n2018-02-22\n2018-06-05\n\n\n708ee54e\nf1b316dbca9c\ndc95b57f0c4c93\nIND\nAMB\n2018-04-05\n93.56\nI10\n35\n12,250\n00\n1998-11-30\nMALE\n2018-02-22\n2018-06-05\n\n\n66d6f1d0\n59e63ffc9688\n191c26c896d6f8\nCOL\nAMB\n2018-02-25\n55.00\nR10\n25\n9,250\n00\n1973-07-28\nFEMALE\n2017-12-18\n2018-12-17\n\n\n66d6f1d0\n59e63ffc9688\n08a64b4465231f\nCOL\nAMB\n2018-02-25\n52.50\nR10\n25\n9,250\n00\n1973-07-28\nFEMALE\n2017-12-18\n2018-12-17\n\n\n66d6f1d0\n59e63ffc9688\n28e2eb59a7cf35\nCOL\nAMB\n2018-02-23\n42.00\nR10\n25\n9,250\n00\n1973-07-28\nFEMALE\n2017-12-18\n2018-12-17\n\n\n6d64a583\nd28db8f7c754\nc0aa9870615af5\nIND\nAMB\n2018-08-31\n37.50\nH57\n30\n12,750\n00\n1979-02-14\nMALE\n2018-01-11\n2021-01-10\n\n\n6d64a583\nd28db8f7c754\nafc1e924e9c9b5\nIND\nAMB\n2018-09-02\n55.00\nH10\n30\n12,750\n00\n1979-02-14\nMALE\n2018-01-11\n2021-01-10\n\n\n\n\n\n\n\nPrimero definimos algunos parámetros que consideramos serán útiles en el desarrollo del ejemplo. Lo primero es definir las fechas que marcan el periodo para el cual estamos interesados en estudiar, con ello seleccionamos la periodicidad con la cual vamos a medir la frecuencia de los reclamos, en este caso de forma anual year. También, seleccionamos de antemano unas edades para formar grupos de riesgo por edad.\n\n\nCode\nidate &lt;- ymd( '2018-01-01' )\nedate &lt;- ymd( '2021-01-01' )\nsdate &lt;- seq( idate, edate, by = 'year' )\ndates &lt;- data.table( di = sdate[ -length( sdate ) ], de = sdate[ -1 ] )\ndates[ , tid := 1 ]\nxgs &lt;- c( 0, 20, 40, 60, Inf )\n\n\nPara generar un modelo de pérdida, asumiremos que la frecuencia de los siniestros \\(N\\) es independiente de los reclamos \\(\\{X_i\\}\\). Con ellos estaremos en la capacidad de formular un modelo colectivo de riesgo.\nAhora tratamos la información correspondiente a la producción de las pólizas, en particular extrayendo las pólizas que intersectan con los periodo de observación y creando los grupos de edades xg.\n\n\nCode\nproduction[ , tid := 1 ]\nprod_prep &lt;- merge.data.table( dates, production, by = 'tid', all.x = TRUE, allow.cartesian = TRUE )\nprod_prep[ , tid := NULL ]\nprod_prep &lt;- prod_prep[ start &lt; de & end &gt;= di ]\nprod_prep[ , fi := pmax( start, di ) ]\nprod_prep[ , fe := pmin( end, de ) ]\nprod_prep[ , fm := fi + ( fe - fi ) / 2 ]\nprod_prep[ , x := pmax( round( interval( bdate, fm ) / dyears( 1 ), 0 ), 0 ) ]\nprod_prep[ , xg := cut( x = x, breaks = xgs, include.lowest = TRUE, ordered_result = TRUE, right = FALSE ) ]\n\nnoclaims_prep &lt;- unique( prod_prep[ , list( policy, id, sex, xg, bdate, di, de, N = 0, S = 0 ) ] )\nnoclaims_prep &lt;- noclaims_prep[ , list( policy, id, sex, xg, bdate, di, de, N, S ) ]\n\n\nAsí mismo, trabajamos con la información correspondiente a los reclamos. Primeramente los agregados por código único de siniestro idc, fecha de siniestro cdate y periodo de observación di a de, esto para agrupar los reclamos que corresponden a un solo siniestros por periodo. Luego, filtramos los siniestros que solo se presentan en el periodo de observación. También, se cuenta cuantos siniestros presenta cada asegurado según por su código único de identificación id y también la póliza a la que pertenecen policy.\nFinalmente, se hace una estadística de frecuencia por cada grupo de riesgo dados por el sexo sex y el grupo de edad xg. En particular se calcula el indicador para la familia de Panjer, esto con la finalidad de caracterizar el tipo de distribución que determina la frecuencia de los reclamos.\n\n\nCode\nclaims_prep &lt;- claims[ , list( DX = sum( claim ) ), by = list( policy, id, idc, type, sex, bdate, cdate ) ]\nclaims_prep[ , x := round( interval( bdate, cdate ) / dyears( 1 ), 0 ) ]\nclaims_prep[ , xg := cut( x = x, breaks = xgs, include.lowest = TRUE, ordered_result = TRUE, right = FALSE ) ]\nclaims_prep[ , tid := 1 ]\nclaims_prep &lt;- merge.data.table( dates, claims_prep, by = c( 'tid' ), all.x = TRUE, allow.cartesian = TRUE )\nclaims_prep[ , tid := NULL ]\nclaims_prep &lt;- claims_prep[ cdate &gt;= di & cdate &lt; de ]\nclaims_prep &lt;- claims_prep[ , list( N = .N, S = sum( DX ) ), by = list( policy, id, sex, xg, bdate, di, de ) ]\nclaims_prep &lt;- rbind( claims_prep, noclaims_prep )\nclaims_prep &lt;- claims_prep[ , list( N = sum( N ), S = sum( S ) ), by = list( policy, id, sex, xg, bdate, di, de ) ]\n\ncount &lt;- copy( claims_prep )\ncount &lt;- count[ , list( fn = .N, S = sum( S ) ), by = list( sex, xg, N ) ]\n\nexpand &lt;- count[ , list( N = max( N ) ), by = list( sex, xg ) ]\nexpand &lt;- expand[ , list( N = seq( 0, N, 1 ) ), by = list( sex, xg ) ]\n\ncount &lt;- merge.data.table( expand, count, by = c( 'sex', 'xg', 'N' ), all.x = TRUE )\ncount[ is.na( fn ), fn := 0 ]\ncount[ is.na( S ), S := 0 ]\nsetorder( count, sex, xg, N )\ncount[ , pn := fn / sum( fn ), by = list( sex, xg ) ]\ncount[ , vn := shift( pn, fill = 0 ), by = list( sex, xg ) ]\ncount[ vn != 0, vn := N * pn / vn ]\ncount[ vn == 0, vn := 0 ]\n\ngrpcl &lt;- unique( count[ , list( sex, xg ) ] )\ngrpcl[ , grp := paste0( sex, ' ', xg ) ]\nsetorder( grpcl, sex, xg )\n\ncount[ , grp := factor( paste0( sex, ' ', xg ), labels = grpcl$grp, ordered = TRUE ) ]\nclaims_prep[ , grp := factor( paste0( sex, ' ', xg ), labels = grpcl$grp, ordered = TRUE ) ]\n\n\nLos grupos de riesgo, que designaremos con un índice \\(g\\), los hemos seleccionado para estudiar la frecuencia y severidad de los reclamos, estos son los siguientes:\n\n\nCode\ngrpcl %&gt;%\n  kable(\n    label = NA,\n    caption = 'Grupos de clasificación del riesgo',\n    row.names = FALSE,\n    col.names = c( 'sex', '$x_g$', 'grupo' ),\n    align = 'lll',\n    digits = c( 0, 0, 0 ),\n    format.args = list( big.mark = ',', decimal.mark = '.', scientific = FALSE ),\n    escape = FALSE,\n    centering = TRUE,\n    booktabs = TRUE ) %&gt;%\n  kable_classic( full_width = FALSE, html_font = \"Cambria\", position = \"center\" ) %&gt;%\n  scroll_box( width = \"100%\", height = \"500px\" )\n\n\n\n\nGrupos de clasificación del riesgo\n\n\nsex\n$x_g$\ngrupo\n\n\n\n\nFEMALE\n[0,20)\nFEMALE [0,20)\n\n\nFEMALE\n[20,40)\nFEMALE [20,40)\n\n\nFEMALE\n[40,60)\nFEMALE [40,60)\n\n\nFEMALE\n[60,Inf]\nFEMALE [60,Inf]\n\n\nMALE\n[0,20)\nMALE [0,20)\n\n\nMALE\n[20,40)\nMALE [20,40)\n\n\nMALE\n[40,60)\nMALE [40,60)\n\n\nMALE\n[60,Inf]\nMALE [60,Inf]\n\n\n\n\n\n\n\nEl gráfico a continuación muestra el comportamiento del indicador \\(v_n = n \\frac{p_n}{p_{n-1}}\\), para cada uno de los grupos \\(g\\) de riesgo.\n\n\nCode\nplt &lt;- ggplot( ) +\n  geom_point( data = count, aes( x = N, y = vn ), colour = 'red3', size = 0.5 ) +\n  facet_wrap( vars( grp ), ncol = 4 ) +\n  xlab( TeX( \"$n$\" ) ) + \n  ylab( TeX( \"$v_n$\" ) ) + \n  theme_bw() +\n  theme( legend.position = \"none\" )\nplot( plt )\n\n\n\n\n\n\n\n\n\nComo hemos evidenciado, para todos los casos en consideración el indicador \\(v_n\\) tiene un tendencia creciente y se evidencia un crecimiento con orden lineal, esto sugiere que la distribución discreta más adecuada para explicar la frecuencia de los reclamos para cada grupo es una binomial negativa \\(N \\rightsquigarrow NBin( \\alpha, p )\\). Para la estimación por maximización de verosimilitud, hacemos uso de la función fitdist con la cual estimaremos los parámetros dentro de una familia de distribuciones binomiales negativas.\nPor su parte, la función de verosimilitud logarítmica para la frecuencia tiene la siguiente forma para cada grupo \\(g\\). \\[\n\\begin{split}\n\\ell_g( \\alpha, p )\n& = \\sum\\limits_{i=1}^{n_g}\n\\left\\{ \\log\\left( \\Gamma( \\alpha + N_i ) \\right)\n- \\log\\left( \\Gamma(N_i + 1) \\right)\n- \\log\\left( \\Gamma(\\alpha) \\right)\n+ \\alpha \\log(p)\n+ N_i \\log( 1 - p ) \\right\\} \\\\\n& = \\sum\\limits_{i=1}^{n_g}\n\\left\\{ \\log\\left( \\Gamma( \\alpha + N_i ) \\right)\n- \\log\\left( \\Gamma(N_i + 1) \\right) \\right\\}\n- n_g \\log\\left( \\Gamma(\\alpha) \\right)\n+ n_g \\alpha \\log(p)\n+ n \\log( 1 - p )\n\\end{split}\n\\] Como ya lo mencionamos, para cada grupo \\(g\\) podemos maximizar la verosimilitud logarítmica de la frecuencia utilizando la función fitdist del paquete fitdistrplus.\n\n\nCode\nfit_count &lt;- list()\nestim_frec &lt;- NULL\n\nfor ( g in grpcl$grp ) {\n  \n  fit_count[[ g ]] &lt;- fitdistrplus::fitdist( \n    claims_prep[ grp == g ]$N, \n    distr = dnbinom, \n    method = 'mle', \n    keepdata = TRUE, \n    discrete = TRUE )\n  \n  \n  estim_frec &lt;- rbind(\n    estim_frec,\n    data.table( grp = g, \n                a = fit_count[[ g ]]$estimate[1], \n                EN = fit_count[[ g ]]$estimate[2], \n                loglik = fit_count[[ g ]]$loglik,\n                aic = fit_count[[ g ]]$aic,\n                bic = fit_count[[ g ]]$bic )\n  )\n}\nestim_frec[ , p := a / ( a + EN ) ]\nestim_frec[ , SDN := sqrt( a * ( 1 - p ) / p^2 ) ]\nestim_frec &lt;- estim_frec[ , list( grp, a, p, EN, SDN, loglik, aic, bic )]\n\n\nEn la lista que hemos creado fit_count hemos almacenado cada una de los objetos con las estimaciones de la frecuencia para cada uno de los grupos. A continuación, presentamos los resultados de la estimación para la frecuencia.\n\nCode\nestim_frec %&gt;%\n  kable(\n    label = NA,\n    caption = 'Estimación de frecuencia',\n    row.names = FALSE,\n    col.names = c( '$g$', '$\\\\alpha$', '$p$', '$\\\\E[ N ]$', '$\\\\sqrt{\\\\V[ N ]}$', '$\\\\ell( n, p )$', '$AIC$', '$BIC$' ),\n    align = 'lrrrrrrr',\n    digits = c( 0, 5, 5, 5, 5,  5, 2, 2 ),\n    format.args = list( big.mark = ',', decimal.mark = '.', scientific = FALSE ),\n    escape = FALSE,\n    centering = TRUE,\n    booktabs = TRUE ) %&gt;%\n  kable_classic( full_width = FALSE, html_font = \"Cambria\", position = \"center\" ) %&gt;%\n  scroll_box( width = \"100%\", height = \"500px\" )\n\n\n\nEstimación de frecuencia\n\n\n$g$\n$\\alpha$\n$p$\n$\\E[ N ]$\n$\\sqrt{\\V[ N ]}$\n$\\ell( n, p )$\n$AIC$\n$BIC$\n\n\n\n\nFEMALE [0,20)\n0.07114\n0.02922\n2.36335\n8.99292\n-1,159.649\n2,323.30\n2,332.90\n\n\nFEMALE [20,40)\n0.06690\n0.01515\n4.34957\n16.94494\n-2,497.060\n4,998.12\n5,008.94\n\n\nFEMALE [40,60)\n0.13472\n0.01344\n9.88593\n27.11744\n-3,159.785\n6,323.57\n6,333.88\n\n\nFEMALE [60,Inf]\n0.16226\n0.01003\n16.01106\n39.94829\n-1,918.017\n3,840.03\n3,848.98\n\n\nMALE [0,20)\n0.06076\n0.02672\n2.21317\n9.10109\n-1,250.564\n2,505.13\n2,515.06\n\n\nMALE [20,40)\n0.05065\n0.02508\n1.96877\n8.86002\n-1,294.692\n2,593.38\n2,603.65\n\n\nMALE [40,60)\n0.10530\n0.01725\n5.99728\n18.64330\n-2,163.142\n4,330.28\n4,340.30\n\n\nMALE [60,Inf]\n0.17932\n0.01091\n16.26105\n38.61118\n-1,568.422\n3,140.84\n3,149.33\n\n\n\n\n\nAhora, nos enfocamos en estudiar la severidad de los reclamos. Desde un inicio sabemos que los valores de los reclamos ya vienen censurados, a causa de la aplicación de la función deducible \\(D\\), como consecuencia tenemos pérdida de información que no podemos recuperar, por tal razón es de esperar que la función de distribución acumulada de los reclamos deducidos \\(D(X)\\) tenga singularidades. Si buscamos utilizar el método de máxima verosimilitud para la estimación de la mejor distribución, debemos hacer uso de la expresión completa que presentamos en \\(\\ref{eq:loglik}\\).\n\n\nCode\nsev_claims &lt;- claims[ , list( policy, id, idc, type, sex, bdate, cdate, deductible, amount, claim ) ]\nsev_claims[ , x := round( interval( bdate, cdate ) / dyears( 1 ), 0 ) ]\nsev_claims[ , xg := cut( x = x, breaks = xgs, include.lowest = TRUE, ordered_result = TRUE, right = FALSE ) ]\nsev_claims[ , tid := 1 ]\nsev_claims &lt;- merge.data.table( dates, sev_claims, by = c( 'tid' ), all.x = TRUE, allow.cartesian = TRUE )\nsev_claims &lt;- sev_claims[ cdate &gt;= di & cdate &lt; de ]\nsev_claims[ , tid := NULL ]\nsev_claims[ , grp := factor( paste0( sex, ' ', xg ), labels = grpcl$grp, ordered = TRUE ) ]\n\n\nLos histogramas a continuación muestran como se distribuyen los reclamos ya censurados por cada grupos de riesgo.\n\n\nCode\nxlim &lt;- c( 0, 1e3 )\nxbrk &lt;- seq( xlim[1], xlim[2], length = 5 )\nxlbl &lt;- xbrk\n\nbins &lt;- sev_claims[ , list( bins = nclass.scott( claim ) ), by = list( grp ) ]\n\nplt &lt;- ggplot( ) +\n  geom_histogram( data = sev_claims, aes( claim, after_stat( density ) ), \n                  fill = 'dodgerblue3', colour = 'dodgerblue4', alpha = 0.3, \n                  bins = min( bins$bins ) ) +\n  facet_wrap( vars( grp ), scale = 'free_x', ncol = 4 ) +\n  scale_x_continuous( breaks = xbrk,\n                      labels = xlbl,\n                      limits = xlim,\n                      expand = c( 0, 0 ) ) +\n  xlab( TeX( \"$D(X)$\" ) ) + \n  ylab( TeX( \"$f_{D(X)}$\" ) ) + \n  theme_bw() +\n  theme( legend.position = \"none\" )\nplot( plt )\n\n\n\n\n\n\n\n\n\nPor no incrementar la complejidad del ejemplo, asumiremos que los reclamos \\(X\\) por cada grupo de riesgo son i.i.d. y estos siguen como distribución alguna presente en la familia log-normal, es decir \\(X \\rightsquigarrow LN( \\mu, \\sigma )\\). Además, es de notar que el razonamiento es válido para cualquier otra distribución. Más aún, sabemos que para cada reclamo \\(i\\)-ésimo se aplicó un deducible \\(d_i\\) y un monto máximo asegurado \\(M_i\\) determinado por la cobertura de la póliza y estos valores a su vez determinan el deducible \\(D_i\\), la distribución de probabilidad para los valores aleatorios ya deducidos \\(Z = D_i(X)\\), según lo ya estudiado en @ref(deducibles), tiene la forma: \\[\nF( z ) = \\mathbf{1}_{[0,M_i)}( z ) F_X( z + d_i ) + \\mathbf{1}_{[M_i,+\\infty)}( z )\n\\]\nEntonces, tomando en cuenta la forma general que hemos desarrollado en @ref(eq:loglik) y la forma anterior para la distribución acumulada de probabilidad, podemos determinar una expresión más precisa para la función de verosimilitud logarítmica, donde debemos tomar en cuenta las singularidades que producen los deducibles para cada póliza. \\[\n\\begin{split}\n\\ell( \\mu, \\sigma )\n& = \\sum\\limits_{x_i \\in \\Dif F} \\log \\left( f(x_i,\\mu, \\sigma) \\right)\n+ \\sum\\limits_{x_i \\notin \\Dif F} \\log \\left( F\\left( x_i, \\mu, \\sigma \\right) - F\\left( x_i-, \\mu, \\sigma \\right) \\right) \\\\\n& = \\sum\\limits_{x_i \\notin\\{0,M_i\\}} \\log \\left( f(x_i,\\mu, \\sigma) \\right)\n+ \\sum\\limits_{x_i \\in\\{0,M_i\\}} \\log \\left( F\\left( x_i, \\mu, \\sigma \\right) - F\\left( x_i-, \\mu, \\sigma \\right) \\right) \\\\\n& = \\sum\\limits_{x_i \\notin \\{0,M_i\\}}\n\\log \\left( \\frac{1}{x_i\\sqrt{2\\pi} \\sigma}  \\exp\\left( -\\frac{(\\ln(x_i) - \\mu)^2}{\\sigma^2} \\right) \\right)\n+ \\sum\\limits_{x_i = M_i} \\log \\left( 1 - F\\left( M_i-, \\mu, \\sigma \\right) \\right)\n+ \\sum\\limits_{x_i = 0} \\log \\left( F\\left( 0, \\mu, \\sigma \\right) - 0 \\right) \\\\\n& = -\\sum\\limits_{x_i \\notin\\{0,M_i\\}} \\left\\{\n\\frac{(\\ln(x_i) - \\mu)^2}{\\sigma^2}\n+\\log \\left( x_i \\right)\n+\\log \\left( \\sqrt{2\\pi} \\sigma \\right) \\right\\}\n+ \\sum\\limits_{x_i = M_i} \\log \\left( 1 - \\frac{1}{\\sqrt{2\\pi} \\sigma} \\int\\limits_{0}^{M_i + d_i} \\frac{1}{y} \\exp\\left( -\\frac{(\\ln(y) - \\mu)^2}{\\sigma^2} \\right)\\ dy \\right) \\\\\n& + \\sum\\limits_{x_i = 0} \\log \\left( \\frac{1}{\\sqrt{2\\pi} \\sigma} \\int\\limits_{0}^{d_i} \\frac{1}{y} \\exp\\left( -\\frac{(\\ln(y) - \\mu)^2}{\\sigma^2} \\right)\\ dy \\right) \\\\\n\\end{split}\n\\]\nPreparamos la función de verosimilitud en R, en este caso debemos construirla explícitamente, sin la ayuda de alguna función en un paquete, esto se debe a que el deducible \\(d\\) y el monto máximo de cobertura \\(M\\) varían entre pólizas. Esta construcción debe ser realizada para cada grupo de riesgo y a su vez debemos maximizar cada una de las verosimilitudes logarítmicas, esto lo podemos realizar de forma integrada utilizando un bucle de programación for y el método de optimización no lineal nloptr del paquete con el mismo nombre.\n\n\nCode\nFd &lt;- plnorm\nfd &lt;- dlnorm\n\ntheta0 &lt;- c( 1, 1 )\nloglik_max &lt;- list()\nestim_sev &lt;- NULL\n\nfor ( g in grpcl$grp ) {\n  \n  datsev &lt;- sev_claims[ grp == g ]\n  \n  # Definimos la función de verosimilitud logarítmica para cada grupo\n  lglk &lt;- function( theta ) {\n    \n    # suma en puntos de continuidad\n    l &lt;- sum( sapply( datsev[ claim &gt; 0 & claim &lt; amount ]$claim,\n                      FUN = function( x ) log( fd( x, meanlog = theta[ 1 ], sdlog = theta[ 2 ] ) ) ) )\n    \n    # suma en las discontinuidades\n    # singularidad en la parte superior, hacia el máximo monto\n    sg &lt;- datsev[ claim == amount, list( x = amount + deductible ) ]\n    sg &lt;- sg$x\n    \n    if ( length( sg ) &gt; 0 ) {\n      l &lt;- l + sum( sapply( sg, FUN = function( x ) log( 1 - Fd( x, meanlog = theta[ 1 ], sdlog = theta[ 2 ] ) ) ) )  \n    }\n    \n    # singularidad en la parte inferior, hacia el deducible\n    sg &lt;- datsev[ claim == 0, list( x = deductible ) ]\n    sg &lt;- sg$x\n    \n    if ( length( sg ) &gt; 0 ) {\n      l &lt;- l + sum( sapply( sg, FUN = function( x ) log( Fd( x, meanlog = theta[ 1 ], sdlog = theta[ 2 ] ) ) ) )  \n    }\n    \n    return( -l ) # con menos ya que el optimizador minimiza\n  }\n  \n  dlglk &lt;- function( x ) nl.grad( x, lglk )\n  \n  # Maximización de la verosimilitud\n  loglik_max[[ g ]] &lt;- nloptr(\n    x0 = theta0,\n    eval_f = lglk,\n    eval_grad_f = dlglk,\n    lb = c( 0, 0 ),\n    ub = c( Inf, Inf ),\n    opts = list( maxeval = 1e4, \n                 ftol_abs = 1e-15,\n                 xtol_abs = c( 1e-12, 1e-12 ),\n                 algorithm = 'NLOPT_LD_LBFGS' ) )\n  \n  estim_sev &lt;- rbind( \n    estim_sev, \n    data.table( grp = g, \n                n = nrow( datsev ),\n                meanlog = loglik_max[[ g ]]$solution[ 1 ], \n                sdlog = loglik_max[[ g ]]$solution[ 2 ],\n                loglik = -loglik_max[[ g ]]$objective )\n  )\n}\n\nestim_sev[ , EX := exp( meanlog + 0.5 * sdlog^2 ) ]\nestim_sev[ , SDX := sqrt( ( exp( sdlog^2 ) - 1 ) * exp( 2 * meanlog + sdlog^2 ) ) ]\nestim_sev[ , aic := 2 * 2 - 2 * loglik ]\nestim_sev[ , bic := 2 * log( n ) - 2 * loglik ]\n\n\nEn este caso tenemos un problema donde existe convergencia para cada una de las optimizaciones por verosimilitud sin embargo, en la práctica un bucle de esta naturaleza puede implicar un costo computacional significativo en memoría y tiempo. Dependiendo de la dimensión del problema, se recomienda usar algunas técnicas que utilicen cálculo en paralelo o manejo óptimo de memoria, en algunas otras circunstancias puede ser conveniente utilizar un método de descomposición por batchs de información.\nLos resultados de la estimación mediante maximización de verosimilitud para la severidad de los reclamos, los presentamos en la tabla a continuación.\n\n\nCode\nestim_sev %&gt;%\n  kable(\n    label = NA,\n    caption = 'Estimación de severidad de los reclamos',\n    row.names = FALSE,\n    col.names = c( '$g$', '$n$', '$\\\\mu$', '$\\\\sigma$', '$\\\\ell( \\\\mu, \\\\sigma )$', '$\\\\E[ X ]$', \n                   '$\\\\sqrt{\\\\V[ X ]}$', '$AIC$', '$BIC$' ),\n    align = 'lrrrrrrrr',\n    digits = c( 0, 0, 5, 5, 5, 2, 2, 2, 2 ),\n    format.args = list( big.mark = ',', decimal.mark = '.', scientific = FALSE ),\n    escape = FALSE,\n    centering = TRUE,\n    booktabs = TRUE ) %&gt;%\n  kable_classic( full_width = FALSE, html_font = \"Cambria\", position = \"center\" ) %&gt;%\n  scroll_box( width = \"100%\", height = \"500px\" )\n\n\n\n\nEstimación de severidad de los reclamos\n\n\n$g$\n$n$\n$\\mu$\n$\\sigma$\n$\\ell( \\mu, \\sigma )$\n$\\E[ X ]$\n$\\sqrt{\\V[ X ]}$\n$AIC$\n$BIC$\n\n\n\n\nFEMALE [0,20)\n2,127\n3.80928\n0.59566\n-10,018.47\n53.88\n35.16\n20,040.94\n20,052.26\n\n\nFEMALE [20,40)\n7,197\n3.81895\n0.62012\n-34,258.09\n55.21\n37.81\n68,520.18\n68,533.94\n\n\nFEMALE [40,60)\n12,676\n3.85843\n0.64676\n-61,371.99\n58.42\n42.10\n122,747.97\n122,762.87\n\n\nFEMALE [60,Inf]\n10,375\n3.91002\n0.67717\n-51,243.42\n62.76\n47.87\n102,490.83\n102,505.33\n\n\nMALE [0,20)\n2,343\n3.81387\n0.61320\n-11,114.59\n54.70\n36.96\n22,233.19\n22,244.70\n\n\nMALE [20,40)\n2,468\n3.84725\n0.66988\n-12,008.13\n58.65\n44.14\n24,020.27\n24,031.89\n\n\nMALE [40,60)\n6,614\n3.89478\n0.70528\n-32,835.54\n63.02\n50.59\n65,675.08\n65,688.67\n\n\nMALE [60,Inf]\n8,372\n3.92278\n0.66965\n-41,363.77\n63.24\n47.57\n82,731.54\n82,745.60\n\n\n\n\n\n\n\nSumarizando, hemos determinado para cada grupo de riesgo \\(g\\) una distribución para la variable aleatoria correspondiente al número de reclamos \\(N_g \\rightsquigarrow NBinom( \\alpha_g, p_g )\\), de igual forma hemos estimado una distribución para la severidad de los reclamos \\(X \\rightsquigarrow LN( \\mu_g, \\sigma_g )\\).\nEs de notar que la estimación de la variable \\(N_g\\) del número de siniestro se lo realizó por individuo y por cada periodo de observación di a de, esto nos permite estimar una frecuencia anualizada, debido a los anchos de los intervalos de observación. Pero si se considera pólizas con periodos de cobertura diferentes, hay que realizar el respectivo ajuste por la exposición al riesgo.\nEl modelo de colectivo para este caso en particular debe tomar en cuenta el número de individuos asegurados \\(n_g\\) en el total de pólizas por cada grupo de riesgo \\(g\\). Esto bajo el supuesto que todas las pólizas de las \\(n_g\\) tiene la misma exposición al riesgo de un año, si no es el caso habrá que realizar ajustes en la variable \\(N_g\\) que cuenta el número de reclamos. \\[\nS_g = \\sum\\limits_{i=1}^{n_g} \\sum\\limits_{j=1}^{N_g} D_{g,i}\\left( X_{g,i,j} \\right)\n\\]\nLa perdida total que sumariza cada grupo de riesgo \\(g\\), es la dada por la expresión: \\[\nS = \\sum\\limits_{g \\in G} S_g\n= \\sum\\limits_{g \\in G} \\sum\\limits_{i=1}^{n_g} \\sum\\limits_{j=1}^{N_g} D_{g,i}\\left( X_{g,i,j} \\right)\n\\] Para realizar el cálculo de los valores deducidos se puede examinar la expresión del deducible, con ello llegamos al siguiente resultado. \\[\n\\begin{split}\n\\E\\left[D(X)\\right]\n& = \\int\\limits_0^{+\\infty} D( x ) dF( x ) \\\\\n& = \\int\\limits_0^{+\\infty} \\min\\left( \\max( x - d, 0 ), M \\right) dF( x ) \\\\\n& = \\int\\limits_d^{M+d} ( x - d ) dF( x ) + \\int\\limits_{M+d}^{+\\infty} M dF( x ) \\\\\n& = \\int\\limits_d^{M+d} x dF( x ) - d P\\left( d \\leq X \\leq M + d \\right) + M P\\left( X &gt; M + d \\right) \\\\\n& = \\int\\limits_d^{M+d} x dF( x ) - d \\left( F( M + d ) - F( d ) \\right) + M \\left( 1 - F( M + d ) \\right) \\\\\n& = \\int\\limits_d^{M+d} x dF( x ) + M + d\\ F( d ) - ( M + d ) F( M + d )\n\\end{split}\n\\]\nLa integral a la izquierda pude ser calculada con el uso de una aproximación numérica a la integral. Por su parte, los términos asociados a la probabilidad pueden ser calculados utilizando la función de distribución acumulada de la variable aleatoria de los reclamos \\(X\\).\n\n\nCode\nestim &lt;- merge.data.table( estim_frec[ , list( grp, EN, SDN ) ], \n                           estim_sev[ , list( grp, EX, SDX, meanlog, sdlog ) ], by = 'grp' )\n\n\nAhora configuramos un posible portafolio, el cual ya en la práctica debe considerar los objetivos comerciales para la venta de diferentes productos de seguro. Para cada grupo de riesgo \\(g\\) se puede considerar un número de póliza de un determinado producto, caracterizados por su deducible \\(d\\) y monto máximo asegurado \\(M\\). Así, para grupo de riesgo \\(g\\) y producto \\(r\\) \\[\nS_{g,r} = \\sum\\limits_{i=1}^{n_{g,r}} \\sum\\limits_{j=1}^{N_g} D\\left( X_{i,j} \\right)\n\\] El valor esperado y varianza de cada uno de estos reclamos totales por grupo de riesgo \\(g\\) y producto \\(r\\), están dados por: \\[\n\\begin{split}\n\\E\\left[ S_{g,r} \\right]\n& = n_{g,r} \\E\\left[ N_g \\right] \\E\\left[ D( X_g ) \\right] \\\\\n\\V\\left[ S_{g,r} \\right]\n& = n_{g,r} \\left( \\E\\left[ N_g \\right] \\V\\left[ D( X_g ) \\right] + \\V\\left[ N_g \\right] \\E\\left[ D( X_g ) \\right]^2 \\right)\n\\end{split}\n\\] El reclamo total por grupo de riesgo \\(g\\), tan solo resulta de la suma de los reclamos totales por cada producto. \\[\nS_g = \\sum\\limits_{r=1}^{R} S_{g,r}\n\\] de igual forma su valor esperado y varianza están dados por: \\[\n\\begin{split}\n\\E\\left[ S_g \\right] = \\sum\\limits_{r=1}^R \\E\\left[ S_{g,r} \\right]\n& = \\sum\\limits_{r=1}^R n_{g,r} \\E\\left[ N_g \\right] \\E\\left[ D( X_g ) \\right] \\\\\n\\V\\left[ S_{g} \\right] = \\sum\\limits_{r=1}^R \\V\\left[ S_{g,r} \\right]\n& = \\sum\\limits_{r=1}^R n_{g,r} \\left( \\E\\left[ N_g \\right] \\V\\left[ D( X_g ) \\right] + \\V\\left[ N_g \\right] \\E\\left[ D( X_g ) \\right]^2 \\right)\n\\end{split}\n\\]\n\n\nCode\nprods &lt;- data.table( ngr = c( 1000, 800, 600, 400, 200 ), \n                     d = c( 20, 20, 20, 20, 20 ), \n                     M = c( 2e4, 2e4, 2.5e4, 3e4, 4e4 ) )\nprods[ , id := 1 ]\ngrpcl[ , id := 1 ]\nprods &lt;- merge.data.table( prods, grpcl, by = 'id', allow.cartesian = TRUE )\ngrpcl[ , id := NULL ]\nprods[ , id := NULL ]\n\nprods &lt;- merge.data.table( prods, estim, by = 'grp', allow.cartesian = TRUE )\nsetorder( prods, sex, xg, -d )\nprods[ , r := 1 ]\nprods[ , r := cumsum( r ), by = list( sex, xg ) ]\nintf &lt;- function( meanlog, sdlog, d, M, k ) integrate( function( x ) ( x - d )^k * fd( x, meanlog = meanlog, sdlog = sdlog ), lower = d, upper = M, abs.tol = 1e-15 )$value\n\nprods[ , EDX := mapply( FUN = intf, meanlog, sdlog, d, M, 1 ) + M * ( 1 - Fd( M + d ) )]\nprods[ , EDX2 := mapply( FUN = intf, meanlog, sdlog, d, M, 2 ) + M^2 * ( 1 - Fd( M + d ) ) ]\nprods[ , SDDX := sqrt( EDX2 - EDX^2 ) ]\n\nprods[ , ES := ngr * EN * EDX ]\nprods[ , SDS := sqrt( ngr * ( EN * SDDX^2 + SDN^2 * EDX^2 ) ) ]\n\n\n\n\nCode\nprods[ , list( sex, xg, r, ngr, d, M, EN, SDN, EX, SDX, EDX, SDDX, ES, SDS ) ] %&gt;%\n  kable(\n    label = NA,\n    caption = 'Estimación de los reclamos totales',\n    row.names = FALSE,\n    col.names = c( 'sex', '$x_g$', '$r$', '$n_{g,r}$', '$d$', '$M$', '$\\\\E[ N ]$', '$\\\\sqrt{\\\\V[ N ]}$',\n                   '$\\\\E[ X ]$', '$\\\\sqrt{\\\\V[ X ]}$', '$\\\\E[ D(X) ]$', '$\\\\sqrt{\\\\V[ D(X) ]}$',\n                   '$\\\\E[ S_{g,r} ]$', '$\\\\sqrt{\\\\V[ S_{g,r} ]}$' ),\n    align = 'llrrrrrrrrrrr',\n    digits = c( 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2 ),\n    format.args = list( big.mark = ',', decimal.mark = '.', scientific = FALSE ),\n    escape = FALSE,\n    centering = TRUE,\n    booktabs = TRUE,\n    longtable = TRUE ) %&gt;%\n  kable_classic( \n    full_width = FALSE, \n    html_font = \"Cambria\", \n    position = 'center', \n    latex_options = c( 'hold_position', 'repeat_header', 'scale_down' ) ) %&gt;%\n  scroll_box( width = \"100%\", height = \"500px\" )\n\n\n\n\nEstimación de los reclamos totales\n\n\nsex\n$x_g$\n$r$\n$n_{g,r}$\n$d$\n$M$\n$\\E[ N ]$\n$\\sqrt{\\V[ N ]}$\n$\\E[ X ]$\n$\\sqrt{\\V[ X ]}$\n$\\E[ D(X) ]$\n$\\sqrt{\\V[ D(X) ]}$\n$\\E[ S_{g,r} ]$\n$\\sqrt{\\V[ S_{g,r} ]}$\n\n\n\n\nFEMALE\n[0,20)\n1\n1,000\n20\n20,000\n2.36\n8.99\n53.88\n35.16\n34.25\n34.76\n80,954.12\n9,886.63\n\n\nFEMALE\n[0,20)\n2\n800\n20\n20,000\n2.36\n8.99\n53.88\n35.16\n34.25\n34.76\n64,763.30\n8,842.87\n\n\nFEMALE\n[0,20)\n3\n600\n20\n25,000\n2.36\n8.99\n53.88\n35.16\n34.25\n34.76\n48,572.47\n7,658.15\n\n\nFEMALE\n[0,20)\n4\n400\n20\n30,000\n2.36\n8.99\n53.88\n35.16\n34.25\n34.76\n32,381.65\n6,252.86\n\n\nFEMALE\n[0,20)\n5\n200\n20\n40,000\n2.36\n8.99\n53.88\n35.16\n34.25\n34.76\n16,190.82\n4,421.44\n\n\nFEMALE\n[20,40)\n1\n1,000\n20\n20,000\n4.35\n16.94\n55.21\n37.81\n35.64\n37.37\n155,007.68\n19,254.60\n\n\nFEMALE\n[20,40)\n2\n800\n20\n20,000\n4.35\n16.94\n55.21\n37.81\n35.64\n37.37\n124,006.14\n17,221.83\n\n\nFEMALE\n[20,40)\n3\n600\n20\n25,000\n4.35\n16.94\n55.21\n37.81\n35.64\n37.37\n93,004.61\n14,914.55\n\n\nFEMALE\n[20,40)\n4\n400\n20\n30,000\n4.35\n16.94\n55.21\n37.81\n35.64\n37.37\n62,003.07\n12,177.68\n\n\nFEMALE\n[20,40)\n5\n200\n20\n40,000\n4.35\n16.94\n55.21\n37.81\n35.64\n37.37\n31,001.54\n8,610.92\n\n\nFEMALE\n[40,60)\n1\n1,000\n20\n20,000\n9.89\n27.12\n58.42\n42.10\n38.85\n41.66\n384,035.64\n33,568.72\n\n\nFEMALE\n[40,60)\n2\n800\n20\n20,000\n9.89\n27.12\n58.42\n42.10\n38.85\n41.66\n307,228.51\n30,024.78\n\n\nFEMALE\n[40,60)\n3\n600\n20\n25,000\n9.89\n27.12\n58.42\n42.10\n38.85\n41.66\n230,421.39\n26,002.22\n\n\nFEMALE\n[40,60)\n4\n400\n20\n30,000\n9.89\n27.12\n58.42\n42.10\n38.85\n41.66\n153,614.26\n21,230.72\n\n\nFEMALE\n[40,60)\n5\n200\n20\n40,000\n9.89\n27.12\n58.42\n42.10\n38.85\n41.66\n76,807.13\n15,012.39\n\n\nFEMALE\n[60,Inf]\n1\n1,000\n20\n20,000\n16.01\n39.95\n62.76\n47.87\n43.19\n47.45\n691,528.26\n54,891.01\n\n\nFEMALE\n[60,Inf]\n2\n800\n20\n20,000\n16.01\n39.95\n62.76\n47.87\n43.19\n47.45\n553,222.61\n49,096.01\n\n\nFEMALE\n[60,Inf]\n3\n600\n20\n25,000\n16.01\n39.95\n62.76\n47.87\n43.19\n47.45\n414,916.96\n42,518.40\n\n\nFEMALE\n[60,Inf]\n4\n400\n20\n30,000\n16.01\n39.95\n62.76\n47.87\n43.19\n47.45\n276,611.30\n34,716.12\n\n\nFEMALE\n[60,Inf]\n5\n200\n20\n40,000\n16.01\n39.95\n62.76\n47.87\n43.19\n47.45\n138,305.65\n24,548.01\n\n\nMALE\n[0,20)\n1\n1,000\n20\n20,000\n2.21\n9.10\n54.70\n36.96\n35.11\n36.53\n77,713.10\n10,250.89\n\n\nMALE\n[0,20)\n2\n800\n20\n20,000\n2.21\n9.10\n54.70\n36.96\n35.11\n36.53\n62,170.48\n9,168.68\n\n\nMALE\n[0,20)\n3\n600\n20\n25,000\n2.21\n9.10\n54.70\n36.96\n35.11\n36.53\n46,627.86\n7,940.31\n\n\nMALE\n[0,20)\n4\n400\n20\n30,000\n2.21\n9.10\n54.70\n36.96\n35.11\n36.53\n31,085.24\n6,483.23\n\n\nMALE\n[0,20)\n5\n200\n20\n40,000\n2.21\n9.10\n54.70\n36.96\n35.11\n36.53\n15,542.62\n4,584.34\n\n\nMALE\n[20,40)\n1\n1,000\n20\n20,000\n1.97\n8.86\n58.65\n44.14\n39.16\n43.65\n77,089.70\n11,140.40\n\n\nMALE\n[20,40)\n2\n800\n20\n20,000\n1.97\n8.86\n58.65\n44.14\n39.16\n43.65\n61,671.76\n9,964.28\n\n\nMALE\n[20,40)\n3\n600\n20\n25,000\n1.97\n8.86\n58.65\n44.14\n39.16\n43.65\n46,253.82\n8,629.32\n\n\nMALE\n[20,40)\n4\n400\n20\n30,000\n1.97\n8.86\n58.65\n44.14\n39.16\n43.65\n30,835.88\n7,045.81\n\n\nMALE\n[20,40)\n5\n200\n20\n40,000\n1.97\n8.86\n58.65\n44.14\n39.16\n43.65\n15,417.94\n4,982.14\n\n\nMALE\n[40,60)\n1\n1,000\n20\n20,000\n6.00\n18.64\n63.02\n50.59\n43.54\n50.11\n261,139.22\n25,962.48\n\n\nMALE\n[40,60)\n2\n800\n20\n20,000\n6.00\n18.64\n63.02\n50.59\n43.54\n50.11\n208,911.37\n23,221.55\n\n\nMALE\n[40,60)\n3\n600\n20\n25,000\n6.00\n18.64\n63.02\n50.59\n43.54\n50.11\n156,683.53\n20,110.45\n\n\nMALE\n[40,60)\n4\n400\n20\n30,000\n6.00\n18.64\n63.02\n50.59\n43.54\n50.11\n104,455.69\n16,420.12\n\n\nMALE\n[40,60)\n5\n200\n20\n40,000\n6.00\n18.64\n63.02\n50.59\n43.54\n50.11\n52,227.84\n11,610.77\n\n\nMALE\n[60,Inf]\n1\n1,000\n20\n20,000\n16.26\n38.61\n63.24\n47.57\n43.64\n47.18\n709,672.56\n53,625.70\n\n\nMALE\n[60,Inf]\n2\n800\n20\n20,000\n16.26\n38.61\n63.24\n47.57\n43.64\n47.18\n567,738.05\n47,964.28\n\n\nMALE\n[60,Inf]\n3\n600\n20\n25,000\n16.26\n38.61\n63.24\n47.57\n43.64\n47.18\n425,803.54\n41,538.29\n\n\nMALE\n[60,Inf]\n4\n400\n20\n30,000\n16.26\n38.61\n63.24\n47.57\n43.64\n47.18\n283,869.02\n33,915.87\n\n\nMALE\n[60,Inf]\n5\n200\n20\n40,000\n16.26\n38.61\n63.24\n47.57\n43.64\n47.18\n141,934.51\n23,982.14",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Modelos de pérdida agregada</span>"
    ]
  },
  {
    "objectID": "lectura_06.html#modelos_variables_explicativas",
    "href": "lectura_06.html#modelos_variables_explicativas",
    "title": "7  Modelos de pérdida agregada",
    "section": "7.10 Modelos con variables explicativas",
    "text": "7.10 Modelos con variables explicativas\nEn algunos casos más generales, donde la población presenta heterogeneidad respecto del riesgo al cual están expuestos, como también de las dimensiones de sus reclamos, se considera que existen variables aleatorias adicionales \\(Y_1, \\ldots, Y_n\\) que determinan el número de reclamos \\(N\\) y el valor de los siniestros \\(\\{X_i\\}\\). Es decir no hay independencia entre \\(\\{Y_j\\}\\) y \\(N\\) así como tampoco entre \\(\\{Y_j\\}\\) y \\(\\{X_i\\}\\). Es así que un modelo colectivo \\(S = \\sum\\limits_{i=1}^N X_i\\) su estudio, estimación y tarificación debe ser realizada de forma condicional respecto de las variables aleatorias explicativas \\(\\{Y_j\\}\\), \\[\n\\E[ S ]\n= \\E\\left[ \\E\\left[ S \\middle| Y_1, \\ldots, Y_n \\right] \\right]\n\\] de donde es necesario estimar cada una de las esperanzas condicionadas \\(\\E\\left[ S \\middle| Y_1, \\ldots, Y_n \\right]\\).\nEn la práctica las variables explicativas \\(\\{ Y_j \\}\\) suelen ser seleccionadas para caracterizar el perfil de riesgo de cada cliente. Para su selección se suele utilizar algunos criterios de tipo económico, financiero, legal y estadístico.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Modelos de pérdida agregada</span>"
    ]
  },
  {
    "objectID": "lectura_06.html#proceso_estocastico_reclamos",
    "href": "lectura_06.html#proceso_estocastico_reclamos",
    "title": "7  Modelos de pérdida agregada",
    "section": "7.11 Proceso estocástico de reclamos totales",
    "text": "7.11 Proceso estocástico de reclamos totales\nHasta el momento no hemos considerado que los reclamos se producen en el tiempo, que cada reclamo está bien vinculado al tiempo; es más podemos identificar algunos instantes en el tiempo que le corresponde, tenemos así el tiempo cuando se suscita el siniestro o tiempo de ocurrencia, el tiempo de aviso cuando se comunica al asegurador el siniestro y un tiempo de pago. Los dos últimos no son directos asociados al evento del siniestro sino están asociados a otras variables que pueden afectar la demora para comunicar un siniestro por parte del asegurado y la demora para cancelarlo por parte del asegurador.\nAsí por tanto si tenemos una secuencia de siniestros \\(\\{X_n\\}_{n\\in\\N^*}\\), estos los podemos considerar ordenados en el tiempo conforme han venido presentándose en instantes \\(0= T_0 \\leq T_1 \\leq \\cdots \\leq T_n \\leq \\cdots\\), es decir se le asocia a los reclamos \\(\\{X_n\\}\\) la secuencia de los tiempos de arribo \\(\\{T_n\\}_{n\\in\\N}\\), donde \\(T_{n+1} \\geq T_n\\), para cualquier \\(n \\in \\N\\). Ciertamente, es de esperar que los tiempos de arribo sean variables aleatorias y están asociados a la frecuencia \\(N\\) en un periodo dado, ya que \\(N\\) cuenta los tiempos de arribo hasta un instante dado \\(t \\geq 0\\), bajo esta perspectiva transformamos a \\(N\\) en una variable aleatoria dependiente del tiempo, en lo que se conoce como un proceso estocástico. \\[\nN( t )\n= \\#\\left\\{ n \\in \\N\\ \\middle|\\ T_n \\leq t \\right\\}\n= \\sum\\limits_{n=0}^{+\\infty} \\mathbf{1}_{(-\\infty,t]}\\left( T_n \\right)\n\\]\nInmediatamente de este razonamiento, resulta que los reclamos totales también se transforman en proceso estocástico, conforme se van considerando los arribos de los reclamos. \\[\nS( t ) = \\sum\\limits_{i=1}^{N( t )} X_i\n\\] donde claramente, como hemos venido haciéndolo, si \\(N(t) = 0\\), entonces la suma total de reclamos es cero, \\(S(t) = 0\\).\nAdemás, junto con lo anterior, se pueden identificar las variables aleatorias de tiempos entre arribos, dadas por las diferencias entre tiempos de arribo contiguos \\[\nW_n = T_n - T_{n-1},\\quad \\forall n \\in \\N^*\n\\]\nUn hipótesis bien común en el modelamiento de un proceso de pérdida agregada utilizando esta aproximación es asumir que los tiempos entre arribo \\(\\{W_n\\}\\) son independientes entre si, es decir, el momento que se producen cada uno de los siniestros es de forma independiente. También se suele suponer independencia entre los tiempos entre arribo \\(\\{W_n\\}\\) y las variables que representan la magnitud de cada uno de los reclamos \\(\\{X_n\\}\\).\nLos resultados para el cálculo de la esperanza y la varianza se pueden obtener de forma similar a lo que realizamos en la sección @ref(modelo_individual). \\[\n\\begin{split}\n\\E[S(t)]\n& = \\E\\left[ \\sum\\limits_{i=1}^{N(t)} X_i \\right] \\\\\n& = \\sum\\limits_{n=0}^{+\\infty} \\E\\left[ \\sum\\limits_{i=1}^n X_i\\ \\middle|\\ N(t) = n \\right]P( N(t) = n )\n\\quad \\text{utilizando la esperanza condicional} \\\\\n& = 0 P( N(t) = 0 ) + \\sum\\limits_{n=1}^{+\\infty}\n\\E\\left[ \\sum\\limits_{i=1}^n X_i\\ \\middle|\\ N(t) = n \\right]P( N(t) = n ) \\\\\n& = \\sum\\limits_{n=1}^{+\\infty} \\sum\\limits_{i=1}^n \\E\\left[ X_i \\mid N(t) = n \\right]P( N(t) = n )\n\\quad \\text{linealidad de la esperanza} \\\\\n& = \\sum\\limits_{n=0}^{+\\infty} n \\E\\left[ X \\mid N(t) = n \\right]P( N(t) = n )\n\\quad \\text{si $\\{X_i\\}$ son idénticamente distribuidas} \\\\\n& = \\E\\left[ X \\right] \\sum\\limits_{n=0}^{+\\infty} n P( N(t) = n )\n\\quad \\text{si $X$ y $N(t)$ son independientes} \\\\\n& = \\E[ N(t) ] \\E[ X ]\n\\end{split}\n\\]\nel segundo momento de \\(S(t)\\) se calcula de forma similar \\[\n\\begin{split}\n\\E[S(t)^2]\n& = \\E\\left[ \\left( \\sum\\limits_{i=1}^{N(t)} X_i \\right)^2 \\right] \\\\\n& = \\sum\\limits_{n=0}^\\infty \\E\\left[ \\sum\\limits_{i, j=1}^n X_i X_j \\middle| N(t) = n \\right] P( N = n )\n\\quad \\text{propiedades de la esperanza condicional} \\\\\n& = \\sum\\limits_{n=0}^\\infty \\sum\\limits_{i, j=1}^n \\E\\left[ X_i X_j \\right] P( N(t) = n )\n\\quad \\text{si $\\{X_i\\}$ y $N(t)$ son independientes} \\\\\n& = \\sum\\limits_{n=0}^\\infty \\left( \\sum\\limits_{i=1}^n \\E\\left[ X_i^2 \\right]\n+ \\sum\\limits_{i,j=1,i\\neq j}^n \\E\\left[ X_i X_j \\right] \\right) P( N(t) = n ) \\\\\n& = \\sum\\limits_{n=0}^\\infty \\left( n \\E\\left[ X^2 \\right] + n(n-1) \\E\\left[ X \\right]^2 \\right) P( N(t) = n )\n\\quad \\text{si $\\{X_i\\}$ son i.i.d} \\\\\n& = \\E\\left[N(t)\\right] \\E\\left[X^2\\right] + \\E\\left[N(t)^2\\right] \\E\\left[X\\right]^2\n- \\E\\left[N(t)\\right] \\E\\left[X\\right]^2 \\\\\n& = \\E\\left[N(t)\\right] \\V\\left[X\\right] + \\E\\left[N(t)^2\\right] \\E\\left[X\\right]^2\n\\end{split}\n\\]\ny finalmente la varianza de \\(S(t)\\) \\[\n\\begin{split}\n\\V\\left[S(t)\\right]\n& = \\E\\left[S(t)^2\\right] - \\E\\left[S(t)\\right]^2 \\\\\n& = \\E\\left[N(t)\\right] \\V\\left[X\\right]\n+ \\E\\left[N(t)^2\\right] \\E\\left[X\\right]^2 - \\E\\left[N(t)\\right]^2 \\E\\left[X\\right]^2 \\\\\n& = \\E\\left[N(t)\\right] \\V\\left[X\\right] + \\V\\left[N(t)\\right] \\E\\left[X\\right]^2\n\\end{split}\n\\] Si trabajamos bajo la hipótesis que los tiempos entre arribos \\(\\{W_n\\}\\) son i.i.d, con distribución de probabilidad común \\(F_W\\). Podemos comprender y estudiar de una forma un poco más sencilla el proceso estocástico atado a la frecuencia de los siniestros \\(N\\).\nNotemos primeramente que se tienen la siguiente igualdad entre los eventos \\[\n\\{N(t) = n\\}\n= \\{T_n \\leq \\land T_{n+1} &gt; t\\}\n= \\{T_n \\leq t \\} \\cap \\{ T_{n+1} \\leq t\\}^c\n= \\{T_n \\leq t \\} \\setminus \\{ T_{n+1} \\leq t\\}\n\\] además es de observar que \\(\\{ T_{n+1} \\leq t\\} \\subset \\{ T_{n} \\leq t\\}\\). Con todo esto en consideración para cualquier \\(n &gt; 0\\). \\[\n\\begin{split}\nP( N(t) = n )\n& = P( T_n \\leq t \\land T_{n+1} &gt; t ) \\\\\n& = P( \\{T_n \\leq t \\} \\setminus \\{ T_{n+1} \\leq t\\} ) \\\\\n& = P( T_n \\leq t ) - P( T_{n+1} \\leq t ) \\\\\n& = F_W^{\\star n}( t ) - F_W^{\\star n + 1}( t )\n\\end{split}\n\\]\nel caso \\(n = 0\\) también es sencillo \\[\n\\begin{split}\nP( N(t) = 0 )\n& = P( T_0 \\leq t \\land T_1 &gt; t ) \\\\\n& = P( \\{T_0 \\leq t \\} \\setminus \\{ T_1 \\leq t\\} ) \\\\\n& = P( T_0 \\leq t ) - P( T_1 \\leq t ) \\\\\n& = 1 - F_W^{\\star 1}( t ) \\\\\n& = F_W^{\\star 0}( t ) - F_W^{\\star 1}( t )\n\\end{split}\n\\]\nDe lo anterior resulta que la esperanza de \\(N(t)\\), está dada por la siguiente serie. \\[\n\\begin{split}\n\\E[ N( t ) ]\n& = \\sum\\limits_{n=0}^{+\\infty} n P( N(t) = n ) \\\\\n& = \\sum\\limits_{n=0}^{+\\infty} n \\left( F_W^{\\star n}( t ) - F_W^{\\star n + 1}( t ) \\right) \\\\\n& = 0\\left( F_W^{\\star 0}( t ) - F_W^{\\star 1}( t ) \\right)\n+ 1\\left( F_W^{\\star 1}( t ) - F_W^{\\star 2}( t ) \\right)\n+ 2\\left( F_W^{\\star 2}( t ) - F_W^{\\star 3}( t ) \\right)\n+ \\cdots \\\\\n& = \\sum\\limits_{n=1}^{+\\infty} F_W^{\\star n}( t )\n\\end{split}\n\\]\n\n7.11.1 Algoritmo de simulación\n\nSe selecciona el número adecuado de simulaciones \\(m \\in \\N^*\\),\nSe selecciona el número de simulaciones de tiempos de arribo \\(M \\in \\N^*\\),\nDada la distribución de los tiempos entre arribos \\(F_W\\) para cada simulación \\(i \\in \\{1, \\ldots, m\\}\\) se genera una muestra \\(W_{i,1}, \\ldots, W_{i,M}\\) de tamaño de \\(M\\),\nPara cada simulación \\(i \\in \\{1, \\ldots, m\\}\\) y cada \\(n \\in \\{0, \\ldots, M\\}\\) calculamos los tiempos de arribo \\(T_{i,0},\\ldots, T_{i,M}\\) \\[\nT_{i,0} = 0,\\qquad\nT_{i,n} = \\sum\\limits_{k=1}^n W_{i,k}\n\\]\nSe toma una malla de discretización en el tiempo \\(0 = t_0 &lt; t_1 &lt; \\cdots &lt; t_p = T\\) y calculamos para cada simulación \\(i \\in \\{1,\\ldots,m\\}\\) y tiempo \\(t_l\\), con \\(l \\in \\{0,\\ldots,p\\}\\). \\[\nN_{i,l} = \\#\\left\\{ n \\in \\N\\ \\middle|\\ T_{i,n} \\leq t_l \\right\\}\n= \\sum\\limits_{n=0}^{M} \\mathbf{1}_{(-\\infty,t_l]}\\left( T_n \\right)\n\\] precisamente para cada simulación \\(i\\), el término \\(N_{i,l}\\) es una aproximación al proceso \\(N( t_l )\\) evaluado en el tiempo \\(t_l\\). Así se puede tener la aproximación de la media \\[\n\\E\\left[ N( t_l ) \\right] \\approx \\frac{1}{m} \\sum\\limits_{i=1}^m N_{i,l}\n\\]\nCon la simulación el proceso estocástico \\(N\\) podemos ahora proceder a simular los reclamos totales. Para cada simulación \\(i \\in \\{1,\\ldots,m\\}\\), tomamos una muestra de forma independiente e igualmente distribuida de los reclamos individuales \\(X_{i,1}, \\ldots, X_{i,N_{i,p}}\\), la cual tiene tamaño \\(N_{i,p}\\), y son tomados de la distribución de probabilidad \\(F_X\\)\nPara cada simulación \\(i \\in \\{1,\\ldots, m\\}\\) y cada tiempo \\(t_l\\) con \\(l \\in \\{0,\\ldots,p\\}\\), calculamos la suma \\[\nS_{i,l} = \\sum\\limits_{j=1}^{N_{i,l}} X_{i,j}\n\\] cada \\(S_{i,l}\\) es una aproximación al proceso del total de reclamos \\(S(t_l)\\) precisamente en el tiempo \\(t_l\\). Así resulta la aproximación \\[\n\\E\\left[ S( t_l ) \\right] \\approx \\frac{1}{m} \\sum\\limits_{i=1}^m S_{i,l}\n\\]\nAdemás, para cada instante \\(t_l\\), se puede aproximar la función de distribución acumulada \\(F_{S(t_l)}\\) de la variable aleatoria \\(S( t_l )\\) utilizando la distribución acumulada empírica generada con la muestra \\(S_{1,l},\\ldots,S_{m,l}\\). Para cualquier \\(s \\in \\R\\), tenemos la aproximación \\[\nF_{S(t_l)}( s ) \\approx F_{m,l}( s )\n= \\frac{1}{m} \\sum\\limits_{i=1}^m \\mathbf{1}_{(-\\infty,s]}\\left( S_{i,l} \\right)\n\\] El algoritmo de simulación anterior tiene una falencia, no se sabe a priori que valor \\(M\\) se debe escoger para simular de forma correcta los procesos \\(N\\) y \\(S\\) hasta un tiempo máximo \\(T\\) dado. Esto implica que si \\(M\\) no es del tamaño adecuado, las estimaciones que se realice utilizando la simulación va a degradarse conforme avanza el tiempo.\n\n\nEl siguiente ejemplo es de especial atención ya que es utilizado en muchas aplicaciones, podemos considerar el caso donde el tiempo entre arribos \\(\\{W_n\\}\\) es i.i.d y con distribución de probabilidad común \\(F_W\\) dada por una ley exponencial \\(Exp( \\lambda )\\). Un resultado clásico muestra que la suma de \\(n\\) exponenciales sigue una ley gamma \\(Gamma(n,\\lambda)\\), esto implica que \\(T_n = W_1 + \\cdots + W_n \\rightsquigarrow Gamma( n, \\lambda )\\) lo que quiere decir que la convolución \\(F_W^{\\star n}\\) es una \\(Gamma( n, \\lambda )\\). Así por tanto, el proceso estocástico de conteo \\(N(t) = \\#\\left\\{ n \\in \\N\\ \\middle|\\ T_n \\leq t \\right\\}\\) tiene las siguientes probabilidades \\[\n\\begin{split}\nP( N( t ) = n )\n& = F_W^{\\star n}( t ) - F_W^{\\star n + 1}( t ) \\\\\n& = \\exp( -\\lambda t ) \\sum\\limits_{k=n}^{+\\infty} \\frac{(\\lambda t)^k}{k!}\n- \\exp( -\\lambda t ) \\sum\\limits_{k={n+1}}^{+\\infty} \\frac{(\\lambda t)^k}{k!} \\\\\n& = \\exp( -\\lambda t ) \\frac{(\\lambda t)^n}{n!}\n\\end{split}\n\\]\nEn conclusión \\(N(t)\\) tiene como distribución una ley de Poisson \\(Pois( \\lambda t )\\), en particular se conoce a \\(N\\) como un proceso de Poisson.\n\n\n\nCode\nm &lt;- 1000\nlambda &lt;- 10\nM &lt;- 30\np &lt;- 365\nu &lt;- 4\ns &lt;- 2\n\nset.seed( 432147 )\nW &lt;- lapply( 1:m, FUN = function( n ) rexp( M, rate = lambda ) )\nTn &lt;- lapply( W, FUN = cumsum )\nTnmax &lt;- max( sapply( Tn, FUN = max ) )\nt &lt;- seq( 0, Tnmax, length = p )\n\nN &lt;- lapply( Tn, FUN = function( Tk ) sapply( t, FUN = function( t ) sum( Tk &lt;= t ) ) )\nNmax &lt;- max( sapply( N, FUN = function( Ni ) last( Ni ) ) )\n\nset.seed( 7324312 )\nX &lt;- lapply( N, FUN = function( Ni ) rlnorm( last( Ni ), meanlog = u, sdlog = s ) )\nS &lt;- lapply( 1:m, FUN = function( i ) {\n  Si &lt;- cumsum( X[[ i ]] )\n  return( sapply( N[[ i ]], FUN = function( Nj ) ifelse( Nj == 0, 0, Si[ Nj ] ) ) )\n} )\nSmax &lt;- max( sapply( S, FUN = function( Si ) last( Si ) ) )\n\nEN &lt;- lambda * t\nEeN &lt;- sapply( 1:p, FUN = function( l ) mean( sapply( 1:m, function( i ) N[[ i ]][ l ] ) ) )\n\nES &lt;- EN * exp( u + 0.5 * s^2 )\nEeS &lt;- sapply( 1:p, FUN = function( l ) mean( sapply( 1:m, function( i ) S[[ i ]][ l ] ) ) )\n\n\nEn el gráfico a continuación presentamos todos los procesos de conteo \\(N_{i}\\) simulados, como es de esperarse son funciones de escalera, siempre crecientes, que toman solo valores positivos.\n\n\nCode\nSord &lt;- order( sapply( 1:m, FUN = function( j ) last( S[[j]] ) ) )\nNdat &lt;- NULL\nfor ( j in 1:m ) {\n  Ndat &lt;- rbindlist( list( Ndat, data.table( t = t, m = as.factor( Sord[ j ] ), N = N[[ j ]], S = S[[ j ]] ) ) )\n}\ncols &lt;- wes_palette( m, name = \"FantasticFox1\", type = \"continuous\")\n\ntime &lt;- t \n\nplt &lt;- ggplot( ) +\n  geom_step( data = Ndat, aes( x = t, y = N, group = m, colour = m ), linewidth = 0.2 ) +\n  geom_line( aes( x = time, y = EN, colour = 'a' ), linewidth = 1.5 ) +\n  geom_line( aes( x = time, y = EeN, colour = 'b' ), linewidth = 1 ) +\n  scale_color_manual( breaks = c( as.factor( 1:m ), 'a', 'b' ), \n                      values = c( cols, 'dodgerblue3', 'olivedrab3' ) ) +\n  scale_x_continuous( breaks = seq( 0, Tnmax, length = 11 ),\n                      labels = formatC( seq( 0, Tnmax, length = 11 ), digits = 2, format = 'f' ), \n                      limits = c( 0, Tnmax ), \n                      expand = c( 0, 0 ) ) +\n  scale_y_continuous( breaks = seq( 0, Nmax, length = 11 ), \n                      labels = formatC( seq( 0, Nmax, length = 11 ), digits = 2, format = 'f' ), \n                      limits = c( 0, Nmax ), \n                      expand = c( 0, 0 ) ) +\n  xlab( TeX( \"$t$\" ) ) + \n  ylab( TeX( \"$N(t)$\" ) ) + \n  theme_bw() +\n  theme( legend.position = \"none\" )\nplot( plt )\n\n\n\n\n\n\n\n\n\nEl gráfico a continuación presenta la simulación de los procesos correspondientes al total de reclamos, conforme avanza el tiempo. De igual manera los procesos \\(S_i\\) son funciones de escalera, siempre crecientes, con saltos correspondientes a los reclamos individuales.\n\n\nCode\nSqmax &lt;- quantile( sapply( S, FUN = function( Si ) last( Si ) ), probs = 0.99 )\n\nplt &lt;- ggplot( ) +\n  geom_step( data = Ndat, aes( x = t, y = S, group = m, colour = m ), linewidth = 0.2 ) +\n  geom_line( aes( x = time, y = ES, colour = 'a' ), linewidth = 1.5 ) +\n  geom_line( aes( x = time, y = EeS, colour = 'b' ), linewidth = 1 ) +\n  scale_color_manual( breaks = c( as.factor( 1:m ), 'a', 'b' ), \n                      values = c( cols, 'dodgerblue3', 'olivedrab3' ) ) +\n  scale_x_continuous( breaks = seq( 0, Tnmax, length = 11 ),\n                      labels = formatC( seq( 0, Tnmax, length = 11 ), digits = 2, format = 'f' ), \n                      limits = c( 0, Tnmax ), expand = c( 0, 0 ) ) +\n  scale_y_continuous( breaks = seq( 0, Sqmax, length = 11 ),\n                      labels = formatC( seq( 0, Sqmax, length = 11 ), digits = 2, format = 'f' ), \n                      limits = c( 0, Sqmax ), expand = c( 0, 0 ) ) +\n  xlab( TeX( \"$t$\" ) ) + \n  ylab( TeX( \"$S(t)$\" ) ) + \n  theme_bw() +\n  theme( legend.position = \"none\" )\nplot( plt )\n\n\n\n\n\n\n\n\n\nComo se describe en el paso 8 del algoritmo de simulación, para cada tiempo \\(t_l\\) se puede estimar la distribución de probabilidad de la variable aleatoria \\(S(t_l)\\).\n\n\nCode\nFeS &lt;- lapply( 1:p, FUN = function( l ) ecdf( sapply( S, FUN = function( Si ) Si[ l ] ) ) )\nns &lt;- 1e3\ns &lt;- seq( 0, Smax, length = ns )\nFes &lt;- lapply( FeS, FUN = function( FeSl ) sapply( s, FUN = FeSl ) )\n\n\nLa figura a continuación muestra cada una de las distribuciones de probabilidad empíricas estimadas \\(F_{m,l}\\). Se puede observar que conforme avanza el tiempo, aumenta la probabilidad de observar valores mayores del proceso \\(S\\) correspondiente al total de reclamos.\n\n\nCode\nFeSdat &lt;- NULL\nfor ( j in 1:p ) {\n  FeSdat &lt;- rbindlist( list( FeSdat, data.table( s = s, m = as.factor( j ), Fes = Fes[[ j ]] ) ) )\n}\ncols &lt;- wes_palette( p, name = \"FantasticFox1\", type = \"continuous\")\n\nplt &lt;- ggplot( ) +\n  geom_step( data = FeSdat, aes( x = s, y = Fes, group = m, colour = m ), linewidth = 0.1 ) +\n  scale_color_manual( values = c( cols ) ) +\n  scale_x_continuous( breaks = seq( 0, Smax, length = 5 ),\n                      labels = formatC( seq( 0, Smax, length = 5 ), digits = 2, format = 'f' ), \n                      limits = c( 0, Smax ), \n                      expand = c( 0, 0 ) ) +\n  scale_y_continuous( breaks = seq( 0, 1, length = 11 ),\n                      labels = formatC( seq( 0, 1, length = 11 ), digits = 2, format = 'f' ), \n                      limits = c( 0, 1 ), \n                      expand = c( 0.005, 0.005 ) ) +\n  xlab( TeX( \"$S$\" ) ) + \n  ylab( TeX( \"$F_{m,l}$\" ) ) + \n  theme_bw() +\n  theme( legend.position = \"none\" )\n\nplot( plt )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1. A. Klugman S, H. Panjer H, E. Willmot G (2012) Loss Models, From Data to Decisions, 4th ed. John Wiley & Sons, Inc, Hoboken, New Jersey, United States\n\n\n2. A. Klugman S, H. Panjer H, E. Willmot G (2013) Loss Models, Further Topics, 1st ed. John Wiley & Sons, Inc, Hoboken, New Jersey, United States\n\n\n3. Arbenz P, Embrechts P, Puccetti G (2011) The AEP algorithm for the fast computation of the distribution of the sum of dependent random variables. Bernoulli 17. doi: 10.3150/10-bej284\n\n\n4. Arbenz P, Embrechts P, Puccetti G (2012) The GAEP algorithm for the fast computation of the distribution of a function of dependent random variables. Stochastics: An International Journal of Probability and Stochastic Processes 84",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Modelos de pérdida agregada</span>"
    ]
  },
  {
    "objectID": "lectura_07.html",
    "href": "lectura_07.html",
    "title": "8  Tarificación",
    "section": "",
    "text": "8.1 Medidas de riesgo\nUna vez estimado los reclamos totales se trata de establecer una forma optima para estimar el valor que deberá pagar el asegurado, la prima, para que la compañia de seguros se haga cargo de riesgo. En esto debe incluirse algunas consideraciones adicionales, además de la estimación de la frecuencia y severidad de los reclamos, es también relevante tener en consideración otros valores asociados a diferentes costo, el retorno esperado por el inversionista y finalmente el rendimiento de inversiones. Todo lo anterior debe ser conjugado para obtener una adecuada estimación de la prima.\nAdicionalmente, se debe tener en cuenta que el riesgo cubierto está sujeto a variaciones propias de la aleatoriedad del fenómeno y que puede presentar variaciones significativas respecto de la media, es decir, que la volatilidad del riesgo cubierto puede influenciar significativamente el pago total de los reclamos. Ante ello surge un concepto de vital importancia, el cual lo desarrollamos a continuación y se lo conoce como medidas de riesgo.\nEn lo que continúa citamos algunas de las medidas de riesgo usualmente utilizadas.\nPor otra parte, \\(\\VaR_{\\alpha}\\) para cualquier \\(\\alpha\\) no es una medida de riesgo sub-aditiva.\nEl método directo para el calcular el \\(\\VaR_\\alpha\\) es directamente atacando el problema de encontrar el ínfimo, esto es un problema de optimización que puede tener sus complicaciones, ya que muchas de las veces la distribución acumulada \\(F\\) no es conocida o fácilmente computable. Incluso en el caso cuando la función de distribución \\(F\\) es continua, resta el problema de poder invertir esta función.\nOtra forma es acudir a los métodos de simulación estocástica, partiendo de una muestra \\(X_1, \\ldots, X_n\\) y calculando de forma aproximada \\[\n\\VaR_{\\alpha}( X ) \\approx F_n^{-1}( \\alpha )\n\\] En principio no sabemos si esta aproximación será consistente conforme se aumenta el tamaño de la muestra, sin embargo para ello tenemos la siguiente proposición, la cual nos da una certeza al respecto.\nAlgoritmo de simulación\nCon lo anterior se puede crear un algoritmo de simulación estocástico que precisamente contemple el respectivo nivel de convergencia, de ahí que la demostración de la proposición anterior no es inútil, ya que de la misma resulta un criterio para determinar el número de simulaciones que se desea calcular.\nFijamos un valor \\(\\delta &gt; 0\\) el cual acota la probabilidad del evento que mide el error de aproximación, i.e. \\(P\\left( \\left| \\xi_{n,\\alpha} - \\xi_{\\alpha} \\right| &gt; \\varepsilon \\right) \\leq \\delta\\) y tomando en cuenta la desigualdad en 8.4. \\[\n\\begin{split}\n2 C e^{-2n \\min\\left( F( \\xi_{\\alpha} + \\varepsilon ) - \\alpha, \\alpha - F( \\xi_{\\alpha} - \\varepsilon ) \\right)^2 } & \\leq \\delta \\\\\n-2n \\min\\left( F( \\xi_{\\alpha} + \\varepsilon ) - \\alpha, \\alpha - F( \\xi_{\\alpha} - \\varepsilon ) \\right)^2\n& \\leq \\log( \\delta )-\\log(2) - \\log( C ) \\\\\nn & \\geq -\\frac{\\log( \\delta )-\\log(2)-\\log( C )}{2 \\min\\left( F( \\xi_{\\alpha} + \\varepsilon ) - \\alpha, \\alpha - F( \\xi_{\\alpha} - \\varepsilon ) \\right)^2}\n\\end{split}\n\\] Como se evidencia \\(n\\) crece en orden cuadrático proporcional a la aproximación dada por \\(\\min\\left( F( \\xi_{\\alpha} + \\varepsilon ) - \\alpha, \\alpha - F( \\xi_{\\alpha} - \\varepsilon ) \\right)\\) alrededor del percentile \\(\\alpha\\). Lo cual no es un resultado muy alentador.\nSi se establece un valor \\(\\widetilde{\\varepsilon} &gt; 0\\) para el término \\(\\widetilde{\\varepsilon} \\leq \\min\\left( F( \\xi_{\\alpha} + \\varepsilon ) - \\alpha, \\alpha - F( \\xi_{\\alpha} - \\varepsilon ) \\right)\\), la expresión anterior se modifica a la siguiente \\[\nn \\geq -\\frac{\\log( \\delta )-\\log(2)-\\log(C)}{2 \\widetilde{\\varepsilon}^2}\n\\] Claramente la cantidad de simulaciones que se debe realizar para lograr una aproximación de orden \\(\\varepsilon\\), se requiere un orden de simulaciones cuadrático inverso a este valor. El ejemplo numérico a continuación es aleccionador al respecto de la dificultad numérica de aproximar un cálculo de \\(VaR_\\alpha\\).\nEntonces los pasos del pseudo-algoritmo de simulación estarán dados por:\nEsta simulación solo genera una sola estimación del \\(\\VaR_{\\alpha}(X)\\), se puede repetir algunas veces esta algoritmo para tener más valores y sacar la media de estos como estimador.\nCode\nu &lt;- 7\ns &lt;- 4\nFd &lt;- function( x ) plnorm( x, meanlog = u, sdlog = s )\nFdi &lt;- function( x ) qlnorm( x, meanlog = u, sdlog = s )\nRd &lt;- function( x ) rlnorm( n = n, meanlog = u, sdlog = s )\nalpha &lt;- 0.98\ndelta &lt;- 1e-2\nhepsilon &lt;- 0.2e-4\n\nq &lt;- Fdi( alpha )\nepsilon &lt;- max( Fdi( hepsilon + alpha ) - q, q - Fdi( alpha - hepsilon ) )\nC &lt;- 5.001e-3 # seleccionado acorde a cada problema\nn &lt;- round( -( log( delta ) - log( 2 ) - log( C ) ) / ( 2 * hepsilon^2 ), 0 )\n\nm &lt;- 1e3\nqn &lt;- sapply( 1:m, FUN = function( i ) as.numeric( quantile( Rd( n ), probs = alpha ) ) )\n\nP &lt;- mean( abs( qn - q ) &gt; epsilon )\n\\[\nP\\left( \\left| \\xi_{n,\\alpha}- \\xi_{\\alpha} \\right| &gt; \\varepsilon \\right) =\n0.93100000\n\\] Como se observa se tendrá que realizar muchas simulaciones más para llegar a tener en probabilidad un error de aproximación adecuado.\nEs de notar que si se toma una variable aleatoria uniforme \\(U \\rightsquigarrow U(\\alpha,1)\\), tenemos que el valor en riesgo en la cola, puede ser expresado como la esperanza. \\[\n\\TVaR_{\\alpha}( X ) = \\E\\left[ \\VaR_U( X ) \\right]\n\\] De ahí que se una forma de calcular el valor en riesgo en la cola es utilizando una simulación estocástica que aproveche el 5.27. Así se puede generar una muestra \\(U_1, \\ldots, U_m\\) de la distribución uniforme \\(U(\\alpha,1)\\) y para cada uno de estos valores se calcula por simulación el \\(\\xi_{n,U_1} \\approx \\VaR_{U_1}( X ), \\ldots, \\xi_{n,U_n} \\approx  \\VaR_{U_n}( X )\\), luego aproximar utilizando la media empírica \\[\n\\TVaR_{\\alpha}( X ) \\approx \\frac{1}{m} \\sum\\limits_{i=1}^m \\xi_{n,U_i}\n\\] Pero como ya se mencionó, esta aproximación puede ser realmente costosa en términos computacionales; por tal razón, se requiere utilizar métodos numéricos más eficientes.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tarificación</span>"
    ]
  },
  {
    "objectID": "lectura_07.html#medidas_riesgo",
    "href": "lectura_07.html#medidas_riesgo",
    "title": "8  Tarificación",
    "section": "",
    "text": "Definition 8.1: Medida de riesgo coherente\nUna medida de riesgo coeherente es una función \\(\\rho: \\R \\longrightarrow \\R\\), que satisface la siguientes propiedades:\n\nNormalización, el riesgo de nada es nada \\[\n\\rho( 0 ) = 0\n\\]\nHomogenidad positiva, el riesgo crece con la escala, así para cualquier \\(a &gt; 0\\) \\[\n\\rho( a X ) = a \\rho( X )\n\\]\nInvarianza ante las traslaciones, para cualquier \\(a &gt; 0\\) \\[\n\\rho( \\alpha X + a ) = \\rho( \\alpha X ) + a\n\\]\nMonotonicidad, Si \\(X \\leq Y\\) \\[\n\\rho( X ) \\leq \\rho( Y )\n\\]\nSub-aditividad, el diversificar el riesgo es menor a la suma de los riesgos independientes \\[\n\\rho( X + Y ) \\leq \\rho( X ) + \\rho( Y )\n\\]\n\n\n\n\n\nDefinition 8.2: Valor en riesgoDada una variable aleatoria a valores reales \\(X\\), el valor en riesgo (value at risk) de \\(X\\) al nivel de probabilidad \\(\\alpha \\in (0,1)\\) está dado por \\[\n\\VaR_{\\alpha}( X ) = \\inf\\left\\{ x \\in \\R \\middle| F_X( x ) &gt; \\alpha \\right\\}\n\\]\n\n\n\nProposition 8.3Si la función de distribución acumulada \\(F_X\\) es continua, entonces \\(\\VaR_{\\alpha}( X ) = F_X^{-1}( \\alpha )\\).\n\n\n\n\n\n\nProposition 8.4Si consideramos una familia de variables aleatorias \\(\\{X_n\\}_{n\\in \\N}\\) i.i.d con distribución común \\(F\\) no necesariamente continua, entonces el percentil empírico de orden \\(\\alpha\\), \\(\\xi_{n,\\alpha} = F_n^{-1}( \\alpha )\\), que se puede extraer a partir de las \\(n\\) primeras variables en la muestra converge en probabilidad al valor \\(\\xi_{\\alpha} = \\VaR_{\\alpha}( X )\\), correspondiente al valor en riesgo.\n\n\n\nProof. Utilizando la versión unilateral de la desigualdad 5.32, podemos establecer las siguientes acotaciones \\[\n\\begin{split}\nP\\left( \\xi_{n,\\alpha}- \\xi_{\\alpha} &gt; \\varepsilon \\right)\n& = P\\left( \\xi_{n,\\alpha} &gt;  \\xi_{\\alpha} + \\varepsilon \\right) \\\\\n& = P\\left( F_n( \\xi_{n,\\alpha} ) &gt;  F_n( \\xi_{\\alpha} + \\varepsilon ) \\right) \\\\\n& = P\\left( F( \\xi_{\\alpha} + \\varepsilon ) + \\alpha &gt; F( \\xi_{\\alpha} + \\varepsilon ) + F_n( \\xi_{\\alpha} + \\varepsilon ) \\right) \\\\\n& = P\\left( F( \\xi_{\\alpha} + \\varepsilon ) - F_n( \\xi_{\\alpha} + \\varepsilon ) &gt; F( \\xi_{\\alpha} + \\varepsilon ) - \\alpha \\right) \\\\\n& \\leq P\\left( \\underset{x \\in \\R}{\\sup} \\left( F - F_n \\right) &gt; F( \\xi_{\\alpha} + \\varepsilon ) - \\alpha \\right) \\\\\n& \\leq Ce^{-2n \\left( F( \\xi_{\\alpha} + \\varepsilon ) - \\alpha \\right) ^2 }\n\\end{split}\n\\]\n\\[\n\\begin{split}\nP\\left( \\xi_{n,\\alpha} - \\xi_{\\alpha} &lt; -\\varepsilon \\right)\n& = P\\left( \\xi_{n,\\alpha} &lt;  \\xi_{\\alpha} - \\varepsilon \\right) \\\\\n& = P\\left( F_n( \\xi_{n,\\alpha} ) &lt;  F_n( \\xi_{\\alpha} - \\varepsilon ) \\right) \\\\\n& = P\\left( F( \\xi_{\\alpha} - \\varepsilon ) + \\alpha &lt; F( \\xi_{\\alpha} - \\varepsilon ) + F_n( \\xi_{\\alpha} - \\varepsilon ) \\right) \\\\\n& = P\\left( F_n( \\xi_{\\alpha} - \\varepsilon ) - F( \\xi_{\\alpha} - \\varepsilon ) &gt; \\alpha - F( \\xi_{\\alpha} - \\varepsilon ) \\right) \\\\\n& \\leq P\\left( \\underset{x \\in \\R}{\\sup} \\left( F - F_n \\right) &gt; \\alpha - F( \\xi_{\\alpha} - \\varepsilon ) \\right) \\\\\n& \\leq Ce^{-2n \\left( \\alpha - F( \\xi_{\\alpha} - \\varepsilon ) \\right) ^2 }\n\\end{split}\n\\] Uniendo ambos resultados concluimos \\[\nP\\left( \\left| \\xi_{n,\\alpha}- \\xi_{\\alpha} \\right| &gt; \\varepsilon \\right)\n\\leq 2 C e^{-2n \\min\\left( F( \\xi_{\\alpha} + \\varepsilon ) - \\alpha, \\alpha - F( \\xi_{\\alpha} - \\varepsilon ) \\right)^2 }\n\\] Podemos concluir con toda confianza que el percentil \\(\\xi_{n,\\alpha}\\) que se obtiene de la distribución empírica se aproxima en probabilidad al percentil \\(\\xi_{\\alpha} = \\VaR_{\\alpha}( X )\\)\n\n\n\n\n\n\n\n\nFijar el nivel de confianza al que se desea calcular \\(0 &lt; \\alpha &lt; 1\\)\nFijar el nivel probabilidad para los errores de aproximación \\(\\delta &gt; 0\\)\nFijar el nivel erro \\(\\widetilde{\\varepsilon} &gt; 0\\)\nEstimar el posible error de estimación \\(\\varepsilon &gt; 0\\)\nBuscar el número de simulaciones adecuadas \\(n \\in \\N\\) que aseguren con una probabilidad menor a \\(\\delta\\) que se presente errores de aproximación superiores a \\(\\varepsilon\\).\nGenerar una muestra de tamaño \\(n\\) de la variable aleatoria, i.e. \\(X_1, \\ldots, X_n\\)\nCalcular el percentil de la \\(\\alpha\\) de la muestra \\(\\xi_{n,\\alpha} = F_n^{-1}( \\alpha )\\)\n\n\n\n\n\nDefinition 8.5: Valor en riesgo en la colaDada una variable aleatoria a valores reales \\(X\\), el valor en riesgo en la cola (tail value at risk) de \\(X\\) al nivel de probabilidad \\(\\alpha \\in (0,1)\\) está dado por \\[\n\\TVaR_{\\alpha}( X ) = \\frac{1}{1-\\alpha} \\int\\limits_{\\alpha}^1 \\VaR_u( X )\\ du\n\\]\n\n\n\n\nProposition 8.6: Coherencia de la medida TVaRLa medida de riesgo \\(\\TVaR_{\\alpha}\\) es una medida de riesgo coherente si la variable aleatoria sobre la cual se mide es una variable aleatoria continua.\n\n\n\nDefinition 8.7: Esperanza condicional en la colaDada una variable aleatoria a valores reales \\(X\\), la esperanza condicional en la cola (conditional tail expectation) de \\(X\\) al nivel de probabilidad \\(\\alpha \\in (0,1)\\) está dado por \\[\n\\CTE_{\\alpha}( X ) = \\E\\left[ X \\middle| X &gt; \\VaR_{\\alpha}( X ) \\right]\n\\]\n\n\n\nProposition 8.8Si la función de distribución acumulada \\(F_X\\) de la variable aleatoria \\(X\\) es continua, entonces se tiene la siguiente igualdad \\[\n\\CTE_{\\alpha}( X ) = \\TVaR_{\\alpha}( X )\n\\]\n\n\n\nDefinition 8.9: Valor en riesgo condicionadoDada una variable aleatoria a valores reales \\(X\\), el valor en riesgo condicionado (conditional value at risk) de \\(X\\) al nivel de probabilidad \\(\\alpha \\in (0,1)\\) está dado por \\[\n\\CVaR_{\\alpha}( X ) = \\E\\left[ X - \\VaR_{\\alpha}( X ) \\middle| X &gt; \\VaR_{\\alpha}( X ) \\right]\n= \\CTE_{\\alpha}( X ) - \\VaR_{\\alpha}( X )\n\\]\n\n\n\nDefinition 8.10: Déficit esperadoDada una variable aleatoria a valores reales \\(X\\), el déficit esperado (expected shortfall) de \\(X\\) al nivel de probabilidad \\(\\alpha \\in (0,1)\\) está dado por \\[\n\\ES_{\\alpha}( X ) = \\E\\left[ \\max\\left( X - \\VaR_{\\alpha}( X ), 0 \\right) \\right]\n\\]\n\n\n\nDefinition 8.11: Valor en riesgo entrópicoDada una variable aleatoria a valores reales \\(X\\), el valor en riesgo entrópico (entropic value at risk) de \\(X\\) al nivel de probabilidad \\(\\alpha \\in (0,1)\\) está dado por \\[\n\\EVaR_{\\alpha}( X ) = \\inf\\left\\{ \\frac{1}{t} \\ln\\left( \\frac{M_X( t )}{1 - \\alpha} \\right) \\middle| t &gt; 0 \\right\\}\n\\]\n\n\n\nProposition 8.12: Coherencia de la medida EVaRLa medida de riesgo \\(\\EVaR_{\\alpha}\\) es una medida de riesgo coherente.\n\n\n\nPodemos considerar el caso particular donde todos los reclamos se suponen independientes e idénticamente distribuidos (i.i.d), en este caso con distribución \\(X_i \\rightsquigarrow LN(\\mu,\\sigma)\\)\n\n\nCode\nset.seed(94312)\nu &lt;- 4\ns &lt;- 0.5\nn &lt;- 1e4\nX &lt;- rlnorm( n, meanlog = u, sdlog = s )\nalpha &lt;- seq( 0, 1, 0.01 ) \nVaRX &lt;- quantile( X, probs = alpha, names = FALSE )\nTVaRX &lt;- sapply( \n  1:length( VaRX ), \n  FUN = function( i ) ifelse( alpha[ i ] &lt; 1, ( 1 / ( 1 - alpha[ i ] ) ) * mean( X * ( X &gt; VaRX[ i ] ) ), max( X ) ) )\n\n\n\n\nCode\nplt &lt;- ggplot() +\n  geom_point( aes( x = alpha, y = VaRX ), colour = 'darkred' ) + \n  geom_point( aes( x = alpha, y = TVaRX ), colour = 'orange' ) + \n  xlab( TeX( \"$\\\\alpha$\" ) ) + \n  ylab( TeX( \"$VaR,TVaR$\" ) ) + \n  theme_bw()\nplot( plt )",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tarificación</span>"
    ]
  },
  {
    "objectID": "lectura_07.html#tarificacion_grandes_terminos",
    "href": "lectura_07.html#tarificacion_grandes_terminos",
    "title": "8  Tarificación",
    "section": "8.2 Tarificación en grandes términos",
    "text": "8.2 Tarificación en grandes términos\nLa tarificación que conlleva a la selección de la prima \\(\\Pi\\) debe tomar en cuenta como se manejan y equilibran los activos y pasivos en el negocio asegurador.\nPasivos\n\nCapitales propios\nReservas técnicas\nReservas para otros riesgos\nDeudas o depósitos en dinero recibidos por cesiones\nOtras deudas por pagar\n\nActivos\n\nCapital suscrito no desembolsado\nActivos no materiales\nInversiones\nParte de reaseguros en reservas técnicas\nDeudas por cobrar\nOtros activos\n\nEn el proceso de tarificación no es pertinente incluir todos los activos de la empresa, ya que muchos de estos no tienen la liquidez necesaria como para ser considerados un tipo de activo viable para la tarificación. Tampoco se toma en cuenta el dinero recibido por la cesión de primas en un ramo en particular, ya que esto constituye un nivel más arriba propio del negocio reasegurador.\n\nDefinition 8.13: Probabilidad de ruinaEn términos generales se busca equilibrar el resultado operativo del ramo de negocio \\(R\\) a lo largo de la vida del ramo. El resultado \\(R\\) a su vez está dado por la siguiente relación: \\[\nR = \\Pi + I - S - G - K\n\\] donde las variables a considerarse en principio son:\n\n\\(\\Pi\\) Ingreso por primas\n\\(I\\) Ingreso por inversiones\n\\(S\\) Pago de siniestros\n\\(G\\) Gastos agregados, incluyendo gastos de emisión, operativos, gastos por reclamos\n\\(K\\) Coste de capital, esencialmente es el retorno que se espera sobre el capital invertido por los inversores para mantener el nivel de solvencia del ramo.\n\nEn varias ocasiones el ciclo del negocio puede ser corto plazo y no permite considerar un ingreso por inversiones \\(I = 0\\). \\[\nR = \\Pi - S - G - K\n\\]\nSería ideal que a lo largo de la vida del ramo el resultado mantenga \\(R &gt; 0\\), pero al tratarse de un negocio que depende de la aleatoriedad de los reclamos, es bastante complicado encontrar un costo de capital \\(K\\) y una prima \\(\\Pi\\) que siempre asegure ante todo escenario que se mantenga la positividad. Ante este riesgo continuo se busca minimizar la probabilidad de ruina, es decir, acotar la probabilidad del evento \\(R &lt; 0\\) a un nivel de confianza \\(\\alpha &gt; 0\\) adecuado \\[\nP( R &lt; 0 ) &lt; \\alpha\n\\]\n\n\nMuchas de las veces se parte del principio de equilibrio financiero \\(\\ref{equifin}\\), donde se busca la igualdad \\(\\E[R] = 0\\), la misma implica la siguiente relación: \\[\n\\begin{split}\n0 & = \\E[R] \\\\\n0 & = \\E[R\\mid R \\geq 0]P( R \\geq 0 ) + \\E[R\\mid R &lt; 0]P( R &lt; 0 ) \\\\\n\\E[R\\mid R \\geq 0]P( R \\geq 0 ) & = -\\E[R\\mid R &lt; 0]P( R &lt; 0 ) \\\\\n\\frac{P( R &lt; 0 )}{P( R \\geq 0 ) } & = -\\frac{\\E[R\\mid R \\geq 0]}{\\E[R\\mid R &lt; 0]}\n\\end{split}\n\\] en el equilibrio financiero, la proporción de la probabilidad de ruina respecto de la probabilidad de no estar en ruina es igual a la proporción entre la esperanza condicional del resultado cuando no se produce la ruina respecto de la esperanza condicional cuando si se produce la ruina.\nUn modelo puede estar equilibrado financieramente \\(\\E[R] = 0\\), pero se desconoce la probabilidad de ruina \\(P( R &lt; 0)\\), esta podría ser muy grande. La razón anterior permite estimar la relevancia de la probabilidad de ruina en un modelo equilibrado, con el uso de las esperanzas condicionales.\nEs usual asumir que la única parte aleatoria de \\(R\\) viene dada por el valor de los reclamos totales, en razón de esto se tiene: \\[\n\\E[ R ]\n= \\E[ \\Pi - S - G - K ]\n= \\Pi - \\E[S] - G - K\n\\]\nEl coste de capital \\(K\\) como ya lo mencionamos es el retorno mínimo esperado por los inversores sobre el capital invertido. Usualmente este capital puede ser visto como un porcentaje \\(r &gt; 0\\) que se toma sobre el capital colocado para mantener un nivel de solvencia adecuado sobre el valor esperado \\(\\E[S]\\) del total de reclamos \\(S\\). Para ello se suele utilizar precisamente una medida de medida de riesgo \\(\\rho\\) que permita mantener el nivel de solvencia. \\[\nK = r\\left( \\rho( S ) - \\E[S] \\right)\n\\] con esta perspectiva \\[\n\\E[ R ]\n= \\E\\left[ \\Pi - S - G - r\\left( \\rho( S ) - \\E[S] \\right) \\right]\n= \\Pi - \\E[S] - G - r \\left( \\rho(S) - \\E[S] \\right)\n\\]\n\nDefinition 8.14: Tarificación (Pricing)Lo que se busca es evitar la ruina y por tanto se busca cubrirse ante el evento \\(R &lt; 0\\), desde una perspectiva probabilista esto se puede realizar seleccionando un nivel de cobertura \\(\\alpha &gt; 0\\), que acote la probabilidad del evento de ruina. \\[\nP( R &lt; 0 )\n= P( \\Pi - S - G - K &lt; 0 )\n= P\\left( \\Pi - S - G - r\\left( \\rho(S) - \\E[S] \\right) &lt; 0 \\right) &lt; \\alpha.\n\\]\nDe la relación anterior se observa que una vez seleccionado el nivel de cobertura \\(\\alpha\\) y la medida de riesgo \\(\\rho\\), las variables correspondientes a la prima \\(\\Pi\\) y a los gastos \\(G\\) quedan libres, de ahí resulta un margen que permite seleccionar la prima más óptima para un producto de seguro, así como también optimizar los gastos \\(G\\). Este proceso de selección es precisamente lo que llamamos en este contexto como tarificación (pricing).\n\n\nRazonando un poco más al respecto, si asumimos que se dispone de la distribución acumulada \\(F_S\\) de \\(S\\), se puede obtener las siguientes expresiones: \\[\n\\begin{split}\nP\\left( \\Pi - S - G - K &lt; 0 \\right) & &lt; \\alpha \\\\\nP\\left( S &gt; \\Pi - G - K \\right) & &lt; \\alpha  \\\\\n1 - P\\left( S \\leq \\Pi - G - K \\right) & &lt; \\alpha \\\\\nP\\left( S \\leq \\Pi - G - K \\right) & &gt; 1 - \\alpha \\\\\nF_S\\left( \\Pi - G - K \\right) & &gt; 1 - \\alpha \\\\\nF_S\\left( \\Pi - G - K \\right) & \\in \\left( 1 - \\alpha, +\\infty \\right), \\qquad\n\\text{la desigualdad anterior es equivalente a la inclusión}\\\\\n\\Pi - G - K & \\in F_S^{-1}\\left( \\left( 1 - \\alpha, +\\infty \\right) \\right),\\qquad\n\\text{por propiedades de la inversión de funciones}\\\\\n\\Pi & \\geq G + K + F_S^{-1}\\left( 1 - \\alpha \\right), \\qquad\n\\text{como $F_S$ es creciente, mínimo se debe satisfacer la desigualdad}\n\\end{split}\n\\] En especial cuando \\(K = r\\left( \\rho(S) - \\E[S] \\right)\\), la última desigualdad toma la forma \\[\n\\Pi \\geq G + r\\left( \\rho(S) - \\E[S] \\right) + F_S^{-1}\\left( 1 - \\alpha \\right)\n\\]\nAsí, en el caso anterior si se utiliza como medida de riesgo al mismo nivel de confianza \\(1 - \\alpha\\), esto es \\(\\rho( S ) = \\VaR_{1-\\alpha}( S ) = F_S^{-1}( 1 - \\alpha )\\), la última igualdad se cumple si \\(S\\) es una variable aleatoria continua. \\[\n\\Pi \\geq G + (1 + r) F_S^{-1}\\left( 1 - \\alpha \\right) - r \\E[S]\n\\]\nSi en caso los gastos \\(G\\) son proporcionales a la prima \\(G = \\gamma \\Pi\\), para una constante \\(\\gamma &gt; 0\\), y seguramente \\(\\gamma &lt; 1\\) ya que es de esperar gastos no mayores a la misma prima, sino esto estaría en una situación insostenible donde los gastos son mayores a la cobertura del riesgo. Las desigualdades anteriores toman la forma: \\[\n\\begin{split}\n\\Pi & \\geq \\frac{r}{1 - \\gamma} K + \\frac{1}{1 - \\gamma} F_S^{-1}\\left( 1 - \\alpha \\right) \\\\\n\\Pi & \\geq \\frac{r}{1 - \\gamma} \\left( \\rho(S) - \\E[S] \\right) + \\frac{1}{1 - \\gamma} F_S^{-1}\\left( 1 - \\alpha \\right) \\\\\n\\Pi & \\geq \\frac{1 + r}{1 - \\gamma} F_S^{-1}\\left( 1 - \\alpha \\right) -  \\frac{r}{1 - \\gamma} \\E[S]\n\\end{split}\n\\]\nDe lo anterior se observa, que un buen inicio para estimar la prima de riesgo es ciertamente usar la medida \\(\\VaR_{1-\\alpha}\\) al nivel adecuado de confianza \\(\\alpha\\).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tarificación</span>"
    ]
  },
  {
    "objectID": "lectura_07.html#prima",
    "href": "lectura_07.html#prima",
    "title": "8  Tarificación",
    "section": "8.3 Prima",
    "text": "8.3 Prima\nLa prima es la cantidad de dinero que un individuo o entidad pagan por una póliza de seguro, la cual está diseñada para cubrir ciertos riesgos personales o comerciales.\nLa determinación de las primas por parte del asegurador hace uso de la mutualización del riesgo y diversificación, para así poder asumir la transferencia del riesgo por parte de sus asegurados. Así por tanto, es deseable que cualquier método que se utilice para la estimación de primas, se satisfaga, algunas propiedades importantes.\nSin consideramos dos riesgos a cubrir \\(S_1\\) y \\(S_2\\), entonces la función que estima \\(\\rho\\) las primas sería aconsejable satisfaga las siguientes propiedades.\n\nSi se decide cubrir por completo dos riesgos \\(S_1\\) y \\(S_2\\) en un mismo producto, el valor de la prima deberá ser menor o igual al valor que se resultaría de cubrir cada uno de los riesgos con productos separados. \\[\n\\rho( S_1 + S_2 ) \\leq \\rho( S_1 ) + \\rho( S_2 )\n\\]\nEl asumir mayor riesgo debe tener como consecuencia el aumento de la prima \\[\n\\rho( S_1 ) \\leq \\rho( S_1 + S_2 )\n\\] Esta propiedad implica que al configurar un producto de seguro con mejor cobertura, se espera una prima de mayor costo.\nSi el riesgo a cubrir está limitado, es decir \\(P( S \\leq M ) = 1\\), para un valor \\(M &gt; 0\\), entonces jamás la prima será superior a \\(M\\) \\[\n\\rho( S ) \\leq M\n\\] Esto se traduce a que ningún asegurado estará interesado en adquirir una póliza para cubrir un riesgo por encima del valor total asegurado.\n\nEs así que hay algunos principios para la estimación de primas, aquí citamos algunos de los más conocidos:\n\nPrima neta, o prima pura de riesgo \\[\n  \\Pi = \\rho( S ) = \\E[S] \\approx \\overline{S}\n\\]\nPrima de riesgo con recargo sobre la esperanza matemática \\[\n  \\Pi = \\rho( S ) = (1 + \\rho) \\E[S] \\approx (1 + \\rho) \\overline{S}\n\\]\nPrima de riesgo con recargo sobre la varianza \\[\n  \\Pi = \\rho( S ) = \\E[S] + \\rho \\V[S]\n  \\approx \\overline{S} + \\rho \\sigma_S^2\n\\]\nPrima de riesgo con recargo sobre la desviación \\[\n  \\Pi = \\rho( S ) = \\E[S] + \\rho \\sqrt{\\V[S]}\n  \\approx \\overline{S} + \\rho \\sigma_S\n\\]\nPrima de riesgo con principio exponencial para \\(t &gt; 0\\) \\[\n  \\Pi = \\rho( S )\n  = \\frac{1}{2} \\E\\left[\\exp(tS)\\right]\n  = \\frac{1}{2} M_N\\big( \\ln M_X( t ) \\big)\n  \\approx \\frac{1}{m} \\sum\\limits_{i=1}^m \\exp\\left(t S_i\\right)\n\\]\nPrima de percentiles para un valor de confianza \\(\\alpha \\in [0,1]\\) o prima de valor en riesgo \\(VaR_\\alpha\\) \\[\n  \\Pi = \\rho( S ) = \\VaR_\\alpha( S ) =  F_S^{-1}( \\alpha )\n\\]\nPrima de valor en riesgo en la cola (Tail Value at Risk) \\(TVaR_\\alpha\\). Es el promedio uniforme de todos los valores en riesgo \\(VaR_u\\), con \\(u \\geq \\alpha\\). \\[\n  \\Pi = \\rho( S ) = \\TVaR_\\alpha( S ) = \\frac{1}{1-\\alpha} \\int\\limits_{\\alpha}^1 \\VaR_u( S )\\ du\n\\]\n\n\nConsideremos el caso donde todos los siniestros son igualmente distribuidos e independientes (i.i.d) \\(X_i \\rightsquigarrow Gamma( \\alpha_i, \\theta )\\), para \\(i \\in \\{1,\\ldots,n\\}\\), el número de unidades aseguradas está dado por \\(n \\in \\N\\). Utilizaremos el modelo individual para la agregación de los reclamos y obtener el reclamo total \\(S = \\sum\\limits_{i=1}^n X_i\\).\nComo todas las variables \\(X_i\\) siguen una ley \\(Gamma( \\alpha_i, \\theta )\\), sabemos que la familia \\(Gamma\\) es cerrada por adición, es decir, a suma de variables aleatorias con ley \\(Gamma\\) también sigue una ley \\(Gamma\\). En este caso en particular para el reclamo total tenemos que \\(S \\rightsquigarrow Gamma\\left( \\sum\\limits_{i=1}^n \\alpha_i, \\theta \\right)\\).\nEs de notar que para cada \\(i\\in \\{1,\\ldots,n\\}\\), la variable aleatoria \\(X_i\\) correspondiente al \\(i\\)-ésimo reclamo tiene como parámetro un diferente factor \\(\\alpha_i\\), pero el mismo factor \\(\\theta\\).\nYa que cada \\(X_i \\rightsquigarrow Gamma( \\alpha_i, \\theta )\\), podemos calcular de forma determinista las esperanzas de \\(\\E[X_i]\\), para cada \\(i \\in \\{1,\\ldots,n\\}\\) y de igual forma podemos calcular la esperanza del reclamo total \\(\\E[S] = \\sum\\limits_{i=1}^n \\E[X_i]\\). Además como asumimos independencia entre los \\(X_i\\), la varianza de \\(S\\) también puede ser calculada fácilmente como \\(\\V[S] = \\sum\\limits_{i=1}^n \\V[X_i]\\).\nTambién estamos en la capacidad de simular la variable \\(S\\) utilizando un algoritmo de aleatorio, para así aproximar los cálculos de sus momentos y otros estadísticos. Primeramente definamos los parámetros.\n\n\nCode\n# número de unidades aseguradas\nn &lt;- 1000\n\n# parámetros para cada X_i\na &lt;- runif( n, 5, 10 )\n\n# parámetro theta, común a todas las X_i\ntheta &lt;- 4\n\n# parámetro para S\nA &lt;- sum( a )\n\n\n\n\nCode\nEX &lt;- a * theta\nVX &lt;- a * theta^2\nES &lt;- sum( EX )\nVS &lt;- sum( VX )\nSDS &lt;- sqrt( VS )\n\n\nLa variable aleatoria del reclamo total \\(S\\) la podemos simular tomando una muestra i.i.d de tamaño \\(m\\), i.e. \\(S_1,\\ldots,S_m\\) de la distribución \\(Gamma\\left( \\sum\\limits_{i=1}^n \\alpha_i, \\theta \\right)\\).\n\n\nCode\nm &lt;- 1e5\nS &lt;- rgamma( m, shape = A, scale = theta )\n\n\nPrima pura:\n\n\nCode\nP &lt;- ES\nP &lt;- mean( S )\n\nalpha &lt;- 0.95\nP_avg &lt;- ( 1 + alpha ) * ES\nP_avg &lt;- ( 1 + alpha ) * mean( S )\n\nP_var &lt;- ES + alpha * VS\nP_var &lt;- mean( S ) + alpha * var( S )\n\nP_sde &lt;- ES + alpha * SDS\nP_sde &lt;- mean( S ) + alpha * sd( S )\n\nVaRS &lt;- qgamma( alpha, shape = A, scale = theta )\nP_VaR &lt;- VaRS\nP_VaR &lt;- quantile( S, probs = alpha )\n\nP_TVaR &lt;- ( 1 / ( 1 - alpha ) ) * ( A * theta ) * ( 1 - pgamma( VaRS, shape = A + 1, scale = theta ) )\nP_TVaR &lt;- ( 1 / ( 1 - alpha ) ) * integrate( f = function( u ) qgamma( u, shape = A, scale = theta ), alpha, 1 )$value\nP_TVaR &lt;- mean( sapply( runif( m, alpha, 1 ), FUN = function( k ) qgamma( k, shape = A, scale = theta ) ) )\nI &lt;- as.numeric( S &gt; VaRS )\nP_TVaR &lt;- mean( S * I ) / mean( I )",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tarificación</span>"
    ]
  },
  {
    "objectID": "lectura_07.html#segmentación",
    "href": "lectura_07.html#segmentación",
    "title": "8  Tarificación",
    "section": "8.4 Segmentación",
    "text": "8.4 Segmentación\nEn muchas ocasiones es necesario tener en cuenta algunas características asociadas al riesgo asegurado, de tal forma que la prima sea lo más eficiente y adecuada según el riesgo cubierto y las características del mismo. La idea de segmentar la población es obtener grupos homogéneos con riesgos similares.\nSin segmentación de la población, este el caso más sencillo para la tarificación, el asegurado\n\n\nCode\ndat &lt;- data.table( nom = c( 'Pago', 'Pago esperado', 'Varianza' ),\n                   cl = c( \"$\\\\E[S]$\", \"$\\\\E[S]$\", \"$0$\" ), \n                   as = c( \"$S - \\\\E[S]$\", \"$0$\", \"$\\\\V[S]$\" ) )\ndat %&gt;%\n  kable(\n    label = NA,\n    caption = 'Transferencia de riesgo sin segmentación',\n    row.names = FALSE,\n    col.names = c( '', 'Asegurado', 'Asegurador' ),\n    align = 'lrr',\n    digits = c( 0, 0, 0 ),\n    format.args = list( big.mark = ',', decimal.mark = '.', scientific = FALSE ),\n    escape = FALSE,\n    centering = TRUE, \n    booktabs = TRUE ) %&gt;%\n  kable_classic( full_width = FALSE, html_font = \"Cambria\", position = \"center\" )\n\n\n\nTransferencia de riesgo sin segmentación\n\n\n\nAsegurado\nAsegurador\n\n\n\n\nPago\n$\\E[S]$\n$S - \\E[S]$\n\n\nPago esperado\n$\\E[S]$\n$0$\n\n\nVarianza\n$0$\n$\\V[S]$\n\n\n\n\n\n\n\nPara el caso cuando se puede caracterizar la severidad total a partir de otra variable explicativa \\(Y\\), condicionando así cada una de las estimaciones según los valores que va tomando la variable explicativa. Esto permite estables primas de forma más adecuada según las diferentes características del riesgo cubierto.\n\n\nCode\ndat &lt;- data.table( nom = c( 'Pago', 'Pago esperado', 'Varianza' ),\n                   cl = c( \"$\\\\E[S \\\\mid Y]$\", \"$\\\\E[S]$\", \"$\\\\V[\\\\E[S \\\\mid Y]]$\" ), \n                   as = c( \"$S - \\\\E[S \\\\mid Y]$\", \"$0$\", \"$\\\\V[S-\\\\E[S \\\\mid Y]]$\" ) )\ndat %&gt;%\n  kable(\n    label = NA,\n    caption = 'Transferencia de riesgo con segmentación',\n    row.names = FALSE,\n    col.names = c( '', 'Asegurado', 'Asegurador' ),\n    align = 'lrr',\n    digits = c( 0, 0, 0 ),\n    format.args = list( big.mark = ',', decimal.mark = '.', scientific = FALSE ),\n    escape = FALSE,\n    centering = TRUE,\n    booktabs = TRUE ) %&gt;%\n  kable_classic( full_width = FALSE, html_font = \"Cambria\", position = \"center\" )\n\n\n\nTransferencia de riesgo con segmentación\n\n\n\nAsegurado\nAsegurador\n\n\n\n\nPago\n$\\E[S \\mid Y]$\n$S - \\E[S \\mid Y]$\n\n\nPago esperado\n$\\E[S]$\n$0$\n\n\nVarianza\n$\\V[\\E[S \\mid Y]]$\n$\\V[S-\\E[S \\mid Y]]$\n\n\n\n\n\n\n\nLa varianza \\(\\V[\\E[S \\mid Y]]\\) es atribuible a la heterogeneidad del portafolio de asegurados, y la misma se asigna a cada asegurado, más no es absorbida por el asegurador.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tarificación</span>"
    ]
  },
  {
    "objectID": "lectura_08.html",
    "href": "lectura_08.html",
    "title": "9  Modelos lineales generalizados (GLM)",
    "section": "",
    "text": "9.1 Familia exponencial\nAl tener varias observaciones independientes de la misma variable aleatoria \\(X_1, \\ldots, X_N\\), su distribución conjunta se puede expresar como: \\[\nf_{X_1,\\ldots,X_N}( x_1, \\ldots, x_N \\mid \\theta )\n= \\prod\\limits_{i=1}^N f_{X_i}( x_i \\mid \\theta )\n\\]\nLa función de verosimilitud logarítmica, “log-likelihood”, toma una forma específica, la cual puede ser trabajada con comodidad para la maximización de verosimilitud. \\[\n\\begin{split}\n\\ell( \\theta )\n& = \\log f_{X_1,\\ldots,X_N}( x_1, \\ldots, x_N \\mid \\theta ) \\\\\n& = \\sum\\limits_{i=1}^N \\log f_{X_i}( x_i \\mid \\theta ) \\\\\n& = \\sum\\limits_{i=1}^N \\left( \\log h( x_i ) + \\eta( \\theta ) \\cdot T( x_i ) - A( \\theta )  \\right)\n\\end{split}\n\\] lo que podemos observar es que existe una casi linealidad respecto de la variable \\(\\theta\\). Esta propiedad permite obtener un problema de maximización de verosimilitud que puede ser atacado con varios paquetes de optimización numérica de una forma eficiente.\n\\[\n\\underset{\\theta \\in \\Theta}{\\sup} \\ell( \\theta )\n= \\underset{\\theta \\in \\Theta}{\\sup}  \n\\sum\\limits_{i=1}^N \\left( \\eta( \\theta ) \\cdot T( x_i ) - A( \\theta )  \\right)\n\\]\nUsualmente, este problema se suele atacar mediante la anulación del gradiente de la función objetivo, esto lo realiza la mayor parte de paquetes de software. El sistema a resolver es el siguiente sistema, usualmente no lineal, de dimensión \\(m\\):\n\\[\n\\frac{\\D \\ell}{\\D \\theta_i}( \\theta )\n= \\sum\\limits_{i=1}^N \\left( \\frac{\\D \\eta}{\\D \\theta_i}( \\theta ) \\cdot T( x_i ) -\n\\frac{\\D A}{\\D \\theta_i}( \\theta ) \\right)\n= 0,\\qquad \\forall i \\in \\{1,\\ldots, m\\}\n\\]\nEn la familia exponential tenemos las siguientes distribuciones: normal, exponencial, log-normal, gamma, chi-cuadrado, beta, Dirichlet, Bernoulli, Poisson, binomial, geométrica, binomial negativa, von Mises-Fisher, Pareto con valor mínimo conocido, Gaussiana inversa, gamma inversa, multinomial con número \\(n\\) conocido, Wishart, categórica.\nPor la condición de normalización de la densidad de probabilidad \\(f_X\\), se satisface la igualdad: \\[\n\\begin{split}\n1 & = \\int\\limits f_X( x \\mid \\theta )\\ dx \\\\\n1 & = \\int\\limits h( x ) \\exp\\left( \\eta( \\theta ) \\cdot T( x ) - A( \\theta ) \\right)\\ dx \\\\\n1 & = \\int\\limits h( x ) \\exp\\left( \\eta( \\theta ) \\cdot T( x ) \\right) \\exp\\left( - A( \\theta ) \\right)\\ dx \\\\\n1 & = \\exp\\left( - A( \\theta ) \\right) \\int\\limits h( x ) \\exp\\left( \\eta( \\theta ) \\cdot T( x ) \\right)\\ dx \\\\\n\\exp\\left( A( \\theta ) \\right) & = \\int\\limits h( x ) \\exp\\left( \\eta( \\theta ) \\cdot T( x ) \\right)\\ dx \\\\\nA( \\theta ) & = \\log\\left( \\int\\limits h( x ) \\exp\\left( \\eta( \\theta ) \\cdot T( x ) \\right)\\ dx \\right)\n\\end{split}\n\\] de donde se determina precisamente la forma que tiene la función \\(A\\) y su el papel como factor de normalización para la distribución de la variable aleatoria \\(X\\).\nAsí mismo, podemos establecer una cierta relación para la espera de los valores observados de \\(T(X)\\) en función de \\(A\\) \\[\n\\begin{split}\n0 & = \\frac{\\D}{\\D \\theta_i} 1 \\\\\n0 & = \\frac{\\D}{\\D \\theta_i} \\int\\limits f_X( x \\mid \\theta )\\ dx \\\\\n0 & = \\int\\limits h( x ) \\frac{\\D}{\\D \\theta_i} \\exp\\left( \\eta( \\theta ) \\cdot T( x ) - A( \\theta ) \\right)\\ dx \\\\\n0 & = \\int\\limits h( x ) \\exp\\left( \\eta( \\theta ) \\cdot T( x ) - A( \\theta ) \\right)\n\\left( \\frac{\\D \\eta}{\\D \\theta_i}( \\theta )  \\cdot T( x ) -  \\frac{\\D A}{\\D \\theta_i}( \\theta ) \\right)\\ dx \\\\\n0 & = \\int\\limits h( x ) \\exp\\left( \\eta( \\theta ) \\cdot T( x ) - A( \\theta ) \\right)\n\\frac{\\D \\eta}{\\D \\theta_i}( \\theta )  \\cdot T( x )\\ dx \\\\\n& - \\frac{\\D A}{\\D \\theta_i} ( \\theta )\n\\int\\limits h( x ) \\exp\\left( \\eta( \\theta ) \\cdot T( x ) - A( \\theta ) \\right)\\ dx \\\\\n0 & = \\int \\limits \\frac{\\D \\eta}{\\D \\theta_i}( \\theta )  \\cdot T( x ) f_X( x \\mid \\theta )\\ dx\n- \\frac{\\D A}{\\D \\theta_i}( \\theta ) \\int \\limits f_X( x \\mid \\theta )\\ dx\n\\end{split}\n\\] estamos en la capacidad de concluir que: \\[\n\\E\\left[ \\frac{\\D \\eta}{\\D \\theta_i}( \\theta )  \\cdot T( X ) \\right]\n= \\frac{\\D A}{\\D \\theta_i}( \\theta )\n\\]\nCuando se da el caso que \\(\\eta\\) y \\(T\\) son las funciones identidad, la relación anterior se reduce a la siguiente igualdad: \\[\n\\E\\left[ X_i \\right]\n= \\frac{\\D A}{\\D \\theta_i}( \\theta )\n\\]",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Modelos lineales generalizados (GLM)</span>"
    ]
  },
  {
    "objectID": "lectura_08.html#familia-exponencial",
    "href": "lectura_08.html#familia-exponencial",
    "title": "9  Modelos lineales generalizados (GLM)",
    "section": "",
    "text": "Definition 9.1: Familia exponencialDe forma amplia, un modelo lineal generalizado aprovecha algunas propiedades de la distribución de probabilidad \\(f_X\\) de la variable (vector) aleatoria \\(X : \\Omega \\longrightarrow \\R^n\\) en estudio. Se parte de asumir que \\(X\\) tiene una distribución dentro de la familia exponencial, es decir, existen:\n\nUn conjunto admisible de parámetros \\(\\Theta \\subset \\R^m\\), así cada parámetro \\(\\theta = ( \\theta_1, \\ldots, \\theta_m )^T \\in \\Theta\\),\nUna función \\(T: \\R^n \\longrightarrow \\R^p\\)\nUna función \\(A: \\R^m \\longrightarrow \\R\\)\nUna función \\(\\eta: \\R^m \\longrightarrow \\R^p\\)\n\nde tal forma que \\(X \\rightsquigarrow f_X\\), siendo: \\[\nf_X( x \\mid \\theta ) = h( x ) \\exp\\left( \\eta( \\theta ) \\cdot T( x ) - A( \\theta ) \\right)\n\\]",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Modelos lineales generalizados (GLM)</span>"
    ]
  },
  {
    "objectID": "lectura_08.html#modelo-lineal-generalizado-glm",
    "href": "lectura_08.html#modelo-lineal-generalizado-glm",
    "title": "9  Modelos lineales generalizados (GLM)",
    "section": "9.2 Modelo lineal generalizado (GLM)",
    "text": "9.2 Modelo lineal generalizado (GLM)\n\nDefinition 9.2: Modelo lineal generalizado (GLM)Precisamente, un modelo lineal generalizado (GLM por Generalized Linear Model) busca explotar las propiedades de las variables aleatorias que poseen una densidad de probabilidad dada por alguna de la funciones de la familia exponencial. Para más detalles se puede consultar [1].\nLa idea general es poder describir el comportamiento de una variable aleatoria \\(Y\\) que se presume puede ser descrita por alguna función de la familia exponencial, a partir de otras variables aleatorias \\(X : \\Omega \\longrightarrow \\R^n\\) mediante una función de vínculo \\(g: \\R^q \\longrightarrow \\R^m\\) entre \\(\\theta\\) y \\(X\\), a través del paso por una composición con una función lineal \\(\\beta : \\R^n \\longrightarrow \\R^q\\), de tal forma que: \\[\n\\theta = g( \\beta( X ) ) = g( \\beta X )\n\\]\nAsí, un GLM prescribe una distribución de probabilidad para \\(Y\\) que tendrá la siguiente forma y será dependiente de los parámetros \\(\\beta\\) y las variables explicativas \\(X\\) \\[\nf_Y( y \\mid \\theta )\n= f_Y( y \\mid g( \\beta x ) )\n= h( y ) \\exp\\left( \\eta( g( \\beta x ) ) \\cdot T( y ) - A( g( \\beta x ) ) \\right)\n\\]\n\n\nEn la aplicación, el ajuste de un modelo GLM a \\(N \\in \\N\\) observaciones \\(y_1, \\ldots, y_N\\) de la variable aleatoria \\(Y\\) y sus respectivas variables explicativas \\(x_1, \\ldots, x_N\\). Se asume independencias entre las observaciones y se busca maximizar la verosimilitud de los valores observados, pero en función del nuevo parámetro \\(\\beta\\), de ahí la parte lineal del modelo. \\[\n\\ell( \\beta )\n= \\sum\\limits_{i=1}^N \\left( \\log h( y_i ) +\n\\eta( g( \\beta x_i ) ) \\cdot T( y_i ) - A( g( \\beta x_i ) \\right)\n\\]\nDe ello resulta el problema de maximización de verosimilitud que se busca resolver para ajustar un modelo GLM. \\[\n\\underset{\\beta \\in \\R^{q \\times n}}{\\sup} \\ell( \\beta )\n= \\underset{\\beta \\in \\R^{q \\times n}}{\\sup} \\sum\\limits_{i=1}^N \\left( \\log h( y_i ) +\n\\eta( g( \\beta x_i ) ) \\cdot T( y_i ) - A( g( \\beta x_i ) \\right)\n\\] es de notar que el término \\(\\sum\\limits_{i=1}^N \\log h( y_i )\\) no depende de \\(\\beta\\) y por tal razón puede ser omitido al momento de maximizar la verosimilitud.\nEn varias ocasiones no todos los parámetros \\(\\theta\\) de la densidad de probabilidad son considerados, sino tan solo una parte de los mismos, los otros parámetros se consideran como un valor constante ya dado o también como un parámetro a determinar. Así, se divide \\(\\theta = ( \\theta_1, \\theta_2 ), \\theta_1 \\in \\R^{m_1}, \\theta_2 \\in \\R^{m_2}, m = m_1 + m_2\\), donde solo se establece una función de vínculo para la primera parte \\(\\theta_1 = g( \\beta X )\\). Esta consideración en muchos casos ayuda a simplificar la formulación del modelo y el costo computacional de estimación. En este contexto la función de verosimilitud tomaría la forma: \\[\n\\ell( \\beta, \\theta_2 )\n= \\sum\\limits_{i=1}^N \\left( \\log h( y_i ) +\n\\eta( g( \\beta x_i ), \\theta_2 ) \\cdot T( y_i ) - A( g( \\beta x_i ), \\theta_2 ) \\right)\n\\]\n\nEn este caso presentamos el modelo bastante utilizado conocido como regresión de Poisson. En donde suponemos que \\(N \\rightsquigarrow Pois( \\lambda( x ) ER( x ) ) = Pois( g( \\beta x ) ER( x ) )\\) donde \\(\\lambda\\) es el parámetro a estimar en función de las variables explicativas \\(x\\). \\[\n\\E[ N ] = \\lambda( x ) ER( x ) = g( \\beta x ) ER( x )\n\\]\n\\[\nP( N = n ) = \\exp(-g( \\beta x ) ER( x )) \\frac{g( \\beta x )^n ER( x )^n}{n!}\n\\]\n\n\nCode\n# la librería CASdatasets fue previamente cargada\ndata( beMTPL97 )\nbeMTPL97 &lt;- as.data.table( beMTPL97 )\n\n# Impresión de las primeras filas de la tabla\nbeMTPL97[ 1:15 ] %&gt;%\n  kable(\n    label = NA,\n    caption = \"Pólizas y reclamos de automóviles en Bélgica\",\n    row.names = FALSE,\n    col.names = names( beMTPL97 ),\n    align = paste0( rep( \"r\", ncol( beMTPL97 ) ), collapse = \"\" ),\n    digits = c( 0, 2, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4 ),\n    format.args = list( big.mark = ',', decimal.mark = '.', scientific = FALSE ),\n    escape = FALSE,\n    centering = TRUE,\n    booktabs = TRUE,\n    longtable = TRUE ) %&gt;%\n  kable_classic( \n    full_width = FALSE, \n    html_font = \"Cambria\", \n    position = 'center', \n    latex_options = c( 'hold_position', 'repeat_header', 'scale_down' ) ) %&gt;%\n  scroll_box( width = \"100%\", height = \"500px\" )\n\n\n\n\nPólizas y reclamos de automóviles en Bélgica\n\n\nid\nexpo\nclaim\nnclaims\namount\naverage\ncoverage\nageph\nsex\nbm\npower\nagec\nfuel\nuse\nfleet\npostcode\nlong\nlat\n\n\n\n\n1\n1.00\n1\n1\n1,618.00\n1,618.00\nTPL\n50\nmale\n5\n77\n12\ngasoline\nprivate\n0\n1,000\n4.3552\n50.8454\n\n\n2\n1.00\n0\n0\n0.00\nNaN\nTPL+\n64\nfemale\n5\n66\n3\ngasoline\nprivate\n0\n1,000\n4.3552\n50.8454\n\n\n3\n1.00\n0\n0\n0.00\nNaN\nTPL\n60\nmale\n0\n70\n10\ndiesel\nprivate\n0\n1,000\n4.3552\n50.8454\n\n\n4\n1.00\n0\n0\n0.00\nNaN\nTPL\n77\nmale\n0\n57\n15\ngasoline\nprivate\n0\n1,000\n4.3552\n50.8454\n\n\n5\n0.05\n1\n1\n155.97\n155.97\nTPL\n28\nfemale\n9\n70\n7\ngasoline\nprivate\n0\n1,000\n4.3552\n50.8454\n\n\n6\n1.00\n0\n0\n0.00\nNaN\nTPL\n26\nmale\n11\n70\n12\ngasoline\nprivate\n0\n1,000\n4.3552\n50.8454\n\n\n7\n1.00\n1\n1\n155.97\n155.97\nTPL++\n26\nmale\n11\n55\n8\ngasoline\nprivate\n0\n1,000\n4.3552\n50.8454\n\n\n8\n0.40\n0\n0\n0.00\nNaN\nTPL\n58\nfemale\n11\n47\n14\ngasoline\nprivate\n0\n1,000\n4.3552\n50.8454\n\n\n9\n1.00\n0\n0\n0.00\nNaN\nTPL++\n59\nmale\n0\n98\n3\ngasoline\nprivate\n0\n1,000\n4.3552\n50.8454\n\n\n10\n1.00\n0\n0\n0.00\nNaN\nTPL+\n34\nmale\n7\n74\n6\ngasoline\nprivate\n0\n1,000\n4.3552\n50.8454\n\n\n11\n1.00\n1\n1\n544.87\n544.87\nTPL++\n39\nmale\n1\n85\n2\ndiesel\nprivate\n0\n1,000\n4.3552\n50.8454\n\n\n12\n0.98\n0\n0\n0.00\nNaN\nTPL\n55\nmale\n11\n87\n6\ndiesel\nprivate\n0\n1,000\n4.3552\n50.8454\n\n\n13\n1.00\n0\n0\n0.00\nNaN\nTPL+\n77\nmale\n9\n104\n6\ngasoline\nprivate\n0\n1,000\n4.3552\n50.8454\n\n\n14\n0.97\n0\n0\n0.00\nNaN\nTPL\n49\nmale\n8\n71\n19\ngasoline\nprivate\n0\n1,000\n4.3552\n50.8454\n\n\n15\n0.58\n0\n0\n0.00\nNaN\nTPL\n63\nmale\n0\n104\n7\ngasoline\nprivate\n0\n1,000\n4.3552\n50.8454\n\n\n\n\n\n\n\n\n\nCode\nmodel_N &lt;- glm( formula = nclaims ~ ageph + sex + power + fuel + offset( log( expo ) ) + 0, \n                family = poisson( link = 'log' ), \n                data = beMTPL97 )\n\n\n\n\nCode\ndat &lt;- tidy( model_N )\ndat %&gt;%\n  kable(\n    label = NA,\n    caption = \"Resultados del modelo GLM\",\n    row.names = FALSE,\n    col.names = names( dat ),\n    align = 'lrrrr',\n    digits = c( 0, 6, 6, 6, 6 ),\n    format.args = list( big.mark = ',', decimal.mark = '.', scientific = FALSE ),\n    escape = FALSE,\n    centering = TRUE, \n    booktabs = TRUE ) %&gt;%\n  kable_classic( \n    full_width = FALSE, \n    html_font = \"Cambria\", \n    position = 'center', \n    latex_options = c( 'hold_position', 'repeat_header', 'scale_down' ) ) %&gt;%\n  scroll_box( width = \"100%\", height = \"300px\" )\n\n\n\n\nResultados del modelo GLM\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\nageph\n-0.015627\n0.000508\n-30.790409\n0\n\n\nsexfemale\n-1.409836\n0.032166\n-43.830099\n0\n\n\nsexmale\n-1.458010\n0.034274\n-42.539517\n0\n\n\npower\n0.002499\n0.000372\n6.714624\n0\n\n\nfueldiesel\n0.140048\n0.015098\n9.275900\n0\n\n\n\n\n\n\n\n\n\nCode\ndat &lt;- glance( model_N )\ndat %&gt;%\n  kable(\n    label = NA,\n    caption = \"Resultados del modelo GLM\",\n    row.names = FALSE,\n    col.names = names( dat ),\n    align = paste0( rep( \"r\", ncol( dat ) ), collapse = \"\" ),\n    digits = c( 4, 0, 2, 2, 2, 2, 0, 0 ),\n    format.args = list( big.mark = ',', decimal.mark = '.', scientific = FALSE ),\n    escape = FALSE,\n    centering = TRUE,\n    booktabs = TRUE ) %&gt;%\n  kable_classic( \n    full_width = FALSE, \n    html_font = \"Cambria\", \n    position = 'center', \n    latex_options = c( 'hold_position', 'repeat_header', 'scale_down' ) ) %&gt;%\n  scroll_box( width = \"100%\", height = \"200px\" )\n\n\n\n\nResultados del modelo GLM\n\n\nnull.deviance\ndf.null\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n260,163.9\n163,212\n-63,171.99\n126,354\n126,404\n88,651.89\n163,207\n163,212\n\n\n\n\n\n\n\n\n\nCode\npob &lt;- expand.grid( ageph = seq( 18, 90, 1 ), \n                    sex = c( 'male', 'female' ),\n                    power = seq( 10, 250 ),\n                    fuel = c( 'gasoline', 'diesel' ),\n                    expo = 1 )\npob &lt;- as.data.table( pob )\npob[ , EN := predict( model_N, newdata = pob, type = 'response' ) ]\n\n\n\n\n\n\n\n\n\n1. Bickel PJ, Doksum KA (2015) Mathematical statistics: Basic ideas and selected topics. CRC Press",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Modelos lineales generalizados (GLM)</span>"
    ]
  },
  {
    "objectID": "lectura_09.html",
    "href": "lectura_09.html",
    "title": "10  Reservas técnicas",
    "section": "",
    "text": "10.1 Reservas para primas no adquiridas",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Reservas técnicas</span>"
    ]
  },
  {
    "objectID": "lectura_09.html#reservas-de-riesgos-en-curso",
    "href": "lectura_09.html#reservas-de-riesgos-en-curso",
    "title": "10  Reservas técnicas",
    "section": "10.2 Reservas de riesgos en curso",
    "text": "10.2 Reservas de riesgos en curso\nEsta reserva corresponde a la cantidad restante prima que debería ser prorrateada en el tiempo para cubrir de forma uniforme los reclamos de cada póliza hasta su fecha término. En otras palabras, si tenemos una cantidad de pólizas vendidas \\(n\\), a cada póliza \\(i \\in \\{1, \\ldots, n \\}\\), le corresponde una duración \\(D_i = \\left[ T^1_i,  T^2_i \\right]\\), donde \\(T^1_i &gt; 0\\) el tiempo inicial de la vigencia de la póliza, \\(T^2_i &gt; T^1_i\\) el tiempo final de vigencia de la póliza \\(i\\)-ésima y una prima pagada por la cobertura adquirida \\(\\Pi_i\\) por la venta de la \\(i\\)-ésima póliza.\nAsí, la forma más sencilla de calcular la reserva de riesgos en curso \\(RRC\\) en el tiempo \\(t\\), está dada por: \\[\nRRC( t ) = \\sum\\limits_{i=1}^n \\mathbf{1}_{D_i}( t ) \\frac{T^2_i - t}{T^2_i - T^1_i} \\Pi_i\n\\]\nEsta metodología es la más sencilla, pero no toma en cuenta que no siempre sucede que los reclamos son proporcionales a la cantidad de prima restante en el tiempo. Incluso en el caso que el valor de los reclamos sea proporcional al tiempo restante, no necesariamente será uniformemente proporcional al tiempo restante de cobertura. Esto puede llevar a manera niveles de reservas de primas no óptimos acorde a las operaciones del asegurador.\nLa cantidad de prima que va quedando a favor del asegurador y de la cual en teoría este puede disponer está dada por la prima devengada \\(PDG\\): \\[\nPDG( t ) = \\sum\\limits_{i=1}^n \\mathbf{1}_{\\left[T^1_i, +\\infty\\right)}( t )\n\\frac{\\min\\left( t, T^2_i \\right) - T^1_i}{T^2_i - T^1_i} \\Pi_i\n\\]\nEn algunas situaciones se suele considerar tener reserva de primas por el mismo valor constante a los largo de la duración de la póliza. Usualmente esto se lo suele hacer para los productos de corto plazo con duraciones menores de un año. Así se suele dividir el conjunto de pólizas \\(\\{1,\\ldots,n\\} = I \\cup J\\) en dos conjuntos disjuntos, donde \\(I\\) contiene a las pólizas para las cuales se reserva proporcionalmente al tiempo y la prima y \\(J\\) contiene aquellas pólizas para las cuales se mantiene la reserva constante por toda la duración de la póliza. \\[\nRRC( t ) = \\sum\\limits_{i \\in I} \\mathbf{1}_{D_i}( t ) \\frac{T^2_i - t}{T^2_i - T^1_i} \\Pi_i\n+ \\sum\\limits_{i \\in J} \\mathbf{1}_{D_i}( t ) \\Pi_i\n\\] de igual manera la prima devengada \\(PDG\\) está dada por: \\[\nPDG( t ) = \\sum\\limits_{i \\in I} \\mathbf{1}_{\\left[T^1_i, +\\infty\\right)}( t )\n\\frac{\\min\\left( t, T^2_i \\right) - T^1_i}{T^2_i - T^1_i} \\Pi_i\n+ \\sum\\limits_{i \\in J} \\mathbf{1}_{\\left(T^2_i,+\\infty\\right)}( t ) \\Pi_i\n\\]\nPara dar mayor realidad al cálculo de la reserva de riesgos en curso \\(RRC\\), podemos incluir los costos de adquisición \\(A_i\\) por cada una de las pólizas vendidas \\(i \\in \\{1, \\ldots, n \\}\\). Usualmente, este valor ya se incluye en el valor de la prima, pero es directamente utilizado para solventar los costos de adquisición. De ahí que resulta razonable quitar estos costos de la prima, para así ajustar la \\(RRC\\). \\[\nRRC( t ) = \\sum\\limits_{i \\in I} \\mathbf{1}_{D_i}( t ) \\frac{T^2_i - t}{T^2_i - T^1_i} \\left( \\Pi_i - A_i \\right)\n+ \\sum\\limits_{i \\in J} \\mathbf{1}_{D_i}( t ) \\left( \\Pi_i - A_i \\right)\n\\]\ny de igual forma la prima devengada \\[\nPDG( t ) = \\sum\\limits_{i \\in I} \\mathbf{1}_{\\left[T^1_i, +\\infty\\right)}( t )\n\\frac{\\min\\left( t, T^2_i \\right) - T^1_i}{T^2_i - T^1_i} \\left( \\Pi_i - A_i \\right)\n+ \\sum\\limits_{i \\in J} \\mathbf{1}_{\\left(T^2_i,+\\infty\\right)}( t ) \\left( \\Pi_i - A_i \\right)\n\\]",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Reservas técnicas</span>"
    ]
  },
  {
    "objectID": "lectura_09.html#reservas-de-siniestros-a-pagar",
    "href": "lectura_09.html#reservas-de-siniestros-a-pagar",
    "title": "10  Reservas técnicas",
    "section": "10.3 Reservas de siniestros a pagar",
    "text": "10.3 Reservas de siniestros a pagar\n\n\\(T\\)\n\n\n\n10.3.1 Siniestros sucedidos, reportados, resueltos, no pagados\n\n\n\n10.3.2 Siniestros sucedidos, reportados, no resueltos, no pagados\n\n\n\n10.3.3 Siniestros sucedidos, no reportados, no resueltos, no pagados\nLa siguiente lista, aunque no exhaustiva, citamos algunos de los métodos que se utiliza para estimar reserva \\(IBNR\\), para más detalle se puede consultar [2] o [1].\n\nChain-Ladder\nChain-Ladder con mínimos cuadrados\nBenktander-Hovinen\nCape-Cod\nMack\nModelos Bayesianos\nMarkov chain Monte Carlo\nBühlmann-Straub\nModelos basados en distribuciones\nGLM\n\n\\[\nIBNR\n\\]\n\n\n\n\n\n\n1. Mikosch T (2009) Non-Life Insurance Mathematics: An introduction with the Poisson Process, 2nd ed. Springer\n\n\n2. Wüthrich MV, Merz M (2008) Stochastic claims reserving. John Wiley; Sons Ltd., England",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Reservas técnicas</span>"
    ]
  },
  {
    "objectID": "lectura_10.html",
    "href": "lectura_10.html",
    "title": "11  Estimación de valores extremos",
    "section": "",
    "text": "11.1 Estimación con valores extremos\n::::\nComo consecuencia de la definición anterior, las variables aleatoria a valores reales que tiene cola pesada no poseen función generadora de momentos ya que la integral a continuación diverge para cualquier valor \\(t &gt; 0\\) \\[\n\\int\\limits_{\\R} \\exp( t x ) dF( x ) = + \\infty\n\\]\n::::\n::::\nEl teorema anterior 11.3 en cierta forma sugiere que si en el límite la distribución de exceso a partir de un valor de condicionamiento \\(u\\) cada vez se comporta más como una distribución de generalizada de Pareto. Entonces, la distribución de los máximos que se observen de una muestra de la variable aleatoria \\(X\\) tienen un comportamiento que puede ser descrito por una distribución de valores extremos generalizada.\nUn forma de estimar si hay la posibilidad de presentar valores extremos es tomando un corte a un nivel \\(u\\) de los valores observados de reclamos \\(X_1, \\ldots, X_n\\), es decir tomar de la muestra todos los valores \\(\\{ X_i \\mid X_i \\geq u \\}\\), a estos valores realizarles un test de ajuste a una distribución de Pareto generalizada \\(GPD( \\nu, \\beta, \\varepsilon )\\), si el test es aceptable y no se rechaza la hipótesis nula, entonces tenemos el resultado de aproximación anterior 11.3. Luego se puede simular los reclamos de forma condicional.\nEs importante observar antes de continuar que para estudiar los valores extremos de reclamos estos no deben haber pasado por alguna deducción, después de aplicar alguna función deducible \\(D\\), sino posiblemente estaremos en el caso inútil de tener valores truncados, con una cola limitada, para los cuales es imposible estimar de forma adecuada alguna distribución de valores extremos.\nPara generar un modelo utilizando valores extremos se puede tomar la siguiente aproximación haciendo uso 11.3",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Estimación de valores extremos</span>"
    ]
  },
  {
    "objectID": "lectura_10.html#estimación-con-valores-extremos",
    "href": "lectura_10.html#estimación-con-valores-extremos",
    "title": "11  Estimación de valores extremos",
    "section": "",
    "text": "Seleccionar un valor de corte \\(u &gt; 0\\).\nA partir de la muestra de valores de reclamos observados, tomar aquellos valores observados tales que \\(X_i \\geq u\\).\nAl conjunto \\(\\{ X_i \\mid X_i \\geq u \\}\\) aplicarle un test de ajuste a una distribución de Pareto generalizada \\(GPD( u, \\beta, \\varepsilon )\\), donde \\(u\\) está fijo como parámetro.\nSi el test es satisfactorio, buscar un nuevo \\(u\\), realizar nuevamente los pasos 2 y 3. Comparar el resultado de ajuste con el anterior y decidir cuál \\(u\\) mantener según algún criterio de selección estadística. Usualmente un criterio de información o la significancia de los valores \\(p\\)\nSi llegamos a obtener un resultado satisfactorio para algún \\(u &gt; 0\\), podemos concluir con cierto nivel de certeza, que los valores extremos de los reclamos si pueden ser descritos con una distribución de valores extremos generalizada \\(GEV( \\mu, \\sigma, \\varepsilon )\\) o de forma equivalente que los reclamos en la cola \\(X \\geq u\\) pueden ser descritos con una distribución de Pareto generalizada \\(GPD( u, \\beta, \\varepsilon )\\).\nSe ajusta una distribución de severidad \\(F\\) a todos los reclamos en la muestra \\(X\\). La distribución que se ajusta, debe estar precisamente dentro de la familia de funciones que están el dominio máximo de atracción de la familia de distribuciones de valores extremos generalizadas, i.e. \\(F \\in MDA( GEV( \\mu, \\sigma, \\varepsilon ) )\\).\nSi la prueba anterior fue satisfactoria para algún \\(u &gt; 0\\). Se determina a que percentil \\(p_u\\) pertenece, esto se lo puede realizar a partir de la muestra mismo \\(F_n( X \\leq u ) = p_u\\) o con la distribución ajustada para los reclamos \\(F( X \\leq u ) = p_u\\).",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Estimación de valores extremos</span>"
    ]
  },
  {
    "objectID": "lectura_10.html#simulación-con-valores-extremos",
    "href": "lectura_10.html#simulación-con-valores-extremos",
    "title": "11  Estimación de valores extremos",
    "section": "11.2 Simulación con valores extremos",
    "text": "11.2 Simulación con valores extremos\nSi en caso se desea simular los reclamos, se toma con probabilidad \\(p_u\\) un muestra con la distribución truncada \\(X \\rightsquigarrow F_{(u)}( x ) = F( x \\mid X \\leq u ) = \\frac{F(\\min(u,x))}{F(u)}\\) y con probabilidad \\(1 - p_u\\) una muestra con la distribución de Pareto generalizada estimada \\(X \\rightsquigarrow G = GPD( u, \\beta, \\varepsilon )\\).\nAsí, en particular al esperanza de \\(X\\) tiene al forma \\[\n\\E[ X ]\n= \\E[ X \\mid X \\leq u ] P( X \\leq u ) + \\E[ X \\mid X &gt; u ] P( X &gt; u )\n= p_u \\E_{F_{(u)}}[ X ] + \\left( 1 - p_u \\right) \\E_{G}[ X ]\n\\]\nSi la muestra inicial es lo suficientemente representable y por tanto tiene un buen tamaño, el valor \\(p_u\\) puede ser realmente cercano a \\(1\\), por tal razón si se decide realizar una simulación de valores extremos de tamaño \\(m\\), se tiene que tomar en cuenta que en promedio \\(m ( 1 - p_u )\\) de esos valores serán valores extremos, si \\(m\\) no es lo suficientemente grande puede que los valores extremos simulados sean cero en cantidad y habrá sido inútil el haber realizado una estimación más compleja con valores extremos, ya que no se la está utilizando en las simulaciones. En tales circunstancias es mucho mejor utilizar algún algoritmo determinista que supere este problema.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Estimación de valores extremos</span>"
    ]
  }
]