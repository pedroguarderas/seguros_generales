<!------------------------------------------------------------------------------------------------->
# Distribuciones

<!------------------------------------------------------------------------------------------------->
## Distribuciones discretas

::: {.definition #dbinom name="Distribución binomial"}
Una variable aleatoria $N$ que toma valores en $\mathbb{N}$ se dice que sigue una distribución 
o ley de binomial $N \rightsquigarrow Bin( n, p )$, con parámetros $n \in \mathbb{N}$ y 
$p \in [0, 1]$, si:
\begin{equation}
P( N = k ) = \binom{n}{k} p^k ( 1 - p )^{n-k}, \qquad
\forall k \in \{0,\ldots, n\}
\end{equation}
esta distribución discreta se caracteriza por presentar el valor $k \frac{p_k}{p_{k-1}}$ decreciente
conforme cambia $k \in \mathbb{N}$
:::
```{r l3c1}
n <- 50
p <- 0.3
k <- 2
m <- 100
N <- rbinom( n = m, size = n, prob = p ) # simular una muestra de tamaño m
pk <- dbinom( x = k, size = n, prob = p ) # cálculo de probabilidad P( N = k )
Pk <- pbinom( q = k, size = n, prob = p ) # cálculo de probabilidad P( N <= k )

p <- dbinom( x = 0:n, size = n, prob = p )
v <- 1:n * p[ 2:(n + 1) ] / p[ 1:n ]

plt <- ggplot() +
  geom_point( aes( x = 1:n, y = v ), colour = 'darkred' ) + 
  xlab( TeX( "$k$" ) ) + 
  ylab( TeX( "$k \\frac{p_k}{p_{k-1}}$" ) ) + 
  theme_bw()
plot( plt )
```

::: {.definition #dpois name="Distribución de Poisson"}
Una variable aleatoria $N$ que toma valores en $\mathbb{N}$ se dice que sigue una distribución 
o ley de Poisson $N \rightsquigarrow Pois( n, p )$, con parámetro $\lambda \in \mathbb{R}$, si:
\begin{equation}
P( N = k ) = \exp\left( -\lambda \right) \frac{\lambda^k}{k!}, \qquad
\forall k \in \mathbb{N}
\end{equation}
esta distribución discreta se caracteriza por presentar el valor $k \frac{p_k}{p_{k-1}}$ constante
conforme cambia $k \in \mathbb{N}$
:::
```{r l3c2}
lambda <- 2
k <- 2
m <- 100
N <- rpois( n = m, lambda = lambda ) # simular una muestra de tamaño m
pk <- dpois( x = k, lambda = lambda ) # cálculo de probabilidad P( N = k )
Pk <- ppois( q = k, lambda = lambda ) # cálculo de probabilidad P( N <= k )

n <- 50
p <- dpois( x = 0:n, lambda = lambda )
v <- 1:n * p[ 2:(n + 1) ] / p[ 1:n ]

plt <- ggplot() +
  geom_point( aes( x = 1:n, y = v ), colour = 'darkred' ) + 
  xlab( TeX( "$k$" ) ) + 
  ylab( TeX( "$k \\frac{p_k}{p_{k-1}}$" ) ) + 
  theme_bw()
plot( plt )
```

::: {.definition #dnbinom name="Distribución binomial negativa"}
Una variable aleatoria $N$ que toma valores en $\mathbb{N}$ se dice que sigue una distribución 
o ley de  binomial negativa $N \rightsquigarrow NBin( \alpha, p )$, con parámetro $\alpha > 0$ y 
$p \in (0,1)$, si:
\begin{equation}
P( N = k ) = \binom{\alpha + k - 1}{k} p^\alpha ( 1 - p )^k
= \frac{\Gamma( \alpha + k )}{\Gamma(k+1) \Gamma(\alpha)}p^\alpha ( 1 - p )^k, \qquad
\forall k \in \mathbb{N}
\end{equation}
donde $\Gamma( \alpha ) = \int\limits_0^{+\infty} x^{\alpha - 1} \exp(-x)\ dx$, $\forall \alpha \geq 0$.

Esta distribución discreta se caracteriza por presentar el valor $k \frac{p_k}{p_{k-1}}$ creciente
conforme cambia $k \in \mathbb{N}$
:::
```{r l3c3}
alpha <- 2.5
p <- 0.3
k <- 2
m <- 100
N <- rnbinom( n = m, size = alpha, prob = p  ) # simular una muestra de tamaño m
pk <- dnbinom( x = k, size = alpha, prob = p ) # cálculo de probabilidad P( N = k )
Pk <- pnbinom( q = k, size = alpha, prob = p ) # cálculo de probabilidad P( N <= k )

n <- 50
p <- dnbinom( x = 0:n, size = alpha, prob = p )
v <- 1:n * p[ 2:(n + 1) ] / p[ 1:n ]

plt <- ggplot() +
  geom_point( aes( x = 1:n, y = v ), colour = 'darkred' ) + 
  xlab( TeX( "$k$" ) ) + 
  ylab( TeX( "$k \\frac{p_k}{p_{k-1}}$" ) ) + 
  theme_bw()
plot( plt )
```

::: {.definition #dgeo name="Distribución geométrica"}
Una variable aleatoria $N$ que toma valores en $\mathbb{N}$ se dice que sigue una distribución 
o ley geométrica $N \rightsquigarrow Geo( p )$, con parámetro $p \in (0,1]$, si:
\begin{equation}
P( N = k ) = ( 1 - p )^k p, \qquad
\forall k \in \mathbb{N}
\end{equation}
:::
```{r l3c4}
p <- 0.3
k <- 2
m <- 100
N <- rgeom( n = m, prob = p  ) # simular una muestra de tamaño m
pk <- dgeom( x = k, prob = p ) # cálculo de probabilidad P( N = k )
Pk <- pgeom( q = k, prob = p ) # cálculo de probabilidad P( N <= k )
```
Es fácil darse cuenta que la distribución geométrica $Geo( p )$ es una binomial negativa $BN( 1, p )$,
con $\alpha = 1$.

Asociado a estas distribuciones discretas existe un resultado de caracterización, el cual permite
seleccionar la distribución de conteo.

<!------------------------------------------------------------------------------------------------->
## Familia de Panjer

El criterio anterior para identificar el tipo de distribución, mediante la observación del
comportamiento de la variable $k \frac{p_k}{p_{k-1}}$, se formaliza precisamente en la definición
de la familia de Panjer.

::: {.definition #fampanjer name="Familia de Panjer"}
Una variable aleatoria discreta $N$, que toma valores enteros positivos $N \in \mathbb{N}$, se dice
que pertenece a la **familia de Panjer**, si sus probabilidades $p_k = P( N = k )$ para cada 
$k \in \mathbb{N}$, satisfacen la siguiente relación de recurrencia.
\begin{equation}
p_k = \left( a + \frac{b}{k} \right)p_{k-1},\qquad \forall k \in \mathbb{N} \setminus \{0\} 
\end{equation}
:::

Además tenemos la siguiente proposición que caracteriza a la distribución de las variables 
aleatorias en la familia de Panjer.

::: {.proposition #chfampanjer name="Caracterización familia de Panjer"}
Las únicas leyes de probabilidad que satisfacen la relación de recurrencia anterior son:

1. La ley de Poisson, la cual se obtiene para $a = 0$ y $b > 0$
\begin{equation}
k \frac{p_k}{p_{k-1}} = b > 0,\quad \text{constante en $k$}
\end{equation}

2. La ley binomial negativa, la cual se obtiene para $0 < a < 1$ y $a + b > 0$
\begin{equation}
k \frac{p_k}{p_{k-1}} = a k + b > 0,\quad \text{creciente en $k$}
\end{equation}

3. La ley binomial, la cual se obtenida para $a < 0$ y $b = -a(m + 1)$, para cierto $m$ entero y 
positivo.
\begin{equation}
k \frac{p_k}{p_{k-1}} = a( k - m - 1 ) < 0, \quad \text{decreciente en $k$}
\end{equation}
:::

Para una demostración detallada de la proposición anterior se puede consultar @MathAssuNV1 o 
en https://nonlifemaths.github.io/.


```{r l3t1, echo = TRUE, results = 'asis'}
# la librería CASdatasets fue previamente cargada
data( beMTPL97 )
beMTPL97 <- as.data.table( beMTPL97 )
conteo <- beMTPL97[ , list( fn = .N ), by = list( sex, fuel, N = nclaims ) ]
conteo[ , pn := fn / sum( fn ), by = list( sex, fuel ) ]
setorder( conteo, sex, fuel, N )
conteo[ , pns := shift( pn, type = 'lag', fill = 0 ) ]
conteo[ , jn := N * pns / pn ]

conteo %>%
  kable(
    caption = 'Estimación conteos por sexo',
    row.names = FALSE,
    col.names = c( "sexo", "fuel", "$N$", "$f_k$", "$p_k$", "$p_{k+1}$", "$k \\frac{p_{k+1}}{p_k}$" ),
    align = 'llrrrrr',
    digits = c( 0, 0, 0, 0, 5, 5, 5 ),
    format.args = list( big.mark = ',', decimal.mark = '.', scientific = FALSE ),
    escape = FALSE ) %>%
  kable_classic( font_size = 14, full_width = FALSE, html_font = "Cambria" ) %>%
  scroll_box( width = "750px", height = "500px" )
```

```{r l3c5}
plot( conteo$N, conteo$jn )
```

<!------------------------------------------------------------------------------------------------->
## Distribuciones continuas

::: {.definition #dunif name="Distribución uniforme"}
Una variable aleatoria $X$ a valores reales, sigue una distribución uniforme 
$X \rightsquigarrow Unif( a, b )$ de parámetros $a, b \in \mathbb{R}$, si su función de distribución
acumulada es de la siguiente forma:
\begin{equation}
F_X( x ) = \frac{x-a}{b-a} \mathbf{1}_{[a,b)}( x ) + \mathbf{1}_{[b,+\infty)}( x )
\end{equation}
sin mucho esfuerzo se puede verificar que su densidad de probabilidad está dada por la función
\begin{equation}
f_X( x ) = \frac{1}{b-a}\mathbf{1}_{[a,b]}( x )
\end{equation}
\begin{equation}
M_X( t ) = \frac{\exp(bt)-\exp(at)}{t(b-a)}
\end{equation}
\begin{equation}
\mathbb{E}[X] = \frac{a + b}{2},\qquad \mathbb{V}[X] = \frac{(b - a)^2}{12}
\end{equation}
:::
```{r l3c6}
a <- 1
b <- 2
x <- 1.5
m <- 100
X <- runif( n = m, min = a, max = b ) # simular una muestra de tamaño m
fx <- dunif( x = x, min = a, max = b ) # cálculo de la densidad f(x)
Fk <- punif( q = x, min = a, max = b ) # cálculo de probabilidad F(x)
```

::: {.definition #dexp name="Distribución exponencial"}
Una variable aleatoria $X$ a valores reales, sigue una distribución exponencial 
$X \rightsquigarrow Exp( \lambda )$ de parámetros $\lambda > 0$, si su función de distribución
acumulada es de la siguiente forma:
\begin{equation}
F_X( x ) = \mathbf{1}_{(0,+\infty)}( x ) \left( 1 - \exp\left( -\lambda x \right) \right)
\end{equation}
sin mucho esfuerzo se puede verificar que su densidad de probabilidad está dada por la función
\begin{equation}
f_X( x ) = \mathbf{1}_{(0,+\infty)}( x ) \lambda \exp\left( -\lambda x \right)
\end{equation}
\begin{equation}
M_X( t ) = \frac{\lambda}{\lambda - t}
\end{equation}
\begin{equation}
\mathbb{E}[X] = \frac{1}{\lambda},\qquad \mathbb{V}[X] = \frac{1}{\lambda^2}
\end{equation}
:::
```{r l3c7}
lambda <- 2
x <- 1.5
m <- 100
X <- rexp( n = m, rate = lambda ) # simular una muestra de tamaño m
fx <- dexp( x = x, rate = lambda ) # cálculo de la densidad f(x)
Fk <- pexp( q = x, rate = lambda ) # cálculo de probabilidad F(x)
```

::: {.definition #dnorm name="Distribución gamma"}
Una variable aleatoria $X$ a valores reales, sigue una distribución gamma 
$X \rightsquigarrow Gamma( \alpha, \beta )$ de parámetros $\alpha > 0$, $\beta > 0$, si su 
función de distribución acumulada es de la siguiente forma:
\begin{equation}
F_X( x ) = \frac{\beta^\alpha}{\Gamma( \alpha )} \int\limits_{0}^{x} u^{\alpha-1} \exp(-\beta u)\ du
\end{equation}
la densidad de probabilidad automáticamente está dada por la función:
\begin{equation}
f_X( x ) = \mathbf{1}_{[0,+\infty}( x ) \frac{\beta^\alpha}{\Gamma( \alpha )} x^{\alpha-1} \exp(-\beta x)
\end{equation}
\begin{equation}
M_X( t ) = \left( \frac{\beta}{\beta - t} \right)^\alpha,\qquad \text{si}\ t < \beta
\end{equation}
\begin{equation}
\mathbb{E}[X] = \frac{\alpha}{\beta},\qquad 
\mathbb{V}[X] = \frac{\alpha}{\beta^2}
\end{equation}
:::
```{r l3c8}
alpha <- 2
beta <- 1
x <- 3
m <- 100
X <- rgamma( n = m, shape = alpha, scale = beta ) # simular una muestra de tamaño m
fx <- dgamma( x = x, shape = alpha, scale = beta ) # cálculo de la densidad f(x)
Fk <- pgamma( q = x, shape = alpha, scale = beta ) # cálculo de probabilidad F(x)
```

::: {.definition #dnorm name="Distribución normal"}
Una variable aleatoria $X$ a valores reales, sigue una distribución normal 
$X \rightsquigarrow N( \mu, \sigma )$ de parámetros $\mu \in \mathbb{R}$, $\sigma > 0$, si su 
función de distribución acumulada es de la siguiente forma:
\begin{equation}
F_X( x ) = \frac{1}{\sqrt{2\pi} \sigma} \int\limits_{-\infty}^x \exp\left( -\frac{(y - \mu)^2}{\sigma^2} \right)\ dy
\end{equation}
la densidad de probabilidad automáticamente está dada por la función:
\begin{equation}
f_X( x ) = \frac{1}{\sqrt{2\pi}} \exp\left( -\frac{(x - \mu)^2}{\sigma^2} \right)
\end{equation}
\begin{equation}
M_X( t ) = \exp\left( t \mu + \frac{1}{2} t^2 \sigma^2 \right)
\end{equation}
\begin{equation}
\mathbb{E}[X] = \mu,\qquad \mathbb{V}[X] = \sigma^2
\end{equation}
:::
```{r l3c9}
mu <- 2
sigma <- 1
x <- 3
m <- 100
X <- rnorm( n = m, mean = mu, sd = sigma ) # simular una muestra de tamaño m
fx <- dnorm( x = x, mean = mu, sd = sigma ) # cálculo de la densidad f(x)
Fk <- pnorm( q = x, mean = mu, sd = sigma ) # cálculo de probabilidad F(x)
```

::: {.definition #dlnorm name="Distribución log-normal"}
Una variable aleatoria $X$ a valores reales, sigue una distribución log-normal 
$X \rightsquigarrow LN( \mu, \sigma )$ de parámetros $\mu > 0$, $\sigma > 0$, si su 
función de distribución acumulada es de la siguiente forma:
\begin{equation}
F_X( x ) = \frac{1}{\sqrt{2\pi} \sigma} \int\limits_{0}^{x} \frac{1}{y} \exp\left( -\frac{(\ln(y) - \mu)^2}{\sigma^2} \right)\ dy
\end{equation}
la densidad de probabilidad automáticamente está dada por la función:
\begin{equation}
f_X( x ) = \frac{1}{x\sqrt{2\pi} \sigma}  \exp\left( -\frac{(\ln(x) - \mu)^2}{\sigma^2} \right)
\end{equation}
No hay forma analítica para $M_X$
\begin{equation}
\mathbb{E}[X] = \exp\left( \mu + \frac{1}{2}\sigma^2 \right),\qquad 
\mathbb{V}[X] = \exp\left( 2 \mu + \sigma^2 \right) \left( \exp( \sigma^2 ) - 1 \right)
\end{equation}
:::
```{r l3c10}
mu <- 2
sigma <- 1
x <- 3
m <- 100
X <- rlnorm( n = m, meanlog = mu, sdlog = sigma ) # simular una muestra de tamaño m
fx <- dlnorm( x = x, meanlog = mu, sdlog = sigma ) # cálculo de la densidad f(x)
Fk <- plnorm( q = x, meanlog = mu, sdlog = sigma ) # cálculo de probabilidad F(x)
```
En pocas, una variable aleatoria $X \rightsquigarrow LN( \mu, \sigma )$ sigue una distribución 
log-normal si y solamente si la variable aleatoria dada por su logaritmo $\ln( X ) \rightsquigarrow N( \mu, \sigma )$ 
sigue una distribución normal.

::: {.definition #dgpd name="Distribución de Pareto generalizada"}
Una variable aleatoria $X$ a valores reales, sigue una distribución de Pareto generalizada
$X \rightsquigarrow GPD( \mu, \sigma, \xi )$ de parámetros $\mu \in \mathbb{R}, \sigma > 0, \xi \in \mathbb{R}$, 
si su función de distribución acumulada es de la siguiente forma:
\begin{equation}
F_X( x )
= \left \{
\begin{array}{ll}
1 - \left( 1 + \xi \frac{x-\mu}{\sigma} \right)^{-\frac{1}{\xi}} & \text{si}\ \xi \neq 0 \\
1 - \exp\left( -\frac{x-\mu}{\sigma} \right) & \text{si}\ \xi = 0
\end{array}
\right.
\end{equation}
y su densidad de probabilidad está dada por la función
\begin{equation}
f_X( x )
= \left \{
\begin{array}{ll}
\frac{1}{\sigma} \left( 1 + \xi \frac{x-\mu}{\sigma} \right)^{-1-\frac{1}{\xi}} & \text{si}\ \xi \neq 0 \\
\frac{1}{\sigma} \exp\left( -\frac{x-\mu}{\sigma} \right) & \text{si}\ \xi = 0
\end{array}
\right.
\end{equation}
\begin{equation}
M_X( t ) = \exp(\theta \mu) \sum\limits_{j=0}^{+\infty} \frac{\theta^j \sigma^j}
{\prod\limits_{k=0}^j ( 1 - k \xi )}
\end{equation}
:::
```{r l3c11}
xi <- 1
mu <- 2
sigma <- 1
x <- 3
m <- 100
X <- rgpd( n = m, xi = xi, mu = mu, beta = sigma ) # simular una muestra de tamaño m
fx <- dgpd( x = x, xi = xi, mu = mu, beta = sigma ) # cálculo de la densidad f(x)
Fk <- pgpd( q = x, xi = xi, mu = mu, beta = sigma ) # cálculo de probabilidad F(x)
```

::: {.definition #dgev  name="Distribución de valores extremos generalizada"}
Una variable aleatoria $X$ a valores reales, sigue una distribución generalizada de valores extremos
$X \rightsquigarrow GEV( \mu, \sigma, \xi )$ de parámetros $\mu \in \mathbb{R}, \sigma > 0, \xi \in \mathbb{R}$, 
si su función de distribución acumulada es de la siguiente forma:
\begin{equation}
F_X( x ) 
= \left\{
\begin{array}{ll}
\exp\left( -\exp\left( -\frac{x-\mu}{\sigma} \right) \right) 
& \text{si}\ \xi = 0 \\
\exp\left( -\left( 1 + \xi \frac{x-\mu}{\sigma} \right)^{-\frac{1}{\xi}} \right) 
& \text{si}\ \xi \neq 0, 1 + \xi\frac{x - \mu}{\sigma} > 0
\end{array}
\right.
\end{equation}
además se puede verificar que su densidad de probabilidad está dada por la función
\begin{equation}
f_X( x )
= \left\{
\begin{array}{ll}
\exp\left(-\frac{x-\mu}{\sigma}\right) 
\exp\left(-\exp\left(-\frac{x-\mu}{\sigma}\right)\right) 
& \text{si}\ \xi = 0 \\
\left( 1 + \xi \frac{x - \mu}{\sigma}\right)^{-1-\frac{1}{\xi}} 
\exp\left( -\left( 1 + \xi \frac{x-\mu}{\sigma} \right)^{-\frac{1}{\xi}} \right)
 & \text{si}\ \xi \neq 0, 1 + \xi\frac{x - \mu}{\sigma} > 0
\end{array}
\right.
\end{equation}
:::
```{r l3c12}
xi <- -1
mu <- 2
sigma <- 1
x <- 3
m <- 100
X <- rgev( n = m, xi = xi, mu = mu, beta = sigma ) # simular una muestra de tamaño m
fx <- dgev( x = x, xi = xi, mu = mu, beta = sigma ) # cálculo de la densidad f(x)
Fk <- pgev( q = x, xi = xi, mu = mu, beta = sigma ) # cálculo de probabilidad F(x)
```

::: {.definition #dtst  name="Distribución t de Student"}
Una variable aleatoria $X$ a valores reales, sigue una distribución t de Student
$X \rightsquigarrow t( \nu )$ de parámetros $\nu > 0$, 
si su función de distribución acumulada es de la siguiente forma:
\begin{equation}
F_X( x ) 
= \frac{1}{2} + \frac{x}{\sqrt{\pi \nu}} 
\frac{\Gamma\left( \frac{\nu + 1}{2} \right)}{\Gamma\left( \frac{\nu}{2} \right)}
F\left( \frac{1}{2}, \frac{\nu+1}{2}, \frac{3}{2}, -\frac{x^2}{\nu} \right)
\end{equation}
donde $F$ es la función hipergeométrica.
\begin{equation}
F( a, b, c, z ) = \sum\limits_{n=0}^{+\infty} \frac{(a)_n (b)_n}{(c)_n} \frac{z^n}{n!}
\end{equation}
con
\begin{equation}
(a)_n 
= \left\{
\begin{array}{ll}
1 & n = 0 \\
a( a + 1 ) \cdots (a + n - 1) & n > 0
\end{array}
\right.
\end{equation}
Además, se puede verificar que su densidad de probabilidad está dada por la función
\begin{equation}
f_X( x )
= \frac{x}{\sqrt{\pi \nu}} 
\frac{\Gamma\left(\frac{\nu+1}{2}\right)}{\Gamma\left( \frac{\nu}{2} \right)}
\left( 1 + \frac{x^2}{\nu} \right)^{-\frac{\nu+1}{2}}
\end{equation}
La función generadora de momentos $M_X( t )$ no está definida
\begin{equation}
\mathbb{E}[X] 
= \left\{
\begin{array}{ll}
0 & \text{si}\ \nu > 0 \\
\text{no definida} & \text{si}\ \nu \leq 0
\end{array}
\right.
\end{equation}

\begin{equation}
\mathbb{V}[X] 
= \left\{
\begin{array}{ll}
\frac{\nu}{\nu-2} & \text{si}\ \nu > 2 \\
+\infty & \text{si}\ 1 < \nu \leq 2 \\
\text{no definida} & \text{si}\ \nu \leq 1
\end{array}
\right.
\end{equation}
:::
```{r l3c13}
nu <- 3
x <- 4
m <- 100
X <- rt( n = m, df = nu ) # simular una muestra de tamaño m
fx <- dt( x = x, df = nu ) # cálculo de la densidad f(x)
Fk <- pt( q = x, df = nu ) # cálculo de probabilidad F(x)
```

::: {.definition #dgammatra  name="Distribución gamma transformada"}
Una variable aleatoria $X$ a valores reales, sigue una distribución gamma transformada
$X \rightsquigarrow GT( \alpha, \tau, \theta )$ de parámetros $\alpha > 0, \tau > 0, \theta > 0$, 
si su función de distribución acumulada es de la siguiente forma:
\begin{equation}
F_X( x ) = \frac{\tau}{\Gamma( \alpha )}
\int\limits_{0}^x 
\frac{1}{u} \left( \frac{u}{\theta} \right)^{\alpha} \exp\left(-\left( \frac{u}{\theta} \right)^{\tau}\right)\ du
\end{equation}

además se puede verificar que su densidad de probabilidad está dada por la función:
\begin{equation}
f_X( x )
= 
\left\{
\begin{array}{ll}
0 & \text{si}\ x \leq 0 \\
\frac{\tau}{x \Gamma( \alpha )} \left( \frac{x}{\theta} \right)^{\alpha} \exp\left(-\left( \frac{x}{\theta} \right)^{\tau}\right) & \text{si}\ x > 0
\end{array}
\right.
\end{equation}
\begin{eqnarray*}
\mathbb{E}[X^k] 
& = & \frac{\theta^k \Gamma\left( \alpha + \frac{k}{\tau}\right)}{\Gamma( \alpha )},
\quad \text{si}\ k > -\alpha \tau \\
\mathbb{E}[X]
& = & \frac{\theta \Gamma\left( \alpha + \frac{1}{\tau}\right)}{\Gamma( \alpha )},
\quad \text{si}\ 1 > -\alpha \tau \\
\mathbb{V}[X] 
& = & \frac{\theta^2 \Gamma\left( \alpha + \frac{2}{\tau}\right)}{\Gamma( \alpha )}
- \frac{\theta^2 \Gamma\left( \alpha + \frac{1}{\tau}\right)^2}{\Gamma( \alpha )^2}
\end{eqnarray*}
:::
```{r l3c14}
alpha <- 1
tau <- 1
theta <- 1
x <- 3
m <- 100
X <- rtrgamma( n = m, shape1 = alpha, shape2 = tau, scale = theta ) # simular una muestra de tamaño m
fx <- dtrgamma( x = x, shape1 = alpha, shape2 = tau, scale = theta ) # cálculo de la densidad f(x)
Fk <- ptrgamma( q = x, shape1 = alpha, shape2 = tau, scale = theta ) # cálculo de probabilidad F(x)
```

En la familia gamma se incluyen las siguientes distribuciones:

1. La distribución inversa gamma transformada, es decir es una familia estable por inversión

2. La distribución gamma para $\alpha = n/2$ y $\theta = 2$

3. La distribución inversa gamma

4. La distribución de Weibull

5. La distribución inversa de Weibull

6. La distribución exponencial

7. La distribución inversa exponencial

::: {.definition #dbetra  name="Distribución beta transformada"}
Una variable aleatoria $X$ a valores reales, sigue una distribución beta transformada
$X \rightsquigarrow BT( \alpha, \gamma, \tau, \theta )$ de parámetros $\alpha > 0, \gamma > 0, \tau > 0, \theta > 0$, 
si su función de distribución acumulada es de la siguiente forma:
\begin{equation}
F_X( x ) = \frac{\Gamma(\alpha + \tau)}{\Gamma( \alpha ) \Gamma( \tau )}
\int\limits_0^x
\frac{\gamma \left( \frac{u}{\theta} \right)^{\gamma \tau}}{u\left( 1 + \left( \frac{u}{\theta} \right)^{\gamma}\right)^{\alpha + \tau}}\ du
\end{equation}

además se puede verificar que su densidad de probabilidad está dada por la función:
\begin{equation}
f_X( x )
= \mathbf{1}_{[0,+\infty)}( x )
\frac{\Gamma(\alpha + \tau)}{\Gamma( \alpha ) \Gamma( \tau )} 
\frac{ \gamma \left( \frac{x}{\theta} \right)^{\gamma \tau}}{x\left( 1 + \left( \frac{x}{\theta} \right)^{\gamma}\right)^{\alpha + \tau}}
\end{equation}

\begin{eqnarray*}
\mathbb{E}[X^k] 
& = & \frac{\theta^k \Gamma\left( \tau + \frac{k}{\gamma}\right) \Gamma\left( \tau - \frac{k}{\gamma}\right)}{\Gamma( \alpha ) \Gamma( \tau )},
\quad \text{si}\ -\tau \gamma < k < \tau \gamma \\
\mathbb{E}[X]
& = & \frac{\theta \Gamma\left( \tau + \frac{1}{\gamma}\right) \Gamma\left( \tau - \frac{1}{\gamma}\right)}{\Gamma( \alpha ) \Gamma( \tau )} \\
\mathbb{V}[X] 
& = & \frac{\theta^2 \Gamma\left( \tau + \frac{2}{\gamma}\right) \Gamma\left( \tau - \frac{2}{\gamma}\right)}{\Gamma( \alpha ) \Gamma( \tau )}
- \frac{\theta^2 \Gamma\left( \tau + \frac{1}{\gamma}\right)^2 \Gamma\left( \tau - \frac{1}{\gamma}\right)^2}{\Gamma( \alpha )^2 \Gamma( \tau )^2}
\end{eqnarray*}
:::
```{r l3c15}
alpha <- 1
gamma <- 1
tau <- 1
theta <- 1
x <- 3
m <- 100
X <- rtrbeta( n = m, shape1 = alpha, shape2 = gamma, shape3 = tau, scale = theta ) # simular una muestra de tamaño m
fx <- dtrbeta( x = x, shape1 = alpha, shape2 = gamma, shape3 = tau, scale = theta ) # cálculo de la densidad f(x)
Fk <- ptrbeta( q = x, shape1 = alpha, shape2 = gamma, shape3 = tau, scale = theta ) # cálculo de probabilidad F(x)
```

Dentro de la familia beta transformada se cuenta algunas distribuciones de probabilidad:

1. La distribución de Burr para $\tau = 1$

2. La distribución de log-logística para $\alpha = \tau = 1$

3. La distribución de paralogística para $\alpha = \gamma, \tau = 1$

4. La distribución de generalizada de Pareto para $\gamma = 1$

5. La distribución de Pareto para $\gamma = \tau = 1$

6. La distribución de inversa de Burr para $\alpha = 1$

7. La distribución de inversa de Pareto para $\alpha = \gamma = 1$

8. La distribución de inversa paralogística para $\alpha = 1, \gamma = \tau$

La distribución transformada gamma es un caso límite de la distribución transformada beta, cuando
$\theta \rightarrow +\infty, \alpha \rightarrow +\infty$ y 
$\theta \alpha^{-\frac{1}{\gamma}} \rightarrow \xi$


<!------------------------------------------------------------------------------------------------->
## Estimación

En la práctica se observa la realización de una variable aleatoria $X$, es decir se tiene una 
muestra de la misma $X_1, \ldots, X_n$. Pero, no se dispone de la distribución $F$ o de la densidad 
$f$ que la describe. Como ya hemos mencionado, conociendo la distribución se puede inferir algunas
propiedades sobre la variable. De ahí surge la necesidad de buscar la mejor distribución $F$ posible
a partir de la muestra. 

Para ello se ha formulado diferentes aproximaciones, entre las cuales citamos las siguientes:

1. Método de sustitución

2. Método de los momentos

3. Método de la distancia mínima

4. Método de maximización de la verosimilitud

::: {.definition #bestinses  name="Estimador insesgado"}
Se dice que un estimador $\theta_1$ se dice **insesgado** para estimar el parámetro $\theta$, si se 
cumple que:
\begin{equation}
\mathbb{E}\left[ \theta_1 \right] = \theta
\end{equation}
caso contrario se dirá que el estimador es **sesgado** o que presenta un **error sistemático**.
:::

::: {.definition #bestcuadmean  name="Mejor en media cuadrática"}
Se dice que un estimador $\theta_1$ es mejor estimador en media cuadrática que el estimador 
$\theta_2$, respecto del parámetro $\theta$, si se cumple la siguiente desigualdad
\begin{equation}
\mathbb{E}\left[ \left( \theta_1 - \theta \right)^2 \right]
< \mathbb{E}\left[ \left( \theta_2 - \theta \right)^2 \right]
\end{equation}
:::



