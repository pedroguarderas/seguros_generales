<!------------------------------------------------------------------------------------------------->
# Distribuciones

<!------------------------------------------------------------------------------------------------->
## Distribuciones discretas

::: {.definition #dbinom name="Distribución binomial"}
Una variable aleatoria $N$ que toma valores en $\mathbb{N}$ se dice que sigue una distribución 
o ley de binomial $N \rightsquigarrow Bin( n, p )$, con parámetros $n \in \mathbb{N}$ y 
$p \in [0, 1]$, si:
\begin{equation}
P( N = k ) = \binom{n}{k} p^k ( 1 - p )^{n-k}, \qquad
\forall k \in \{0,\ldots, n\}
\end{equation}
:::
```{r l4c1}
n <- 10
p <- 0.3
k <- 2
m <- 100
N <- rbinom( n = m, size = n, prob = p ) # simular una muestra de tamaño m
pk <- dbinom( x = k, size = n, prob = p ) # cálculo de probabilidad P( N = k )
Pk <- pbinom( q = k, size = n, prob = p ) # cálculo de probabilidad P( N <= k )
```

::: {.definition #dpois name="Distribución de Poisson"}
Una variable aleatoria $N$ que toma valores en $\mathbb{N}$ se dice que sigue una distribución 
o ley de Poisson $N \rightsquigarrow Pois( n, p )$, con parámetro $\lambda \in \mathbb{R}$, si:
\begin{equation}
P( N = k ) = \exp\left( -\lambda \right) \frac{\lambda^k}{k!}, \qquad
\forall k \in \mathbb{N}
\end{equation}
:::
```{r l4c2}
lambda <- 2
k <- 2
m <- 100
N <- rpois( n = m, lambda = lambda ) # simular una muestra de tamaño m
pk <- dpois( k, lambda = lambda ) # cálculo de probabilidad P( N = k )
Pk <- ppois( k, lambda = lambda ) # cálculo de probabilidad P( N <= k )
```

::: {.definition #dnbinom name="Distribución binomial negativa"}
Una variable aleatoria $N$ que toma valores en $\mathbb{N}$ se dice que sigue una distribución 
o ley de  binomial negativa $N \rightsquigarrow NBin( \alpha, p )$, con parámetro $\alpha > 0$ y 
$p \in (0,1)$, si:
\begin{equation}
P( N = k ) = \binom{\alpha + k - 1}{k} p^\alpha ( 1 - p )^k
= \frac{\Gamma( \alpha + k )}{\Gamma(k+1) \Gamma(\alpha)}p^\alpha ( 1 - p )^k, \qquad
\forall k \in \mathbb{N}
\end{equation}
donde $\Gamma( \alpha ) = \int\limits_0^{+\infty} x^{\alpha - 1} \exp(-x)\ dx$, $\forall \alpha \geq 0$
:::
```{r l4c3}
n <- 10
p <- 0.3
k <- 2
m <- 100
N <- rnbinom( n = m, size = n, prob = p  ) # simular una muestra de tamaño m
pk <- dnbinom( x = k, size = n, prob = p ) # cálculo de probabilidad P( N = k )
Pk <- pnbinom( q = k, size = n, prob = p ) # cálculo de probabilidad P( N <= k )
```

Asociado a estas distribuciones discretas existe un resultado de caracterización, el cual permite
seleccionar la distribución de conteo.

<!------------------------------------------------------------------------------------------------->
## Distribuciones continuas

::: {.definition #dunif name="Distribución uniforme"}
Una variable aleatoria $X$ a valores reales, sigue una distribución uniforme 
$X \rightsquigarrow Unif( a, b )$ de parámetros $a, b \in \mathbb{R}$, si su función de distribución
acumulada es de la siguiente forma:
\begin{equation}
F_X( x ) = \frac{x-a}{b-a} \mathbf{1}_{[a,b)}( x ) + \mathbf{1}_{[b,+\infty)}( x )
\end{equation}
sin mucho esfuerzo se puede verificar que su densidad de probabilidad está dada por la función
\begin{equation}
f_X( x ) = \frac{1}{b-a}\mathbf{1}_{[a,b]}( x )
\end{equation}
\begin{equation}
M_X( t ) = \frac{\exp(bt)-\exp(at)}{t(b-a)}
\end{equation}
\begin{equation}
\mathbb{E}[X] = \frac{a + b}{2},\qquad \mathbb{V}[X] = \frac{(b - a)^2}{12}
\end{equation}
:::
```{r l4c4}
a <- 1
b <- 2
x <- 1.5
m <- 100
X <- runif( n = m, min = a, max = b ) # simular una muestra de tamaño m
fx <- dunif( x = x, min = a, max = b ) # cálculo de probabilidad f(x)
Fk <- punif( q = x, min = a, max = b ) # cálculo de probabilidad F(x)
```

::: {.definition #dexp name="Distribución exponencial"}
Una variable aleatoria $X$ a valores reales, sigue una distribución exponencial 
$X \rightsquigarrow Exp( \lambda )$ de parámetros $\lambda > 0$, si su función de distribución
acumulada es de la siguiente forma:
\begin{equation}
F_X( x ) = \mathbf{1}_{(0,+\infty)}( x ) \left( 1 - \exp\left( -\lambda x \right) \right)
\end{equation}
sin mucho esfuerzo se puede verificar que su densidad de probabilidad está dada por la función
\begin{equation}
f_X( x ) = \mathbf{1}_{(0,+\infty)}( x ) \lambda \exp\left( -\lambda x \right)
\end{equation}
\begin{equation}
M_X( t ) = \frac{\lambda}{\lambda - t}
\end{equation}
\begin{equation}
\mathbb{E}[X] = \frac{1}{\lambda},\qquad \mathbb{V}[X] = \frac{1}{\lambda^2}
\end{equation}
:::
```{r l4c5}
lambda <- 2
x <- 1.5
m <- 100
X <- rexp( n = m, rate = lambda ) # simular una muestra de tamaño m
fx <- dexp( x = x, rate = lambda ) # cálculo de probabilidad f(x)
Fk <- pexp( q = x, rate = lambda ) # cálculo de probabilidad F(x)
```

::: {.definition #dnorm name="Distribución normal"}
Una variable aleatoria $X$ a valores reales, sigue una distribución normal 
$X \rightsquigarrow N( \mu, \sigma )$ de parámetros $\mu \in \mathbb{R}$, $\sigma > 0$, si su 
función de distribución acumulada es de la siguiente forma:
\begin{equation}
F_X( x ) = \frac{1}{\sqrt{2\pi} \sigma} \int\limits_{-\infty}^x \exp\left( -\frac{(y - \mu)^2}{\sigma^2} \right)\ dy
\end{equation}
la densidad de probabilidad automáticamente está dada por la función:
\begin{equation}
f_X( x ) = \frac{1}{\sqrt{2\pi}} \exp\left( -\frac{(x - \mu)^2}{\sigma^2} \right)
\end{equation}
\begin{equation}
M_X( t ) = \exp\left( t \mu + \frac{1}{2} t^2 \sigma^2 \right)
\end{equation}
\begin{equation}
\mathbb{E}[X] = \mu,\qquad \mathbb{V}[X] = \sigma^2
\end{equation}
:::
```{r l4c6}
mu <- 2
sigma <- 1
x <- 3
m <- 100
X <- rnorm( n = m, mean = mu, sd = sigma ) # simular una muestra de tamaño m
fx <- dnorm( x = x, mean = mu, sd = sigma ) # cálculo de probabilidad f(x)
Fk <- pnorm( q = x, mean = mu, sd = sigma ) # cálculo de probabilidad F(x)
```

::: {.definition #dlnorm name="Distribución log-normal"}
Una variable aleatoria $X$ a valores reales, sigue una distribución log-normal 
$X \rightsquigarrow LN( \mu, \sigma )$ de parámetros $\mu > 0$, $\sigma > 0$, si su 
función de distribución acumulada es de la siguiente forma:
\begin{equation}
F_X( x ) = \frac{1}{\sqrt{2\pi} \sigma} \int\limits_{0}^{\ln(x)} \exp\left( -\frac{(y - \mu)^2}{\sigma^2} \right)\ dy
\end{equation}
la densidad de probabilidad automáticamente está dada por la función:
\begin{equation}
f_X( x ) = \frac{1}{x\sqrt{2\pi} \sigma}  \exp\left( -\frac{(\ln(x) - \mu)^2}{\sigma^2} \right)
\end{equation}
No hay forma analítica para $M_X$
\begin{equation}
\mathbb{E}[X] = \exp\left( \mu + \frac{1}{2}\sigma^2 \right),\qquad 
\mathbb{V}[X] = \exp\left( 2 \mu + \sigma^2 \right) \left( \exp( \sigma^2 ) - 1 \right)
\end{equation}
:::
```{r l4c7}
mu <- 2
sigma <- 1
x <- 3
m <- 100
X <- rlnorm( n = m, meanlog = mu, sdlog = sigma ) # simular una muestra de tamaño m
fx <- dlnorm( x = x, meanlog = mu, sdlog = sigma ) # cálculo de probabilidad f(x)
Fk <- plnorm( q = x, meanlog = mu, sdlog = sigma ) # cálculo de probabilidad F(x)
```
En pocas, una variable aleatoria $X \rightsquigarrow LN( \mu, \sigma )$ sigue una distribución 
log-normal si y solamente si la variable aleatoria dada por su logaritmo $\ln( X ) \rightsquigarrow N( \mu, \sigma )$ 
sigue una distribución normal.

::: {.definition #dgpd name="Distribución de Pareto generalizada"}
Una variable aleatoria $X$ a valores reales, sigue una distribución de Pareto generalizada
$X \rightsquigarrow GPD( \mu, \sigma, \xi )$ de parámetros $\mu \in \mathbb{R}, \sigma > 0, \xi \in \mathbb{R}$, 
si su función de distribución acumulada es de la siguiente forma:
\begin{equation}
F_X( x )
= \left \{
\begin{array}{ll}
1 - \left( 1 + \xi \frac{x-\mu}{\sigma} \right)^{-\frac{1}{\xi}} & \text{si}\ \xi \neq 0 \\
1 - \exp\left( -\frac{x-\mu}{\sigma} \right) & \text{si}\ \xi = 0
\end{array}
\right.
\end{equation}
y su densidad de probabilidad está dada por la función
\begin{equation}
f_X( x )
= \left \{
\begin{array}{ll}
\frac{1}{\sigma} \left( 1 + \xi \frac{x-\mu}{\sigma} \right)^{-1-\frac{1}{\xi}} & \text{si}\ \xi \neq 0 \\
\frac{1}{\sigma} \exp\left( -\frac{x-\mu}{\sigma} \right) & \text{si}\ \xi = 0
\end{array}
\right.
\end{equation}
:::
```{r l4c8}
xi <- 1
mu <- 2
sigma <- 1
x <- 3
m <- 100
X <- rgpd( n = m, xi = xi, mu = mu, beta = sigma ) # simular una muestra de tamaño m
fx <- dgpd( x = x, xi = xi, mu = mu, beta = sigma ) # cálculo de probabilidad f(x)
Fk <- pgpd( q = x, xi = xi, mu = mu, beta = sigma ) # cálculo de probabilidad F(x)
```

::: {.definition #dgev  name="Distribución de valores extremos generalizada"}
Una variable aleatoria $X$ a valores reales, sigue una distribución generalizada de valores extremos
$X \rightsquigarrow GEV( \mu, \sigma, \xi )$ de parámetros $\mu \in \mathbb{R}, \sigma > 0, \xi \in \mathbb{R}$, 
si su función de distribución acumulada es de la siguiente forma:
\begin{equation}
F_X( x ) 
= \left\{
\begin{array}{ll}
\exp\left( -\exp\left( -\frac{x-\mu}{\sigma} \right) \right) 
& \text{si}\ \xi = 0 \\
\exp\left( -\left( 1 + \xi \frac{x-\mu}{\sigma} \right)^{-\frac{1}{\xi}} \right) 
& \text{si}\ \xi \neq 0, 1 + \xi\frac{x - \mu}{\sigma} > 0
\end{array}
\right.
\end{equation}
además se puede verificar que su densidad de probabilidad está dada por la función
\begin{equation}
f_X( x )
= \left\{
\begin{array}{ll}
\exp\left(-\frac{x-\mu}{\sigma}\right) 
\exp\left(-\exp\left(-\frac{x-\mu}{\sigma}\right)\right) 
& \text{si}\ \xi = 0 \\
\left( 1 + \xi \frac{x - \mu}{\sigma}\right)^{-1-\frac{1}{\xi}} 
\exp\left( -\left( 1 + \xi \frac{x-\mu}{\sigma} \right)^{-\frac{1}{\xi}} \right)
 & \text{si}\ \xi \neq 0, 1 + \xi\frac{x - \mu}{\sigma} > 0
\end{array}
\right.
\end{equation}
:::
```{r l4c9}
xi <- -1
mu <- 2
sigma <- 1
x <- 3
m <- 100
X <- rgev( n = m, xi = xi, mu = mu, beta = sigma ) # simular una muestra de tamaño m
fx <- dgev( x = x, xi = xi, mu = mu, beta = sigma ) # cálculo de probabilidad f(x)
Fk <- pgev( q = x, xi = xi, mu = mu, beta = sigma ) # cálculo de probabilidad F(x)
```

<!------------------------------------------------------------------------------------------------->
## Familia de Panjer

::: {.definition #fampanjer name="Familia de Panjer"}
Una variable aleatoria discreta $N$, que toma valores enteros positivos $N \in \mathbb{N}$, se dice
que pertenece a la **familia de Panjer**, si sus probabilidades $p_k = P( N = k )$ para cada 
$k \in \mathbb{N}$, satisfacen la siguiente relación de recurrencia.
\begin{equation}
p_k = \left( a + \frac{b}{k} \right)p_{k-1},\qquad \forall k \in \mathbb{N} \setminus \{0\} 
\end{equation}
:::

Además tenemos la siguiente proposición que caracteriza a la distribución de las variables 
aleatorias en la familia de Panjer.

::: {.proposition #chfampanjer name="Caracterización familia de Panjer"}
Las únicas leyes de probabilidad que satisfacen la relación de recurrencia anterior son:

1. La ley de Poisson, la cual se obtiene para $a = 0$ y $b > 0$

2. La ley binomial negativa, la cual se obtiene para $0 < a < 1$ y $a + b > 0$

3. La ley binomial, la cual se obtenida para $a < 0$ y $b = -a(m + 1)$, para cierto $m$ entero y 
positivo.
:::

Para una demostración detallada de la proposición anterior se puede consultar @MathAssuNV1 o 
en https://nonlifemaths.github.io/.